{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Speech_recognition_wenet.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNrZyibMcPZAFyyHd49hkhB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tingtingyuli800/github-slideshow/blob/tingtingyuli800-patch-1/Speech_recognition_wenet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzAMqoWLF1cV",
        "outputId": "684bd03f-9575-4744-e77b-2e04c43bd93b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"/content/gdrive/My Drive/Colab Notebooks\")"
      ],
      "metadata": {
        "id": "D_215ms1Hb2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%% [code]\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KBGKri8IX44",
        "outputId": "f9837edb-d028-47a0-ff16-988fe253f37b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon May 23 07:07:13 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/wenet-e2e/wenet.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orsHjLciHi5e",
        "outputId": "c88cca2b-737c-4d58-9cb7-48d859dfb408"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'wenet'...\n",
            "remote: Enumerating objects: 8033, done.\u001b[K\n",
            "remote: Counting objects: 100% (382/382), done.\u001b[K\n",
            "remote: Compressing objects: 100% (232/232), done.\u001b[K\n",
            "remote: Total 8033 (delta 163), reused 300 (delta 131), pack-reused 7651\u001b[K\n",
            "Receiving objects: 100% (8033/8033), 16.49 MiB | 17.39 MiB/s, done.\n",
            "Resolving deltas: 100% (4461/4461), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -c https://repo.anaconda.com/miniconda/Miniconda3-py37_4.10.3-Linux-x86_64.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IplS7sNdJLNj",
        "outputId": "50fff85c-e365-4f1f-b881-4961352cf13c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-23 07:07:24--  https://repo.anaconda.com/miniconda/Miniconda3-py37_4.10.3-Linux-x86_64.sh\n",
            "Resolving repo.anaconda.com (repo.anaconda.com)... 104.16.131.3, 104.16.130.3, 2606:4700::6810:8203, ...\n",
            "Connecting to repo.anaconda.com (repo.anaconda.com)|104.16.131.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 89026327 (85M) [application/x-sh]\n",
            "Saving to: ‘Miniconda3-py37_4.10.3-Linux-x86_64.sh’\n",
            "\n",
            "Miniconda3-py37_4.1 100%[===================>]  84.90M  16.1MB/s    in 5.8s    \n",
            "\n",
            "2022-05-23 07:07:30 (14.7 MB/s) - ‘Miniconda3-py37_4.10.3-Linux-x86_64.sh’ saved [89026327/89026327]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9gkuV-qJVPI",
        "outputId": "1a97442e-444e-4520-d84b-0ab0fac4d552"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Miniconda3-py37_4.10.3-Linux-x86_64.sh\tsample_data  wenet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod +x Miniconda3-py37_4.10.3-Linux-x86_64.sh"
      ],
      "metadata": {
        "id": "0D2dquWrJdZm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%env PYTHONPATH=\n",
        "!./Miniconda3-py37_4.10.3-Linux-x86_64.sh -b -f -p /usr/local"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTjtkmMnJzrf",
        "outputId": "1578fc10-71cb-4a67-8513-12dd83e14fe8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: PYTHONPATH=\n",
            "PREFIX=/usr/local\n",
            "Unpacking payload ...\n",
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\bdone\n",
            "Solving environment: / \b\b- \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - _libgcc_mutex==0.1=main\n",
            "    - _openmp_mutex==4.5=1_gnu\n",
            "    - brotlipy==0.7.0=py37h27cfd23_1003\n",
            "    - ca-certificates==2021.7.5=h06a4308_1\n",
            "    - certifi==2021.5.30=py37h06a4308_0\n",
            "    - cffi==1.14.6=py37h400218f_0\n",
            "    - chardet==4.0.0=py37h06a4308_1003\n",
            "    - conda-package-handling==1.7.3=py37h27cfd23_1\n",
            "    - conda==4.10.3=py37h06a4308_0\n",
            "    - cryptography==3.4.7=py37hd23ed53_0\n",
            "    - idna==2.10=pyhd3eb1b0_0\n",
            "    - ld_impl_linux-64==2.35.1=h7274673_9\n",
            "    - libffi==3.3=he6710b0_2\n",
            "    - libgcc-ng==9.3.0=h5101ec6_17\n",
            "    - libgomp==9.3.0=h5101ec6_17\n",
            "    - libstdcxx-ng==9.3.0=hd4cf53a_17\n",
            "    - ncurses==6.2=he6710b0_1\n",
            "    - openssl==1.1.1k=h27cfd23_0\n",
            "    - pip==21.1.3=py37h06a4308_0\n",
            "    - pycosat==0.6.3=py37h27cfd23_0\n",
            "    - pycparser==2.20=py_2\n",
            "    - pyopenssl==20.0.1=pyhd3eb1b0_1\n",
            "    - pysocks==1.7.1=py37_1\n",
            "    - python==3.7.10=h12debd9_4\n",
            "    - readline==8.1=h27cfd23_0\n",
            "    - requests==2.25.1=pyhd3eb1b0_0\n",
            "    - ruamel_yaml==0.15.100=py37h27cfd23_0\n",
            "    - setuptools==52.0.0=py37h06a4308_0\n",
            "    - six==1.16.0=pyhd3eb1b0_0\n",
            "    - sqlite==3.36.0=hc218d9a_0\n",
            "    - tk==8.6.10=hbc83047_0\n",
            "    - tqdm==4.61.2=pyhd3eb1b0_1\n",
            "    - urllib3==1.26.6=pyhd3eb1b0_1\n",
            "    - wheel==0.36.2=pyhd3eb1b0_0\n",
            "    - xz==5.2.5=h7b6447c_0\n",
            "    - yaml==0.2.5=h7b6447c_0\n",
            "    - zlib==1.2.11=h7b6447c_3\n",
            "\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  _libgcc_mutex      pkgs/main/linux-64::_libgcc_mutex-0.1-main\n",
            "  _openmp_mutex      pkgs/main/linux-64::_openmp_mutex-4.5-1_gnu\n",
            "  brotlipy           pkgs/main/linux-64::brotlipy-0.7.0-py37h27cfd23_1003\n",
            "  ca-certificates    pkgs/main/linux-64::ca-certificates-2021.7.5-h06a4308_1\n",
            "  certifi            pkgs/main/linux-64::certifi-2021.5.30-py37h06a4308_0\n",
            "  cffi               pkgs/main/linux-64::cffi-1.14.6-py37h400218f_0\n",
            "  chardet            pkgs/main/linux-64::chardet-4.0.0-py37h06a4308_1003\n",
            "  conda              pkgs/main/linux-64::conda-4.10.3-py37h06a4308_0\n",
            "  conda-package-han~ pkgs/main/linux-64::conda-package-handling-1.7.3-py37h27cfd23_1\n",
            "  cryptography       pkgs/main/linux-64::cryptography-3.4.7-py37hd23ed53_0\n",
            "  idna               pkgs/main/noarch::idna-2.10-pyhd3eb1b0_0\n",
            "  ld_impl_linux-64   pkgs/main/linux-64::ld_impl_linux-64-2.35.1-h7274673_9\n",
            "  libffi             pkgs/main/linux-64::libffi-3.3-he6710b0_2\n",
            "  libgcc-ng          pkgs/main/linux-64::libgcc-ng-9.3.0-h5101ec6_17\n",
            "  libgomp            pkgs/main/linux-64::libgomp-9.3.0-h5101ec6_17\n",
            "  libstdcxx-ng       pkgs/main/linux-64::libstdcxx-ng-9.3.0-hd4cf53a_17\n",
            "  ncurses            pkgs/main/linux-64::ncurses-6.2-he6710b0_1\n",
            "  openssl            pkgs/main/linux-64::openssl-1.1.1k-h27cfd23_0\n",
            "  pip                pkgs/main/linux-64::pip-21.1.3-py37h06a4308_0\n",
            "  pycosat            pkgs/main/linux-64::pycosat-0.6.3-py37h27cfd23_0\n",
            "  pycparser          pkgs/main/noarch::pycparser-2.20-py_2\n",
            "  pyopenssl          pkgs/main/noarch::pyopenssl-20.0.1-pyhd3eb1b0_1\n",
            "  pysocks            pkgs/main/linux-64::pysocks-1.7.1-py37_1\n",
            "  python             pkgs/main/linux-64::python-3.7.10-h12debd9_4\n",
            "  readline           pkgs/main/linux-64::readline-8.1-h27cfd23_0\n",
            "  requests           pkgs/main/noarch::requests-2.25.1-pyhd3eb1b0_0\n",
            "  ruamel_yaml        pkgs/main/linux-64::ruamel_yaml-0.15.100-py37h27cfd23_0\n",
            "  setuptools         pkgs/main/linux-64::setuptools-52.0.0-py37h06a4308_0\n",
            "  six                pkgs/main/noarch::six-1.16.0-pyhd3eb1b0_0\n",
            "  sqlite             pkgs/main/linux-64::sqlite-3.36.0-hc218d9a_0\n",
            "  tk                 pkgs/main/linux-64::tk-8.6.10-hbc83047_0\n",
            "  tqdm               pkgs/main/noarch::tqdm-4.61.2-pyhd3eb1b0_1\n",
            "  urllib3            pkgs/main/noarch::urllib3-1.26.6-pyhd3eb1b0_1\n",
            "  wheel              pkgs/main/noarch::wheel-0.36.2-pyhd3eb1b0_0\n",
            "  xz                 pkgs/main/linux-64::xz-5.2.5-h7b6447c_0\n",
            "  yaml               pkgs/main/linux-64::yaml-0.2.5-h7b6447c_0\n",
            "  zlib               pkgs/main/linux-64::zlib-1.2.11-h7b6447c_3\n",
            "\n",
            "\n",
            "Preparing transaction: | \b\b/ \b\b- \b\bdone\n",
            "Executing transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "installation finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!which python\n",
        "!python -V"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5w24gujJ3vm",
        "outputId": "4b7c4cf0-9feb-44aa-d0e9-45e66a241a4c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/bin/python\n",
            "Python 3.7.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "_ = (sys.path.append(\"/usr/local/lib/python3.7/site-packages\"))"
      ],
      "metadata": {
        "id": "nDl9w_bfJ-Sg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!conda env list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42GrZGKPKPBI",
        "outputId": "8c9f4c2e-45af-4612-a400-ab3cb8221314"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# conda environments:\n",
            "#\n",
            "base                  *  /usr/local\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!conda install --channel defaults conda python=3.8 --yes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPsuyF1lKSRn",
        "outputId": "924ac351-8b3c-454b-ddb3-883460e2df1a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Solving environment: - \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - conda\n",
            "    - python=3.8\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    brotlipy-0.7.0             |py38h27cfd23_1003         323 KB\n",
            "    ca-certificates-2022.4.26  |       h06a4308_0         124 KB\n",
            "    certifi-2022.5.18.1        |   py38h06a4308_0         147 KB\n",
            "    cffi-1.15.0                |   py38hd667e15_1         224 KB\n",
            "    chardet-4.0.0              |py38h06a4308_1003         194 KB\n",
            "    conda-4.12.0               |   py38h06a4308_0        14.5 MB\n",
            "    conda-package-handling-1.8.1|   py38h7f8727e_0         885 KB\n",
            "    cryptography-37.0.1        |   py38h9ce1e76_0         1.3 MB\n",
            "    ncurses-6.3                |       h7f8727e_2         782 KB\n",
            "    openssl-1.1.1o             |       h7f8727e_0         2.5 MB\n",
            "    pip-21.2.4                 |   py38h06a4308_0         1.8 MB\n",
            "    pycosat-0.6.3              |   py38h7b6447c_1          82 KB\n",
            "    pysocks-1.7.1              |   py38h06a4308_0          31 KB\n",
            "    python-3.8.13              |       h12debd9_0        18.8 MB\n",
            "    ruamel_yaml-0.15.100       |   py38h27cfd23_0         258 KB\n",
            "    setuptools-61.2.0          |   py38h06a4308_0        1012 KB\n",
            "    sqlite-3.38.2              |       hc218d9a_0         1.0 MB\n",
            "    tk-8.6.11                  |       h1ccaba5_0         3.0 MB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        46.9 MB\n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  ca-certificates                       2021.7.5-h06a4308_1 --> 2022.4.26-h06a4308_0\n",
            "  certifi                          2021.5.30-py37h06a4308_0 --> 2022.5.18.1-py38h06a4308_0\n",
            "  cffi                                1.14.6-py37h400218f_0 --> 1.15.0-py38hd667e15_1\n",
            "  conda                               4.10.3-py37h06a4308_0 --> 4.12.0-py38h06a4308_0\n",
            "  conda-package-han~                   1.7.3-py37h27cfd23_1 --> 1.8.1-py38h7f8727e_0\n",
            "  cryptography                         3.4.7-py37hd23ed53_0 --> 37.0.1-py38h9ce1e76_0\n",
            "  ncurses                                    6.2-he6710b0_1 --> 6.3-h7f8727e_2\n",
            "  openssl                                 1.1.1k-h27cfd23_0 --> 1.1.1o-h7f8727e_0\n",
            "  pip                                 21.1.3-py37h06a4308_0 --> 21.2.4-py38h06a4308_0\n",
            "  pycosat                              0.6.3-py37h27cfd23_0 --> 0.6.3-py38h7b6447c_1\n",
            "  python                                  3.7.10-h12debd9_4 --> 3.8.13-h12debd9_0\n",
            "  setuptools                          52.0.0-py37h06a4308_0 --> 61.2.0-py38h06a4308_0\n",
            "  sqlite                                  3.36.0-hc218d9a_0 --> 3.38.2-hc218d9a_0\n",
            "  tk                                      8.6.10-hbc83047_0 --> 8.6.11-h1ccaba5_0\n",
            "\n",
            "The following packages will be DOWNGRADED:\n",
            "\n",
            "  brotlipy                          0.7.0-py37h27cfd23_1003 --> 0.7.0-py38h27cfd23_1003\n",
            "  chardet                           4.0.0-py37h06a4308_1003 --> 4.0.0-py38h06a4308_1003\n",
            "  pysocks                                      1.7.1-py37_1 --> 1.7.1-py38h06a4308_0\n",
            "  ruamel_yaml                       0.15.100-py37h27cfd23_0 --> 0.15.100-py38h27cfd23_0\n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "ruamel_yaml-0.15.100 | 258 KB    | : 100% 1.0/1 [00:00<00:00, 13.23it/s]\n",
            "tk-8.6.11            | 3.0 MB    | : 100% 1.0/1 [00:00<00:00,  1.15it/s]               \n",
            "brotlipy-0.7.0       | 323 KB    | : 100% 1.0/1 [00:00<00:00, 21.11it/s]\n",
            "pysocks-1.7.1        | 31 KB     | : 100% 1.0/1 [00:00<00:00, 30.68it/s]\n",
            "conda-package-handli | 885 KB    | : 100% 1.0/1 [00:00<00:00, 17.77it/s]\n",
            "pip-21.2.4           | 1.8 MB    | : 100% 1.0/1 [00:00<00:00,  7.37it/s]\n",
            "cffi-1.15.0          | 224 KB    | : 100% 1.0/1 [00:00<00:00, 17.84it/s]\n",
            "ncurses-6.3          | 782 KB    | : 100% 1.0/1 [00:00<00:00,  4.99it/s]\n",
            "chardet-4.0.0        | 194 KB    | : 100% 1.0/1 [00:00<00:00, 22.66it/s]\n",
            "setuptools-61.2.0    | 1012 KB   | : 100% 1.0/1 [00:00<00:00,  7.62it/s]\n",
            "cryptography-37.0.1  | 1.3 MB    | : 100% 1.0/1 [00:00<00:00,  9.88it/s]\n",
            "conda-4.12.0         | 14.5 MB   | : 100% 1.0/1 [00:00<00:00,  1.93it/s]               \n",
            "python-3.8.13        | 18.8 MB   | : 100% 1.0/1 [00:00<00:00,  1.61it/s]               \n",
            "ca-certificates-2022 | 124 KB    | : 100% 1.0/1 [00:00<00:00, 22.31it/s]\n",
            "sqlite-3.38.2        | 1.0 MB    | : 100% 1.0/1 [00:00<00:00, 15.61it/s]\n",
            "openssl-1.1.1o       | 2.5 MB    | : 100% 1.0/1 [00:00<00:00,  9.49it/s]\n",
            "certifi-2022.5.18.1  | 147 KB    | : 100% 1.0/1 [00:00<00:00, 21.99it/s]\n",
            "pycosat-0.6.3        | 82 KB     | : 100% 1.0/1 [00:00<00:00, 25.32it/s]\n",
            "Preparing transaction: | \b\b/ \b\b- \b\bdone\n",
            "Verifying transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Executing transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -V"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geHhRxPuKWlu",
        "outputId": "4489a712-658a-41fd-b7aa-0a626938bde0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.8.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!conda update --channel defaults --all --yes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXTwcSUVKd16",
        "outputId": "e21cf215-e2b7-43a8-e505-745d4d28a59f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Solving environment: \\ \b\b| \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    _openmp_mutex-5.1          |            1_gnu          21 KB\n",
            "    charset-normalizer-2.0.4   |     pyhd3eb1b0_0          35 KB\n",
            "    idna-3.3                   |     pyhd3eb1b0_0          49 KB\n",
            "    ld_impl_linux-64-2.38      |       h1181459_1         654 KB\n",
            "    libgcc-ng-11.2.0           |       h1234567_0         5.3 MB\n",
            "    libgomp-11.2.0             |       h1234567_0         473 KB\n",
            "    libstdcxx-ng-11.2.0        |       h1234567_0         4.7 MB\n",
            "    pycparser-2.21             |     pyhd3eb1b0_0          94 KB\n",
            "    pyopenssl-22.0.0           |     pyhd3eb1b0_0          50 KB\n",
            "    readline-8.1.2             |       h7f8727e_1         354 KB\n",
            "    requests-2.27.1            |     pyhd3eb1b0_0          54 KB\n",
            "    sqlite-3.38.3              |       hc218d9a_0         1.0 MB\n",
            "    tk-8.6.11                  |       h1ccaba5_1         3.0 MB\n",
            "    tqdm-4.64.0                |   py38h06a4308_0         126 KB\n",
            "    urllib3-1.26.9             |   py38h06a4308_0         180 KB\n",
            "    wheel-0.37.1               |     pyhd3eb1b0_0          33 KB\n",
            "    xz-5.2.5                   |       h7f8727e_1         339 KB\n",
            "    zlib-1.2.12                |       h7f8727e_2         106 KB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        16.6 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  charset-normalizer pkgs/main/noarch::charset-normalizer-2.0.4-pyhd3eb1b0_0\n",
            "\n",
            "The following packages will be REMOVED:\n",
            "\n",
            "  chardet-4.0.0-py38h06a4308_1003\n",
            "  six-1.16.0-pyhd3eb1b0_0\n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  _openmp_mutex                                   4.5-1_gnu --> 5.1-1_gnu\n",
            "  idna                                    2.10-pyhd3eb1b0_0 --> 3.3-pyhd3eb1b0_0\n",
            "  ld_impl_linux-64                        2.35.1-h7274673_9 --> 2.38-h1181459_1\n",
            "  libgcc-ng                               9.3.0-h5101ec6_17 --> 11.2.0-h1234567_0\n",
            "  libgomp                                 9.3.0-h5101ec6_17 --> 11.2.0-h1234567_0\n",
            "  libstdcxx-ng                            9.3.0-hd4cf53a_17 --> 11.2.0-h1234567_0\n",
            "  pycparser                                       2.20-py_2 --> 2.21-pyhd3eb1b0_0\n",
            "  pyopenssl                             20.0.1-pyhd3eb1b0_1 --> 22.0.0-pyhd3eb1b0_0\n",
            "  readline                                   8.1-h27cfd23_0 --> 8.1.2-h7f8727e_1\n",
            "  requests                              2.25.1-pyhd3eb1b0_0 --> 2.27.1-pyhd3eb1b0_0\n",
            "  sqlite                                  3.38.2-hc218d9a_0 --> 3.38.3-hc218d9a_0\n",
            "  tk                                      8.6.11-h1ccaba5_0 --> 8.6.11-h1ccaba5_1\n",
            "  tqdm               pkgs/main/noarch::tqdm-4.61.2-pyhd3eb~ --> pkgs/main/linux-64::tqdm-4.64.0-py38h06a4308_0\n",
            "  urllib3            pkgs/main/noarch::urllib3-1.26.6-pyhd~ --> pkgs/main/linux-64::urllib3-1.26.9-py38h06a4308_0\n",
            "  wheel                                 0.36.2-pyhd3eb1b0_0 --> 0.37.1-pyhd3eb1b0_0\n",
            "  xz                                       5.2.5-h7b6447c_0 --> 5.2.5-h7f8727e_1\n",
            "  zlib                                    1.2.11-h7b6447c_3 --> 1.2.12-h7f8727e_2\n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "ld_impl_linux-64-2.3 | 654 KB    | : 100% 1.0/1 [00:00<00:00, 12.20it/s]\n",
            "tqdm-4.64.0          | 126 KB    | : 100% 1.0/1 [00:00<00:00,  3.28it/s]                \n",
            "libgcc-ng-11.2.0     | 5.3 MB    | : 100% 1.0/1 [00:00<00:00,  6.68it/s]\n",
            "libstdcxx-ng-11.2.0  | 4.7 MB    | : 100% 1.0/1 [00:00<00:00,  6.30it/s]\n",
            "wheel-0.37.1         | 33 KB     | : 100% 1.0/1 [00:00<00:00, 23.90it/s]\n",
            "requests-2.27.1      | 54 KB     | : 100% 1.0/1 [00:00<00:00, 31.44it/s]\n",
            "libgomp-11.2.0       | 473 KB    | : 100% 1.0/1 [00:00<00:00, 20.48it/s]\n",
            "idna-3.3             | 49 KB     | : 100% 1.0/1 [00:00<00:00, 28.57it/s]\n",
            "zlib-1.2.12          | 106 KB    | : 100% 1.0/1 [00:00<00:00, 26.72it/s]\n",
            "tk-8.6.11            | 3.0 MB    | : 100% 1.0/1 [00:00<00:00,  8.67it/s]\n",
            "urllib3-1.26.9       | 180 KB    | : 100% 1.0/1 [00:00<00:00, 25.50it/s]\n",
            "_openmp_mutex-5.1    | 21 KB     | : 100% 1.0/1 [00:00<00:00, 25.58it/s]\n",
            "readline-8.1.2       | 354 KB    | : 100% 1.0/1 [00:00<00:00, 23.94it/s]\n",
            "pycparser-2.21       | 94 KB     | : 100% 1.0/1 [00:00<00:00, 21.12it/s]\n",
            "charset-normalizer-2 | 35 KB     | : 100% 1.0/1 [00:00<00:00, 27.12it/s]\n",
            "sqlite-3.38.3        | 1.0 MB    | : 100% 1.0/1 [00:00<00:00, 20.15it/s]\n",
            "xz-5.2.5             | 339 KB    | : 100% 1.0/1 [00:00<00:00, 19.26it/s]\n",
            "pyopenssl-22.0.0     | 50 KB     | : 100% 1.0/1 [00:00<00:00, 28.67it/s]\n",
            "Preparing transaction: - \b\bdone\n",
            "Verifying transaction: | \b\bdone\n",
            "Executing transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wttWcKiKxUh",
        "outputId": "e082e8bd-9a42-4d22-c2bd-49a959e1986f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;32mMiniconda3-py37_4.10.3-Linux-x86_64.sh\u001b[0m*  \u001b[01;34msample_data\u001b[0m/  \u001b[01;34mwenet\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd wenet/examples/aishell/s0/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51qoIgCXK6pI",
        "outputId": "fd5bb609-d819-492a-93e0-1bf399dc231b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/wenet/examples/aishell/s0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd ../../.."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aJ25iZ1LcMY",
        "outputId": "7cdfe4cf-c397-474e-fa44-25966d11d213"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/wenet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_s_d6KNLmZi",
        "outputId": "36a48865-e4a2-424d-95bf-c8cb2fda5238"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Pillow\n",
            "  Downloading Pillow-9.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 27.4 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (701 kB)\n",
            "\u001b[K     |████████████████████████████████| 701 kB 97.8 MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 100.2 MB/s \n",
            "\u001b[?25hCollecting tensorboard\n",
            "  Downloading tensorboard-2.9.0-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 81.1 MB/s \n",
            "\u001b[?25hCollecting tensorboardX\n",
            "  Downloading tensorboardX-2.5-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 85.8 MB/s \n",
            "\u001b[?25hCollecting typeguard\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Collecting textgrid\n",
            "  Downloading TextGrid-1.5-py3-none-any.whl (10.0 kB)\n",
            "Collecting pytest\n",
            "  Downloading pytest-7.1.2-py3-none-any.whl (297 kB)\n",
            "\u001b[K     |████████████████████████████████| 297 kB 89.6 MB/s \n",
            "\u001b[?25hCollecting flake8==3.8.2\n",
            "  Downloading flake8-3.8.2-py2.py3-none-any.whl (72 kB)\n",
            "\u001b[K     |████████████████████████████████| 72 kB 1.2 MB/s \n",
            "\u001b[?25hCollecting flake8-bugbear\n",
            "  Downloading flake8_bugbear-22.4.25-py3-none-any.whl (19 kB)\n",
            "Collecting flake8-comprehensions\n",
            "  Downloading flake8_comprehensions-3.10.0-py3-none-any.whl (7.3 kB)\n",
            "Collecting flake8-executable\n",
            "  Downloading flake8_executable-2.1.1-py3-none-any.whl (35 kB)\n",
            "Collecting flake8-pyi==20.5.0\n",
            "  Downloading flake8_pyi-20.5.0-py36-none-any.whl (9.6 kB)\n",
            "Collecting mccabe\n",
            "  Downloading mccabe-0.7.0-py2.py3-none-any.whl (7.3 kB)\n",
            "Collecting pycodestyle==2.6.0\n",
            "  Downloading pycodestyle-2.6.0-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 326 kB/s \n",
            "\u001b[?25hCollecting pyflakes==2.2.0\n",
            "  Downloading pyflakes-2.2.0-py2.py3-none-any.whl (66 kB)\n",
            "\u001b[K     |████████████████████████████████| 66 kB 6.0 MB/s \n",
            "\u001b[?25hCollecting mccabe\n",
            "  Downloading mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)\n",
            "Collecting attrs\n",
            "  Downloading attrs-21.4.0-py2.py3-none-any.whl (60 kB)\n",
            "\u001b[K     |████████████████████████████████| 60 kB 9.5 MB/s \n",
            "\u001b[?25hCollecting numpy>=1.12.0\n",
            "  Downloading numpy-1.22.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 16.9 MB 60.5 MB/s \n",
            "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
            "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "\u001b[K     |████████████████████████████████| 781 kB 104.3 MB/s \n",
            "\u001b[?25hCollecting markdown>=2.6.8\n",
            "  Downloading Markdown-3.3.7-py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 7.9 MB/s \n",
            "\u001b[?25hCollecting grpcio>=1.24.3\n",
            "  Downloading grpcio-1.46.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 67.3 MB/s \n",
            "\u001b[?25hCollecting protobuf>=3.9.2\n",
            "  Downloading protobuf-3.20.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 30.6 MB/s \n",
            "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Collecting absl-py>=0.4\n",
            "  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
            "\u001b[K     |████████████████████████████████| 126 kB 97.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 4)) (0.37.1)\n",
            "Collecting google-auth<3,>=1.6.3\n",
            "  Downloading google_auth-2.6.6-py2.py3-none-any.whl (156 kB)\n",
            "\u001b[K     |████████████████████████████████| 156 kB 3.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 4)) (61.2.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 4)) (2.27.1)\n",
            "Collecting werkzeug>=1.0.1\n",
            "  Downloading Werkzeug-2.1.2-py3-none-any.whl (224 kB)\n",
            "\u001b[K     |████████████████████████████████| 224 kB 16.8 MB/s \n",
            "\u001b[?25hCollecting tensorboard-data-server<0.7.0,>=0.6.0\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 85.5 MB/s \n",
            "\u001b[?25hCollecting six\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting pluggy<2.0,>=0.12\n",
            "  Downloading pluggy-1.0.0-py2.py3-none-any.whl (13 kB)\n",
            "Collecting iniconfig\n",
            "  Downloading iniconfig-1.1.1-py2.py3-none-any.whl (5.0 kB)\n",
            "Collecting packaging\n",
            "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
            "\u001b[K     |████████████████████████████████| 40 kB 3.5 MB/s \n",
            "\u001b[?25hCollecting tomli>=1.0.0\n",
            "  Downloading tomli-2.0.1-py3-none-any.whl (12 kB)\n",
            "Collecting py>=1.8.2\n",
            "  Downloading py-1.11.0-py2.py3-none-any.whl (98 kB)\n",
            "\u001b[K     |████████████████████████████████| 98 kB 10.1 MB/s \n",
            "\u001b[?25hCollecting pyasn1-modules>=0.2.1\n",
            "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
            "\u001b[K     |████████████████████████████████| 155 kB 90.0 MB/s \n",
            "\u001b[?25hCollecting cachetools<6.0,>=2.0.0\n",
            "  Downloading cachetools-5.1.0-py3-none-any.whl (9.2 kB)\n",
            "Collecting rsa<5,>=3.1.4\n",
            "  Downloading rsa-4.8-py3-none-any.whl (39 kB)\n",
            "Collecting requests-oauthlib>=0.7.0\n",
            "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
            "Collecting importlib-metadata>=4.4\n",
            "  Downloading importlib_metadata-4.11.4-py3-none-any.whl (18 kB)\n",
            "Collecting zipp>=0.5\n",
            "  Downloading zipp-3.8.0-py3-none-any.whl (5.4 kB)\n",
            "Collecting pyasn1<0.5.0,>=0.4.6\n",
            "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 7.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 4)) (2.0.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 4)) (1.26.9)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 4)) (3.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 4)) (2022.5.18.1)\n",
            "Collecting oauthlib>=3.0.0\n",
            "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
            "\u001b[K     |████████████████████████████████| 151 kB 113.0 MB/s \n",
            "\u001b[?25hCollecting pyparsing!=3.0.5,>=2.0.2\n",
            "  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
            "\u001b[K     |████████████████████████████████| 98 kB 9.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyasn1, zipp, six, rsa, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, pyparsing, pyflakes, pycodestyle, mccabe, importlib-metadata, google-auth, werkzeug, tomli, tensorboard-plugin-wit, tensorboard-data-server, py, protobuf, pluggy, packaging, numpy, markdown, iniconfig, grpcio, google-auth-oauthlib, flake8, attrs, absl-py, typeguard, textgrid, tensorboardX, tensorboard, sentencepiece, pyyaml, pytest, Pillow, flake8-pyi, flake8-executable, flake8-comprehensions, flake8-bugbear\n",
            "Successfully installed Pillow-9.1.1 absl-py-1.0.0 attrs-21.4.0 cachetools-5.1.0 flake8-3.8.2 flake8-bugbear-22.4.25 flake8-comprehensions-3.10.0 flake8-executable-2.1.1 flake8-pyi-20.5.0 google-auth-2.6.6 google-auth-oauthlib-0.4.6 grpcio-1.46.3 importlib-metadata-4.11.4 iniconfig-1.1.1 markdown-3.3.7 mccabe-0.6.1 numpy-1.22.4 oauthlib-3.2.0 packaging-21.3 pluggy-1.0.0 protobuf-3.20.1 py-1.11.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycodestyle-2.6.0 pyflakes-2.2.0 pyparsing-3.0.9 pytest-7.1.2 pyyaml-6.0 requests-oauthlib-1.3.1 rsa-4.8 sentencepiece-0.1.96 six-1.16.0 tensorboard-2.9.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorboardX-2.5 textgrid-1.5 tomli-2.0.1 typeguard-2.13.3 werkzeug-2.1.2 zipp-3.8.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!conda install pytorch=1.10.0 torchvision torchaudio=0.10.0 cudatoolkit=11.1 -c pytorch -c conda-forge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifxvpVF3LyEC",
        "outputId": "0d10aeec-7332-4176-b76b-93916784ffcb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Solving environment: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - cudatoolkit=11.1\n",
            "    - pytorch=1.10.0\n",
            "    - torchaudio=0.10.0\n",
            "    - torchvision\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    blas-1.0                   |              mkl           6 KB\n",
            "    bzip2-1.0.8                |       h7f98852_4         484 KB  conda-forge\n",
            "    ca-certificates-2022.5.18.1|       ha878542_0         144 KB  conda-forge\n",
            "    certifi-2022.5.18.1        |   py38h578d9bd_0         150 KB  conda-forge\n",
            "    conda-4.12.0               |   py38h578d9bd_0         1.0 MB  conda-forge\n",
            "    cudatoolkit-11.1.1         |      ha002fc5_10        1.20 GB  conda-forge\n",
            "    ffmpeg-4.3                 |       hf484d3e_0         9.9 MB  pytorch\n",
            "    freetype-2.10.4            |       h0708190_1         890 KB  conda-forge\n",
            "    gmp-6.2.1                  |       h58526e2_0         806 KB  conda-forge\n",
            "    gnutls-3.6.13              |       h85f3911_1         2.0 MB  conda-forge\n",
            "    intel-openmp-2021.4.0      |    h06a4308_3561         4.2 MB\n",
            "    jpeg-9e                    |       h166bdaf_1         268 KB  conda-forge\n",
            "    lame-3.100                 |    h7f98852_1001         496 KB  conda-forge\n",
            "    libiconv-1.17              |       h166bdaf_0         1.4 MB  conda-forge\n",
            "    libpng-1.6.37              |       h21135ba_2         306 KB  conda-forge\n",
            "    libtiff-4.0.10             |    hc3755c2_1005         602 KB  conda-forge\n",
            "    libuv-1.43.0               |       h7f98852_0         1.0 MB  conda-forge\n",
            "    lz4-c-1.9.3                |       h9c3ff4c_1         179 KB  conda-forge\n",
            "    mkl-2021.4.0               |     h06a4308_640       142.6 MB\n",
            "    mkl-service-2.4.0          |   py38h95df7f1_0          61 KB  conda-forge\n",
            "    mkl_fft-1.3.1              |   py38h8666266_1         213 KB  conda-forge\n",
            "    mkl_random-1.2.2           |   py38h1abd341_0         364 KB  conda-forge\n",
            "    nettle-3.6                 |       he412f7d_0         6.5 MB  conda-forge\n",
            "    numpy-1.22.3               |   py38he7a7128_0          10 KB\n",
            "    numpy-base-1.22.3          |   py38hf524024_0         5.4 MB\n",
            "    olefile-0.46               |     pyh9f0ad1d_1          32 KB  conda-forge\n",
            "    openh264-2.1.1             |       h780b84a_0         1.5 MB  conda-forge\n",
            "    openssl-1.1.1o             |       h166bdaf_0         2.1 MB  conda-forge\n",
            "    pillow-6.2.1               |   py38h6b7be26_0         637 KB  conda-forge\n",
            "    python_abi-3.8             |           2_cp38           4 KB  conda-forge\n",
            "    pytorch-1.10.0             |py3.8_cuda11.1_cudnn8.0.5_0        1.53 GB  pytorch\n",
            "    pytorch-mutex-1.0          |             cuda           3 KB  pytorch\n",
            "    six-1.16.0                 |     pyh6c4a22f_0          14 KB  conda-forge\n",
            "    torchaudio-0.10.0          |       py38_cu111         4.5 MB  pytorch\n",
            "    torchvision-0.11.1         |       py38_cu111        30.4 MB  pytorch\n",
            "    typing_extensions-4.2.0    |     pyha770c72_1          27 KB  conda-forge\n",
            "    zstd-1.4.9                 |       ha95c52a_0         431 KB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        2.95 GB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  blas               pkgs/main/linux-64::blas-1.0-mkl\n",
            "  bzip2              conda-forge/linux-64::bzip2-1.0.8-h7f98852_4\n",
            "  cudatoolkit        conda-forge/linux-64::cudatoolkit-11.1.1-ha002fc5_10\n",
            "  ffmpeg             pytorch/linux-64::ffmpeg-4.3-hf484d3e_0\n",
            "  freetype           conda-forge/linux-64::freetype-2.10.4-h0708190_1\n",
            "  gmp                conda-forge/linux-64::gmp-6.2.1-h58526e2_0\n",
            "  gnutls             conda-forge/linux-64::gnutls-3.6.13-h85f3911_1\n",
            "  intel-openmp       pkgs/main/linux-64::intel-openmp-2021.4.0-h06a4308_3561\n",
            "  jpeg               conda-forge/linux-64::jpeg-9e-h166bdaf_1\n",
            "  lame               conda-forge/linux-64::lame-3.100-h7f98852_1001\n",
            "  libiconv           conda-forge/linux-64::libiconv-1.17-h166bdaf_0\n",
            "  libpng             conda-forge/linux-64::libpng-1.6.37-h21135ba_2\n",
            "  libtiff            conda-forge/linux-64::libtiff-4.0.10-hc3755c2_1005\n",
            "  libuv              conda-forge/linux-64::libuv-1.43.0-h7f98852_0\n",
            "  lz4-c              conda-forge/linux-64::lz4-c-1.9.3-h9c3ff4c_1\n",
            "  mkl                pkgs/main/linux-64::mkl-2021.4.0-h06a4308_640\n",
            "  mkl-service        conda-forge/linux-64::mkl-service-2.4.0-py38h95df7f1_0\n",
            "  mkl_fft            conda-forge/linux-64::mkl_fft-1.3.1-py38h8666266_1\n",
            "  mkl_random         conda-forge/linux-64::mkl_random-1.2.2-py38h1abd341_0\n",
            "  nettle             conda-forge/linux-64::nettle-3.6-he412f7d_0\n",
            "  numpy              pkgs/main/linux-64::numpy-1.22.3-py38he7a7128_0\n",
            "  numpy-base         pkgs/main/linux-64::numpy-base-1.22.3-py38hf524024_0\n",
            "  olefile            conda-forge/noarch::olefile-0.46-pyh9f0ad1d_1\n",
            "  openh264           conda-forge/linux-64::openh264-2.1.1-h780b84a_0\n",
            "  pillow             conda-forge/linux-64::pillow-6.2.1-py38h6b7be26_0\n",
            "  python_abi         conda-forge/linux-64::python_abi-3.8-2_cp38\n",
            "  pytorch            pytorch/linux-64::pytorch-1.10.0-py3.8_cuda11.1_cudnn8.0.5_0\n",
            "  pytorch-mutex      pytorch/noarch::pytorch-mutex-1.0-cuda\n",
            "  six                conda-forge/noarch::six-1.16.0-pyh6c4a22f_0\n",
            "  torchaudio         pytorch/linux-64::torchaudio-0.10.0-py38_cu111\n",
            "  torchvision        pytorch/linux-64::torchvision-0.11.1-py38_cu111\n",
            "  typing_extensions  conda-forge/noarch::typing_extensions-4.2.0-pyha770c72_1\n",
            "  zstd               conda-forge/linux-64::zstd-1.4.9-ha95c52a_0\n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  ca-certificates    pkgs/main::ca-certificates-2022.4.26-~ --> conda-forge::ca-certificates-2022.5.18.1-ha878542_0\n",
            "\n",
            "The following packages will be SUPERSEDED by a higher-priority channel:\n",
            "\n",
            "  certifi            pkgs/main::certifi-2022.5.18.1-py38h0~ --> conda-forge::certifi-2022.5.18.1-py38h578d9bd_0\n",
            "  conda              pkgs/main::conda-4.12.0-py38h06a4308_0 --> conda-forge::conda-4.12.0-py38h578d9bd_0\n",
            "  openssl              pkgs/main::openssl-1.1.1o-h7f8727e_0 --> conda-forge::openssl-1.1.1o-h166bdaf_0\n",
            "\n",
            "\n",
            "Proceed ([y]/n)? y\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "blas-1.0             | 6 KB      | : 100% 1.0/1 [00:00<00:00, 11.17it/s]\n",
            "mkl-2021.4.0         | 142.6 MB  | :  93% 0.9323374578397966/1 [00:00<00:00,  1.13it/s]y\n",
            "mkl-2021.4.0         | 142.6 MB  | : 100% 1.0/1 [00:03<00:00,  3.95s/it]               \n",
            "numpy-1.22.3         | 10 KB     | : 100% 1.0/1 [00:00<00:00, 17.49it/s]\n",
            "lz4-c-1.9.3          | 179 KB    | : 100% 1.0/1 [00:00<00:00, 10.05it/s]\n",
            "cudatoolkit-11.1.1   | 1.20 GB   | : 100% 1.0/1 [02:28<00:00, 148.12s/it]              \n",
            "certifi-2022.5.18.1  | 150 KB    | : 100% 1.0/1 [00:00<00:00, 10.83it/s]\n",
            "openh264-2.1.1       | 1.5 MB    | : 100% 1.0/1 [00:00<00:00,  3.82it/s]\n",
            "mkl-service-2.4.0    | 61 KB     | : 100% 1.0/1 [00:00<00:00, 27.87it/s]\n",
            "libiconv-1.17        | 1.4 MB    | : 100% 1.0/1 [00:00<00:00,  4.96it/s]\n",
            "conda-4.12.0         | 1.0 MB    | : 100% 1.0/1 [00:00<00:00,  4.19it/s]\n",
            "lame-3.100           | 496 KB    | : 100% 1.0/1 [00:00<00:00,  9.95it/s]\n",
            "libpng-1.6.37        | 306 KB    | : 100% 1.0/1 [00:00<00:00, 13.66it/s]\n",
            "mkl_fft-1.3.1        | 213 KB    | : 100% 1.0/1 [00:00<00:00, 15.12it/s]\n",
            "torchaudio-0.10.0    | 4.5 MB    | : 100% 1.0/1 [00:04<00:00,  4.31s/it]\n",
            "openssl-1.1.1o       | 2.1 MB    | : 100% 1.0/1 [00:00<00:00,  3.01it/s]\n",
            "olefile-0.46         | 32 KB     | : 100% 1.0/1 [00:00<00:00, 26.84it/s]\n",
            "python_abi-3.8       | 4 KB      | : 100% 1.0/1 [00:00<00:00, 27.88it/s]\n",
            "pytorch-mutex-1.0    | 3 KB      | : 100% 1.0/1 [00:01<00:00,  1.24s/it]\n",
            "libuv-1.43.0         | 1.0 MB    | : 100% 1.0/1 [00:00<00:00,  5.48it/s]\n",
            "freetype-2.10.4      | 890 KB    | : 100% 1.0/1 [00:00<00:00,  6.35it/s]\n",
            "pytorch-1.10.0       | 1.53 GB   | : 100% 1.0/1 [06:14<00:00, 374.19s/it]              \n",
            "numpy-base-1.22.3    | 5.4 MB    | : 100% 1.0/1 [00:00<00:00,  3.24it/s]\n",
            "jpeg-9e              | 268 KB    | : 100% 1.0/1 [00:00<00:00,  9.46it/s]\n",
            "gnutls-3.6.13        | 2.0 MB    | : 100% 1.0/1 [00:00<00:00,  2.30it/s]\n",
            "torchvision-0.11.1   | 30.4 MB   | : 100% 1.0/1 [00:09<00:00,  9.20s/it]               \n",
            "libtiff-4.0.10       | 602 KB    | : 100% 1.0/1 [00:00<00:00,  7.65it/s]\n",
            "ffmpeg-4.3           | 9.9 MB    | : 100% 1.0/1 [00:03<00:00,  3.28s/it]               \n",
            "ca-certificates-2022 | 144 KB    | : 100% 1.0/1 [00:00<00:00, 17.16it/s]\n",
            "intel-openmp-2021.4. | 4.2 MB    | : 100% 1.0/1 [00:00<00:00,  6.12it/s]\n",
            "bzip2-1.0.8          | 484 KB    | : 100% 1.0/1 [00:00<00:00,  9.22it/s]\n",
            "nettle-3.6           | 6.5 MB    | : 100% 1.0/1 [00:00<00:00,  1.16it/s]\n",
            "mkl_random-1.2.2     | 364 KB    | : 100% 1.0/1 [00:00<00:00,  9.77it/s]\n",
            "six-1.16.0           | 14 KB     | : 100% 1.0/1 [00:00<00:00, 32.71it/s]\n",
            "zstd-1.4.9           | 431 KB    | : 100% 1.0/1 [00:00<00:00, 10.70it/s]\n",
            "gmp-6.2.1            | 806 KB    | : 100% 1.0/1 [00:00<00:00,  7.00it/s]\n",
            "pillow-6.2.1         | 637 KB    | : 100% 1.0/1 [00:00<00:00,  7.73it/s]\n",
            "typing_extensions-4. | 27 KB     | : 100% 1.0/1 [00:00<00:00, 21.60it/s]\n",
            "Preparing transaction: - \b\b\\ \b\b| \b\bdone\n",
            "Verifying transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Executing transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ By downloading and using the CUDA Toolkit conda packages, you accept the terms and conditions of the CUDA End User License Agreement (EULA): https://docs.nvidia.com/cuda/eula/index.html\n",
            "\n",
            "\b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd examples/aishell/s0/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYupElwCOQTB",
        "outputId": "7fdc12bd-acbf-43de-e32d-fa622730670d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/wenet/examples/aishell/s0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "grep \"local\" run.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjlH_xPAOkcT",
        "outputId": "3b2803ee-68ad-4472-ca17-dbf881597b92"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  local/download_and_untar.sh ${data} ${data_url} data_aishell\n",
            "  local/download_and_untar.sh ${data} ${data_url} resource_aishell\n",
            "  local/aishell_data_prep.sh ${data}/data_aishell/wav \\\n",
            "  mkdir -p data/local/dict\n",
            "  cp $unit_file data/local/dict/units.txt\n",
            "    data/local/dict/lexicon.txt\n",
            "  lm=data/local/lm\n",
            "  local/aishell_train_lms.sh\n",
            "    data/local/dict data/local/tmp data/local/lang\n",
            "  tools/fst/make_tlg.sh data/local/lm data/local/lang data/lang_test || exit 1;\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "bash run.sh > run.sh.log.stage.1 2>&1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "KVQun8kNP9ZN",
        "outputId": "6e13ce8f-87ff-45fb-f3b8-449182a7ef38"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "CalledProcessError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-0c3ec099fef3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'shell'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bash run.sh > run.sh.log.stage.1 2>&1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_shell_cell_magic\u001b[0;34m(args, cmd)\u001b[0m\n\u001b[1;32m    111\u001b[0m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_streamed_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparsed_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_errors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_returncode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36mcheck_returncode\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m       raise subprocess.CalledProcessError(\n\u001b[0;32m--> 139\u001b[0;31m           returncode=self.returncode, cmd=self.args, output=self.output)\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_repr_pretty_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=unused-argument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCalledProcessError\u001b[0m: Command 'bash run.sh > run.sh.log.stage.1 2>&1' died with <Signals.SIGINT: 2>."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "np_GrQQHWorh",
        "outputId": "1be8bdaf-8428-4afd-f266-4bd18d97dfe9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/Colab Notebooks/wenet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "chmod +x ./make_raw_list.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsl3xocAXgoY",
        "outputId": "cb1d9918-7525-411a-9578-24cb7e38b639"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "#-1\n",
        "bash run.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KlKOLvpINAR",
        "outputId": "1559027c-96d0-4fdc-a85c-179cbff0e6a0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stage -1: Data Download\n",
            "local/download_and_untar.sh: removing existing file /content/dataset/aishell/data_aishell.tgz because its size in bytes 4899135883\n",
            "does not equal the size of one of the archives.\n",
            "local/download_and_untar.sh: downloading data from www.openslr.org/resources/33/data_aishell.tgz.  This may take some time, please be patient.\n",
            "--2022-05-23 07:32:51--  http://www.openslr.org/resources/33/data_aishell.tgz\n",
            "Resolving www.openslr.org (www.openslr.org)... 46.101.158.64\n",
            "Connecting to www.openslr.org (www.openslr.org)|46.101.158.64|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://us.openslr.org/resources/33/data_aishell.tgz [following]\n",
            "--2022-05-23 07:32:51--  https://us.openslr.org/resources/33/data_aishell.tgz\n",
            "Resolving us.openslr.org (us.openslr.org)... 46.101.158.64\n",
            "Connecting to us.openslr.org (us.openslr.org)|46.101.158.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15582913665 (15G) [application/x-gzip]\n",
            "Saving to: ‘data_aishell.tgz’\n",
            "\n",
            "data_aishell.tgz    100%[===================>]  14.51G  14.9MB/s    in 17m 53s \n",
            "\n",
            "2022-05-23 07:50:45 (13.9 MB/s) - ‘data_aishell.tgz’ saved [15582913665/15582913665]\n",
            "\n",
            "data_aishell/\n",
            "data_aishell/wav/\n",
            "data_aishell/wav/S0724.tar.gz\n",
            "data_aishell/wav/S0725.tar.gz\n",
            "data_aishell/wav/S0726.tar.gz\n",
            "data_aishell/wav/S0727.tar.gz\n",
            "data_aishell/wav/S0728.tar.gz\n",
            "data_aishell/wav/S0729.tar.gz\n",
            "data_aishell/wav/S0730.tar.gz\n",
            "data_aishell/wav/S0731.tar.gz\n",
            "data_aishell/wav/S0732.tar.gz\n",
            "data_aishell/wav/S0733.tar.gz\n",
            "data_aishell/wav/S0734.tar.gz\n",
            "data_aishell/wav/S0735.tar.gz\n",
            "data_aishell/wav/S0736.tar.gz\n",
            "data_aishell/wav/S0737.tar.gz\n",
            "data_aishell/wav/S0738.tar.gz\n",
            "data_aishell/wav/S0739.tar.gz\n",
            "data_aishell/wav/S0740.tar.gz\n",
            "data_aishell/wav/S0741.tar.gz\n",
            "data_aishell/wav/S0742.tar.gz\n",
            "data_aishell/wav/S0743.tar.gz\n",
            "data_aishell/wav/S0744.tar.gz\n",
            "data_aishell/wav/S0745.tar.gz\n",
            "data_aishell/wav/S0746.tar.gz\n",
            "data_aishell/wav/S0747.tar.gz\n",
            "data_aishell/wav/S0748.tar.gz\n",
            "data_aishell/wav/S0749.tar.gz\n",
            "data_aishell/wav/S0750.tar.gz\n",
            "data_aishell/wav/S0751.tar.gz\n",
            "data_aishell/wav/S0752.tar.gz\n",
            "data_aishell/wav/S0753.tar.gz\n",
            "data_aishell/wav/S0754.tar.gz\n",
            "data_aishell/wav/S0755.tar.gz\n",
            "data_aishell/wav/S0756.tar.gz\n",
            "data_aishell/wav/S0757.tar.gz\n",
            "data_aishell/wav/S0758.tar.gz\n",
            "data_aishell/wav/S0759.tar.gz\n",
            "data_aishell/wav/S0760.tar.gz\n",
            "data_aishell/wav/S0761.tar.gz\n",
            "data_aishell/wav/S0762.tar.gz\n",
            "data_aishell/wav/S0763.tar.gz\n",
            "data_aishell/wav/S0764.tar.gz\n",
            "data_aishell/wav/S0765.tar.gz\n",
            "data_aishell/wav/S0766.tar.gz\n",
            "data_aishell/wav/S0767.tar.gz\n",
            "data_aishell/wav/S0768.tar.gz\n",
            "data_aishell/wav/S0769.tar.gz\n",
            "data_aishell/wav/S0770.tar.gz\n",
            "data_aishell/wav/S0901.tar.gz\n",
            "data_aishell/wav/S0902.tar.gz\n",
            "data_aishell/wav/S0903.tar.gz\n",
            "data_aishell/wav/S0904.tar.gz\n",
            "data_aishell/wav/S0905.tar.gz\n",
            "data_aishell/wav/S0906.tar.gz\n",
            "data_aishell/wav/S0907.tar.gz\n",
            "data_aishell/wav/S0908.tar.gz\n",
            "data_aishell/wav/S0912.tar.gz\n",
            "data_aishell/wav/S0913.tar.gz\n",
            "data_aishell/wav/S0914.tar.gz\n",
            "data_aishell/wav/S0915.tar.gz\n",
            "data_aishell/wav/S0916.tar.gz\n",
            "data_aishell/wav/S0002.tar.gz\n",
            "data_aishell/wav/S0003.tar.gz\n",
            "data_aishell/wav/S0004.tar.gz\n",
            "data_aishell/wav/S0005.tar.gz\n",
            "data_aishell/wav/S0006.tar.gz\n",
            "data_aishell/wav/S0007.tar.gz\n",
            "data_aishell/wav/S0008.tar.gz\n",
            "data_aishell/wav/S0009.tar.gz\n",
            "data_aishell/wav/S0010.tar.gz\n",
            "data_aishell/wav/S0011.tar.gz\n",
            "data_aishell/wav/S0012.tar.gz\n",
            "data_aishell/wav/S0013.tar.gz\n",
            "data_aishell/wav/S0014.tar.gz\n",
            "data_aishell/wav/S0015.tar.gz\n",
            "data_aishell/wav/S0016.tar.gz\n",
            "data_aishell/wav/S0017.tar.gz\n",
            "data_aishell/wav/S0018.tar.gz\n",
            "data_aishell/wav/S0019.tar.gz\n",
            "data_aishell/wav/S0020.tar.gz\n",
            "data_aishell/wav/S0021.tar.gz\n",
            "data_aishell/wav/S0022.tar.gz\n",
            "data_aishell/wav/S0023.tar.gz\n",
            "data_aishell/wav/S0024.tar.gz\n",
            "data_aishell/wav/S0025.tar.gz\n",
            "data_aishell/wav/S0026.tar.gz\n",
            "data_aishell/wav/S0027.tar.gz\n",
            "data_aishell/wav/S0028.tar.gz\n",
            "data_aishell/wav/S0029.tar.gz\n",
            "data_aishell/wav/S0030.tar.gz\n",
            "data_aishell/wav/S0031.tar.gz\n",
            "data_aishell/wav/S0032.tar.gz\n",
            "data_aishell/wav/S0033.tar.gz\n",
            "data_aishell/wav/S0034.tar.gz\n",
            "data_aishell/wav/S0035.tar.gz\n",
            "data_aishell/wav/S0036.tar.gz\n",
            "data_aishell/wav/S0037.tar.gz\n",
            "data_aishell/wav/S0038.tar.gz\n",
            "data_aishell/wav/S0039.tar.gz\n",
            "data_aishell/wav/S0040.tar.gz\n",
            "data_aishell/wav/S0041.tar.gz\n",
            "data_aishell/wav/S0042.tar.gz\n",
            "data_aishell/wav/S0043.tar.gz\n",
            "data_aishell/wav/S0044.tar.gz\n",
            "data_aishell/wav/S0045.tar.gz\n",
            "data_aishell/wav/S0046.tar.gz\n",
            "data_aishell/wav/S0047.tar.gz\n",
            "data_aishell/wav/S0048.tar.gz\n",
            "data_aishell/wav/S0049.tar.gz\n",
            "data_aishell/wav/S0050.tar.gz\n",
            "data_aishell/wav/S0051.tar.gz\n",
            "data_aishell/wav/S0052.tar.gz\n",
            "data_aishell/wav/S0053.tar.gz\n",
            "data_aishell/wav/S0054.tar.gz\n",
            "data_aishell/wav/S0055.tar.gz\n",
            "data_aishell/wav/S0056.tar.gz\n",
            "data_aishell/wav/S0057.tar.gz\n",
            "data_aishell/wav/S0058.tar.gz\n",
            "data_aishell/wav/S0059.tar.gz\n",
            "data_aishell/wav/S0060.tar.gz\n",
            "data_aishell/wav/S0061.tar.gz\n",
            "data_aishell/wav/S0062.tar.gz\n",
            "data_aishell/wav/S0063.tar.gz\n",
            "data_aishell/wav/S0064.tar.gz\n",
            "data_aishell/wav/S0065.tar.gz\n",
            "data_aishell/wav/S0066.tar.gz\n",
            "data_aishell/wav/S0067.tar.gz\n",
            "data_aishell/wav/S0068.tar.gz\n",
            "data_aishell/wav/S0069.tar.gz\n",
            "data_aishell/wav/S0070.tar.gz\n",
            "data_aishell/wav/S0071.tar.gz\n",
            "data_aishell/wav/S0072.tar.gz\n",
            "data_aishell/wav/S0073.tar.gz\n",
            "data_aishell/wav/S0074.tar.gz\n",
            "data_aishell/wav/S0075.tar.gz\n",
            "data_aishell/wav/S0076.tar.gz\n",
            "data_aishell/wav/S0077.tar.gz\n",
            "data_aishell/wav/S0078.tar.gz\n",
            "data_aishell/wav/S0079.tar.gz\n",
            "data_aishell/wav/S0080.tar.gz\n",
            "data_aishell/wav/S0081.tar.gz\n",
            "data_aishell/wav/S0082.tar.gz\n",
            "data_aishell/wav/S0083.tar.gz\n",
            "data_aishell/wav/S0084.tar.gz\n",
            "data_aishell/wav/S0085.tar.gz\n",
            "data_aishell/wav/S0086.tar.gz\n",
            "data_aishell/wav/S0087.tar.gz\n",
            "data_aishell/wav/S0088.tar.gz\n",
            "data_aishell/wav/S0089.tar.gz\n",
            "data_aishell/wav/S0090.tar.gz\n",
            "data_aishell/wav/S0091.tar.gz\n",
            "data_aishell/wav/S0092.tar.gz\n",
            "data_aishell/wav/S0093.tar.gz\n",
            "data_aishell/wav/S0094.tar.gz\n",
            "data_aishell/wav/S0095.tar.gz\n",
            "data_aishell/wav/S0096.tar.gz\n",
            "data_aishell/wav/S0097.tar.gz\n",
            "data_aishell/wav/S0098.tar.gz\n",
            "data_aishell/wav/S0099.tar.gz\n",
            "data_aishell/wav/S0100.tar.gz\n",
            "data_aishell/wav/S0101.tar.gz\n",
            "data_aishell/wav/S0102.tar.gz\n",
            "data_aishell/wav/S0103.tar.gz\n",
            "data_aishell/wav/S0104.tar.gz\n",
            "data_aishell/wav/S0105.tar.gz\n",
            "data_aishell/wav/S0106.tar.gz\n",
            "data_aishell/wav/S0107.tar.gz\n",
            "data_aishell/wav/S0108.tar.gz\n",
            "data_aishell/wav/S0109.tar.gz\n",
            "data_aishell/wav/S0110.tar.gz\n",
            "data_aishell/wav/S0111.tar.gz\n",
            "data_aishell/wav/S0112.tar.gz\n",
            "data_aishell/wav/S0113.tar.gz\n",
            "data_aishell/wav/S0114.tar.gz\n",
            "data_aishell/wav/S0115.tar.gz\n",
            "data_aishell/wav/S0116.tar.gz\n",
            "data_aishell/wav/S0117.tar.gz\n",
            "data_aishell/wav/S0118.tar.gz\n",
            "data_aishell/wav/S0119.tar.gz\n",
            "data_aishell/wav/S0120.tar.gz\n",
            "data_aishell/wav/S0121.tar.gz\n",
            "data_aishell/wav/S0122.tar.gz\n",
            "data_aishell/wav/S0123.tar.gz\n",
            "data_aishell/wav/S0124.tar.gz\n",
            "data_aishell/wav/S0125.tar.gz\n",
            "data_aishell/wav/S0126.tar.gz\n",
            "data_aishell/wav/S0127.tar.gz\n",
            "data_aishell/wav/S0128.tar.gz\n",
            "data_aishell/wav/S0129.tar.gz\n",
            "data_aishell/wav/S0130.tar.gz\n",
            "data_aishell/wav/S0131.tar.gz\n",
            "data_aishell/wav/S0132.tar.gz\n",
            "data_aishell/wav/S0133.tar.gz\n",
            "data_aishell/wav/S0134.tar.gz\n",
            "data_aishell/wav/S0135.tar.gz\n",
            "data_aishell/wav/S0136.tar.gz\n",
            "data_aishell/wav/S0137.tar.gz\n",
            "data_aishell/wav/S0138.tar.gz\n",
            "data_aishell/wav/S0139.tar.gz\n",
            "data_aishell/wav/S0140.tar.gz\n",
            "data_aishell/wav/S0141.tar.gz\n",
            "data_aishell/wav/S0142.tar.gz\n",
            "data_aishell/wav/S0143.tar.gz\n",
            "data_aishell/wav/S0144.tar.gz\n",
            "data_aishell/wav/S0145.tar.gz\n",
            "data_aishell/wav/S0146.tar.gz\n",
            "data_aishell/wav/S0147.tar.gz\n",
            "data_aishell/wav/S0148.tar.gz\n",
            "data_aishell/wav/S0149.tar.gz\n",
            "data_aishell/wav/S0150.tar.gz\n",
            "data_aishell/wav/S0151.tar.gz\n",
            "data_aishell/wav/S0152.tar.gz\n",
            "data_aishell/wav/S0153.tar.gz\n",
            "data_aishell/wav/S0154.tar.gz\n",
            "data_aishell/wav/S0155.tar.gz\n",
            "data_aishell/wav/S0156.tar.gz\n",
            "data_aishell/wav/S0157.tar.gz\n",
            "data_aishell/wav/S0158.tar.gz\n",
            "data_aishell/wav/S0159.tar.gz\n",
            "data_aishell/wav/S0160.tar.gz\n",
            "data_aishell/wav/S0161.tar.gz\n",
            "data_aishell/wav/S0162.tar.gz\n",
            "data_aishell/wav/S0163.tar.gz\n",
            "data_aishell/wav/S0164.tar.gz\n",
            "data_aishell/wav/S0165.tar.gz\n",
            "data_aishell/wav/S0166.tar.gz\n",
            "data_aishell/wav/S0167.tar.gz\n",
            "data_aishell/wav/S0168.tar.gz\n",
            "data_aishell/wav/S0169.tar.gz\n",
            "data_aishell/wav/S0170.tar.gz\n",
            "data_aishell/wav/S0171.tar.gz\n",
            "data_aishell/wav/S0172.tar.gz\n",
            "data_aishell/wav/S0173.tar.gz\n",
            "data_aishell/wav/S0174.tar.gz\n",
            "data_aishell/wav/S0175.tar.gz\n",
            "data_aishell/wav/S0176.tar.gz\n",
            "data_aishell/wav/S0177.tar.gz\n",
            "data_aishell/wav/S0178.tar.gz\n",
            "data_aishell/wav/S0179.tar.gz\n",
            "data_aishell/wav/S0180.tar.gz\n",
            "data_aishell/wav/S0181.tar.gz\n",
            "data_aishell/wav/S0182.tar.gz\n",
            "data_aishell/wav/S0183.tar.gz\n",
            "data_aishell/wav/S0184.tar.gz\n",
            "data_aishell/wav/S0185.tar.gz\n",
            "data_aishell/wav/S0186.tar.gz\n",
            "data_aishell/wav/S0187.tar.gz\n",
            "data_aishell/wav/S0188.tar.gz\n",
            "data_aishell/wav/S0189.tar.gz\n",
            "data_aishell/wav/S0190.tar.gz\n",
            "data_aishell/wav/S0191.tar.gz\n",
            "data_aishell/wav/S0192.tar.gz\n",
            "data_aishell/wav/S0193.tar.gz\n",
            "data_aishell/wav/S0194.tar.gz\n",
            "data_aishell/wav/S0195.tar.gz\n",
            "data_aishell/wav/S0196.tar.gz\n",
            "data_aishell/wav/S0197.tar.gz\n",
            "data_aishell/wav/S0198.tar.gz\n",
            "data_aishell/wav/S0199.tar.gz\n",
            "data_aishell/wav/S0200.tar.gz\n",
            "data_aishell/wav/S0201.tar.gz\n",
            "data_aishell/wav/S0202.tar.gz\n",
            "data_aishell/wav/S0203.tar.gz\n",
            "data_aishell/wav/S0204.tar.gz\n",
            "data_aishell/wav/S0205.tar.gz\n",
            "data_aishell/wav/S0206.tar.gz\n",
            "data_aishell/wav/S0207.tar.gz\n",
            "data_aishell/wav/S0208.tar.gz\n",
            "data_aishell/wav/S0209.tar.gz\n",
            "data_aishell/wav/S0210.tar.gz\n",
            "data_aishell/wav/S0211.tar.gz\n",
            "data_aishell/wav/S0212.tar.gz\n",
            "data_aishell/wav/S0213.tar.gz\n",
            "data_aishell/wav/S0214.tar.gz\n",
            "data_aishell/wav/S0215.tar.gz\n",
            "data_aishell/wav/S0216.tar.gz\n",
            "data_aishell/wav/S0217.tar.gz\n",
            "data_aishell/wav/S0218.tar.gz\n",
            "data_aishell/wav/S0219.tar.gz\n",
            "data_aishell/wav/S0220.tar.gz\n",
            "data_aishell/wav/S0221.tar.gz\n",
            "data_aishell/wav/S0222.tar.gz\n",
            "data_aishell/wav/S0223.tar.gz\n",
            "data_aishell/wav/S0224.tar.gz\n",
            "data_aishell/wav/S0225.tar.gz\n",
            "data_aishell/wav/S0226.tar.gz\n",
            "data_aishell/wav/S0227.tar.gz\n",
            "data_aishell/wav/S0228.tar.gz\n",
            "data_aishell/wav/S0229.tar.gz\n",
            "data_aishell/wav/S0230.tar.gz\n",
            "data_aishell/wav/S0231.tar.gz\n",
            "data_aishell/wav/S0232.tar.gz\n",
            "data_aishell/wav/S0233.tar.gz\n",
            "data_aishell/wav/S0234.tar.gz\n",
            "data_aishell/wav/S0235.tar.gz\n",
            "data_aishell/wav/S0236.tar.gz\n",
            "data_aishell/wav/S0237.tar.gz\n",
            "data_aishell/wav/S0238.tar.gz\n",
            "data_aishell/wav/S0239.tar.gz\n",
            "data_aishell/wav/S0240.tar.gz\n",
            "data_aishell/wav/S0241.tar.gz\n",
            "data_aishell/wav/S0242.tar.gz\n",
            "data_aishell/wav/S0243.tar.gz\n",
            "data_aishell/wav/S0244.tar.gz\n",
            "data_aishell/wav/S0245.tar.gz\n",
            "data_aishell/wav/S0246.tar.gz\n",
            "data_aishell/wav/S0247.tar.gz\n",
            "data_aishell/wav/S0248.tar.gz\n",
            "data_aishell/wav/S0249.tar.gz\n",
            "data_aishell/wav/S0250.tar.gz\n",
            "data_aishell/wav/S0251.tar.gz\n",
            "data_aishell/wav/S0252.tar.gz\n",
            "data_aishell/wav/S0334.tar.gz\n",
            "data_aishell/wav/S0335.tar.gz\n",
            "data_aishell/wav/S0336.tar.gz\n",
            "data_aishell/wav/S0337.tar.gz\n",
            "data_aishell/wav/S0338.tar.gz\n",
            "data_aishell/wav/S0339.tar.gz\n",
            "data_aishell/wav/S0340.tar.gz\n",
            "data_aishell/wav/S0341.tar.gz\n",
            "data_aishell/wav/S0342.tar.gz\n",
            "data_aishell/wav/S0343.tar.gz\n",
            "data_aishell/wav/S0344.tar.gz\n",
            "data_aishell/wav/S0345.tar.gz\n",
            "data_aishell/wav/S0346.tar.gz\n",
            "data_aishell/wav/S0347.tar.gz\n",
            "data_aishell/wav/S0348.tar.gz\n",
            "data_aishell/wav/S0349.tar.gz\n",
            "data_aishell/wav/S0350.tar.gz\n",
            "data_aishell/wav/S0351.tar.gz\n",
            "data_aishell/wav/S0352.tar.gz\n",
            "data_aishell/wav/S0353.tar.gz\n",
            "data_aishell/wav/S0354.tar.gz\n",
            "data_aishell/wav/S0355.tar.gz\n",
            "data_aishell/wav/S0356.tar.gz\n",
            "data_aishell/wav/S0357.tar.gz\n",
            "data_aishell/wav/S0358.tar.gz\n",
            "data_aishell/wav/S0359.tar.gz\n",
            "data_aishell/wav/S0360.tar.gz\n",
            "data_aishell/wav/S0361.tar.gz\n",
            "data_aishell/wav/S0362.tar.gz\n",
            "data_aishell/wav/S0363.tar.gz\n",
            "data_aishell/wav/S0421.tar.gz\n",
            "data_aishell/wav/S0422.tar.gz\n",
            "data_aishell/wav/S0423.tar.gz\n",
            "data_aishell/wav/S0509.tar.gz\n",
            "data_aishell/wav/S0510.tar.gz\n",
            "data_aishell/wav/S0511.tar.gz\n",
            "data_aishell/wav/S0512.tar.gz\n",
            "data_aishell/wav/S0513.tar.gz\n",
            "data_aishell/wav/S0514.tar.gz\n",
            "data_aishell/wav/S0515.tar.gz\n",
            "data_aishell/wav/S0516.tar.gz\n",
            "data_aishell/wav/S0517.tar.gz\n",
            "data_aishell/wav/S0518.tar.gz\n",
            "data_aishell/wav/S0519.tar.gz\n",
            "data_aishell/wav/S0520.tar.gz\n",
            "data_aishell/wav/S0521.tar.gz\n",
            "data_aishell/wav/S0522.tar.gz\n",
            "data_aishell/wav/S0523.tar.gz\n",
            "data_aishell/wav/S0596.tar.gz\n",
            "data_aishell/wav/S0597.tar.gz\n",
            "data_aishell/wav/S0598.tar.gz\n",
            "data_aishell/wav/S0599.tar.gz\n",
            "data_aishell/wav/S0600.tar.gz\n",
            "data_aishell/wav/S0601.tar.gz\n",
            "data_aishell/wav/S0655.tar.gz\n",
            "data_aishell/wav/S0656.tar.gz\n",
            "data_aishell/wav/S0657.tar.gz\n",
            "data_aishell/wav/S0658.tar.gz\n",
            "data_aishell/wav/S0659.tar.gz\n",
            "data_aishell/wav/S0660.tar.gz\n",
            "data_aishell/wav/S0661.tar.gz\n",
            "data_aishell/wav/S0662.tar.gz\n",
            "data_aishell/wav/S0663.tar.gz\n",
            "data_aishell/wav/S0664.tar.gz\n",
            "data_aishell/wav/S0665.tar.gz\n",
            "data_aishell/wav/S0666.tar.gz\n",
            "data_aishell/wav/S0701.tar.gz\n",
            "data_aishell/wav/S0702.tar.gz\n",
            "data_aishell/wav/S0703.tar.gz\n",
            "data_aishell/wav/S0704.tar.gz\n",
            "data_aishell/wav/S0705.tar.gz\n",
            "data_aishell/wav/S0706.tar.gz\n",
            "data_aishell/wav/S0707.tar.gz\n",
            "data_aishell/wav/S0708.tar.gz\n",
            "data_aishell/wav/S0709.tar.gz\n",
            "data_aishell/wav/S0710.tar.gz\n",
            "data_aishell/wav/S0711.tar.gz\n",
            "data_aishell/wav/S0712.tar.gz\n",
            "data_aishell/wav/S0713.tar.gz\n",
            "data_aishell/wav/S0714.tar.gz\n",
            "data_aishell/wav/S0715.tar.gz\n",
            "data_aishell/wav/S0716.tar.gz\n",
            "data_aishell/wav/S0717.tar.gz\n",
            "data_aishell/wav/S0718.tar.gz\n",
            "data_aishell/wav/S0719.tar.gz\n",
            "data_aishell/wav/S0720.tar.gz\n",
            "data_aishell/wav/S0721.tar.gz\n",
            "data_aishell/wav/S0722.tar.gz\n",
            "data_aishell/wav/S0723.tar.gz\n",
            "data_aishell/transcript/\n",
            "data_aishell/transcript/aishell_transcript_v0.8.txt\n",
            "Extracting wav from ./S0002.tar.gz\n",
            "Extracting wav from ./S0003.tar.gz\n",
            "Extracting wav from ./S0004.tar.gz\n",
            "Extracting wav from ./S0005.tar.gz\n",
            "Extracting wav from ./S0006.tar.gz\n",
            "Extracting wav from ./S0007.tar.gz\n",
            "Extracting wav from ./S0008.tar.gz\n",
            "Extracting wav from ./S0009.tar.gz\n",
            "Extracting wav from ./S0010.tar.gz\n",
            "Extracting wav from ./S0011.tar.gz\n",
            "Extracting wav from ./S0012.tar.gz\n",
            "Extracting wav from ./S0013.tar.gz\n",
            "Extracting wav from ./S0014.tar.gz\n",
            "Extracting wav from ./S0015.tar.gz\n",
            "Extracting wav from ./S0016.tar.gz\n",
            "Extracting wav from ./S0017.tar.gz\n",
            "Extracting wav from ./S0018.tar.gz\n",
            "Extracting wav from ./S0019.tar.gz\n",
            "Extracting wav from ./S0020.tar.gz\n",
            "Extracting wav from ./S0021.tar.gz\n",
            "Extracting wav from ./S0022.tar.gz\n",
            "Extracting wav from ./S0023.tar.gz\n",
            "Extracting wav from ./S0024.tar.gz\n",
            "Extracting wav from ./S0025.tar.gz\n",
            "Extracting wav from ./S0026.tar.gz\n",
            "Extracting wav from ./S0027.tar.gz\n",
            "Extracting wav from ./S0028.tar.gz\n",
            "Extracting wav from ./S0029.tar.gz\n",
            "Extracting wav from ./S0030.tar.gz\n",
            "Extracting wav from ./S0031.tar.gz\n",
            "Extracting wav from ./S0032.tar.gz\n",
            "Extracting wav from ./S0033.tar.gz\n",
            "Extracting wav from ./S0034.tar.gz\n",
            "Extracting wav from ./S0035.tar.gz\n",
            "Extracting wav from ./S0036.tar.gz\n",
            "Extracting wav from ./S0037.tar.gz\n",
            "Extracting wav from ./S0038.tar.gz\n",
            "Extracting wav from ./S0039.tar.gz\n",
            "Extracting wav from ./S0040.tar.gz\n",
            "Extracting wav from ./S0041.tar.gz\n",
            "Extracting wav from ./S0042.tar.gz\n",
            "Extracting wav from ./S0043.tar.gz\n",
            "Extracting wav from ./S0044.tar.gz\n",
            "Extracting wav from ./S0045.tar.gz\n",
            "Extracting wav from ./S0046.tar.gz\n",
            "Extracting wav from ./S0047.tar.gz\n",
            "Extracting wav from ./S0048.tar.gz\n",
            "Extracting wav from ./S0049.tar.gz\n",
            "Extracting wav from ./S0050.tar.gz\n",
            "Extracting wav from ./S0051.tar.gz\n",
            "Extracting wav from ./S0052.tar.gz\n",
            "Extracting wav from ./S0053.tar.gz\n",
            "Extracting wav from ./S0054.tar.gz\n",
            "Extracting wav from ./S0055.tar.gz\n",
            "Extracting wav from ./S0056.tar.gz\n",
            "Extracting wav from ./S0057.tar.gz\n",
            "Extracting wav from ./S0058.tar.gz\n",
            "Extracting wav from ./S0059.tar.gz\n",
            "Extracting wav from ./S0060.tar.gz\n",
            "Extracting wav from ./S0061.tar.gz\n",
            "Extracting wav from ./S0062.tar.gz\n",
            "Extracting wav from ./S0063.tar.gz\n",
            "Extracting wav from ./S0064.tar.gz\n",
            "Extracting wav from ./S0065.tar.gz\n",
            "Extracting wav from ./S0066.tar.gz\n",
            "Extracting wav from ./S0067.tar.gz\n",
            "Extracting wav from ./S0068.tar.gz\n",
            "Extracting wav from ./S0069.tar.gz\n",
            "Extracting wav from ./S0070.tar.gz\n",
            "Extracting wav from ./S0071.tar.gz\n",
            "Extracting wav from ./S0072.tar.gz\n",
            "Extracting wav from ./S0073.tar.gz\n",
            "Extracting wav from ./S0074.tar.gz\n",
            "Extracting wav from ./S0075.tar.gz\n",
            "Extracting wav from ./S0076.tar.gz\n",
            "Extracting wav from ./S0077.tar.gz\n",
            "Extracting wav from ./S0078.tar.gz\n",
            "Extracting wav from ./S0079.tar.gz\n",
            "Extracting wav from ./S0080.tar.gz\n",
            "Extracting wav from ./S0081.tar.gz\n",
            "Extracting wav from ./S0082.tar.gz\n",
            "Extracting wav from ./S0083.tar.gz\n",
            "Extracting wav from ./S0084.tar.gz\n",
            "Extracting wav from ./S0085.tar.gz\n",
            "Extracting wav from ./S0086.tar.gz\n",
            "Extracting wav from ./S0087.tar.gz\n",
            "Extracting wav from ./S0088.tar.gz\n",
            "Extracting wav from ./S0089.tar.gz\n",
            "Extracting wav from ./S0090.tar.gz\n",
            "Extracting wav from ./S0091.tar.gz\n",
            "Extracting wav from ./S0092.tar.gz\n",
            "Extracting wav from ./S0093.tar.gz\n",
            "Extracting wav from ./S0094.tar.gz\n",
            "Extracting wav from ./S0095.tar.gz\n",
            "Extracting wav from ./S0096.tar.gz\n",
            "Extracting wav from ./S0097.tar.gz\n",
            "Extracting wav from ./S0098.tar.gz\n",
            "Extracting wav from ./S0099.tar.gz\n",
            "Extracting wav from ./S0100.tar.gz\n",
            "Extracting wav from ./S0101.tar.gz\n",
            "Extracting wav from ./S0102.tar.gz\n",
            "Extracting wav from ./S0103.tar.gz\n",
            "Extracting wav from ./S0104.tar.gz\n",
            "Extracting wav from ./S0105.tar.gz\n",
            "Extracting wav from ./S0106.tar.gz\n",
            "Extracting wav from ./S0107.tar.gz\n",
            "Extracting wav from ./S0108.tar.gz\n",
            "Extracting wav from ./S0109.tar.gz\n",
            "Extracting wav from ./S0110.tar.gz\n",
            "Extracting wav from ./S0111.tar.gz\n",
            "Extracting wav from ./S0112.tar.gz\n",
            "Extracting wav from ./S0113.tar.gz\n",
            "Extracting wav from ./S0114.tar.gz\n",
            "Extracting wav from ./S0115.tar.gz\n",
            "Extracting wav from ./S0116.tar.gz\n",
            "Extracting wav from ./S0117.tar.gz\n",
            "Extracting wav from ./S0118.tar.gz\n",
            "Extracting wav from ./S0119.tar.gz\n",
            "Extracting wav from ./S0120.tar.gz\n",
            "Extracting wav from ./S0121.tar.gz\n",
            "Extracting wav from ./S0122.tar.gz\n",
            "Extracting wav from ./S0123.tar.gz\n",
            "Extracting wav from ./S0124.tar.gz\n",
            "Extracting wav from ./S0125.tar.gz\n",
            "Extracting wav from ./S0126.tar.gz\n",
            "Extracting wav from ./S0127.tar.gz\n",
            "Extracting wav from ./S0128.tar.gz\n",
            "Extracting wav from ./S0129.tar.gz\n",
            "Extracting wav from ./S0130.tar.gz\n",
            "Extracting wav from ./S0131.tar.gz\n",
            "Extracting wav from ./S0132.tar.gz\n",
            "Extracting wav from ./S0133.tar.gz\n",
            "Extracting wav from ./S0134.tar.gz\n",
            "Extracting wav from ./S0135.tar.gz\n",
            "Extracting wav from ./S0136.tar.gz\n",
            "Extracting wav from ./S0137.tar.gz\n",
            "Extracting wav from ./S0138.tar.gz\n",
            "Extracting wav from ./S0139.tar.gz\n",
            "Extracting wav from ./S0140.tar.gz\n",
            "Extracting wav from ./S0141.tar.gz\n",
            "Extracting wav from ./S0142.tar.gz\n",
            "Extracting wav from ./S0143.tar.gz\n",
            "Extracting wav from ./S0144.tar.gz\n",
            "Extracting wav from ./S0145.tar.gz\n",
            "Extracting wav from ./S0146.tar.gz\n",
            "Extracting wav from ./S0147.tar.gz\n",
            "Extracting wav from ./S0148.tar.gz\n",
            "Extracting wav from ./S0149.tar.gz\n",
            "Extracting wav from ./S0150.tar.gz\n",
            "Extracting wav from ./S0151.tar.gz\n",
            "Extracting wav from ./S0152.tar.gz\n",
            "Extracting wav from ./S0153.tar.gz\n",
            "Extracting wav from ./S0154.tar.gz\n",
            "Extracting wav from ./S0155.tar.gz\n",
            "Extracting wav from ./S0156.tar.gz\n",
            "Extracting wav from ./S0157.tar.gz\n",
            "Extracting wav from ./S0158.tar.gz\n",
            "Extracting wav from ./S0159.tar.gz\n",
            "Extracting wav from ./S0160.tar.gz\n",
            "Extracting wav from ./S0161.tar.gz\n",
            "Extracting wav from ./S0162.tar.gz\n",
            "Extracting wav from ./S0163.tar.gz\n",
            "Extracting wav from ./S0164.tar.gz\n",
            "Extracting wav from ./S0165.tar.gz\n",
            "Extracting wav from ./S0166.tar.gz\n",
            "Extracting wav from ./S0167.tar.gz\n",
            "Extracting wav from ./S0168.tar.gz\n",
            "Extracting wav from ./S0169.tar.gz\n",
            "Extracting wav from ./S0170.tar.gz\n",
            "Extracting wav from ./S0171.tar.gz\n",
            "Extracting wav from ./S0172.tar.gz\n",
            "Extracting wav from ./S0173.tar.gz\n",
            "Extracting wav from ./S0174.tar.gz\n",
            "Extracting wav from ./S0175.tar.gz\n",
            "Extracting wav from ./S0176.tar.gz\n",
            "Extracting wav from ./S0177.tar.gz\n",
            "Extracting wav from ./S0178.tar.gz\n",
            "Extracting wav from ./S0179.tar.gz\n",
            "Extracting wav from ./S0180.tar.gz\n",
            "Extracting wav from ./S0181.tar.gz\n",
            "Extracting wav from ./S0182.tar.gz\n",
            "Extracting wav from ./S0183.tar.gz\n",
            "Extracting wav from ./S0184.tar.gz\n",
            "Extracting wav from ./S0185.tar.gz\n",
            "Extracting wav from ./S0186.tar.gz\n",
            "Extracting wav from ./S0187.tar.gz\n",
            "Extracting wav from ./S0188.tar.gz\n",
            "Extracting wav from ./S0189.tar.gz\n",
            "Extracting wav from ./S0190.tar.gz\n",
            "Extracting wav from ./S0191.tar.gz\n",
            "Extracting wav from ./S0192.tar.gz\n",
            "Extracting wav from ./S0193.tar.gz\n",
            "Extracting wav from ./S0194.tar.gz\n",
            "Extracting wav from ./S0195.tar.gz\n",
            "Extracting wav from ./S0196.tar.gz\n",
            "Extracting wav from ./S0197.tar.gz\n",
            "Extracting wav from ./S0198.tar.gz\n",
            "Extracting wav from ./S0199.tar.gz\n",
            "Extracting wav from ./S0200.tar.gz\n",
            "Extracting wav from ./S0201.tar.gz\n",
            "Extracting wav from ./S0202.tar.gz\n",
            "Extracting wav from ./S0203.tar.gz\n",
            "Extracting wav from ./S0204.tar.gz\n",
            "Extracting wav from ./S0205.tar.gz\n",
            "Extracting wav from ./S0206.tar.gz\n",
            "Extracting wav from ./S0207.tar.gz\n",
            "Extracting wav from ./S0208.tar.gz\n",
            "Extracting wav from ./S0209.tar.gz\n",
            "Extracting wav from ./S0210.tar.gz\n",
            "Extracting wav from ./S0211.tar.gz\n",
            "Extracting wav from ./S0212.tar.gz\n",
            "Extracting wav from ./S0213.tar.gz\n",
            "Extracting wav from ./S0214.tar.gz\n",
            "Extracting wav from ./S0215.tar.gz\n",
            "Extracting wav from ./S0216.tar.gz\n",
            "Extracting wav from ./S0217.tar.gz\n",
            "Extracting wav from ./S0218.tar.gz\n",
            "Extracting wav from ./S0219.tar.gz\n",
            "Extracting wav from ./S0220.tar.gz\n",
            "Extracting wav from ./S0221.tar.gz\n",
            "Extracting wav from ./S0222.tar.gz\n",
            "Extracting wav from ./S0223.tar.gz\n",
            "Extracting wav from ./S0224.tar.gz\n",
            "Extracting wav from ./S0225.tar.gz\n",
            "Extracting wav from ./S0226.tar.gz\n",
            "Extracting wav from ./S0227.tar.gz\n",
            "Extracting wav from ./S0228.tar.gz\n",
            "Extracting wav from ./S0229.tar.gz\n",
            "Extracting wav from ./S0230.tar.gz\n",
            "Extracting wav from ./S0231.tar.gz\n",
            "Extracting wav from ./S0232.tar.gz\n",
            "Extracting wav from ./S0233.tar.gz\n",
            "Extracting wav from ./S0234.tar.gz\n",
            "Extracting wav from ./S0235.tar.gz\n",
            "Extracting wav from ./S0236.tar.gz\n",
            "Extracting wav from ./S0237.tar.gz\n",
            "Extracting wav from ./S0238.tar.gz\n",
            "Extracting wav from ./S0239.tar.gz\n",
            "Extracting wav from ./S0240.tar.gz\n",
            "Extracting wav from ./S0241.tar.gz\n",
            "Extracting wav from ./S0242.tar.gz\n",
            "Extracting wav from ./S0243.tar.gz\n",
            "Extracting wav from ./S0244.tar.gz\n",
            "Extracting wav from ./S0245.tar.gz\n",
            "Extracting wav from ./S0246.tar.gz\n",
            "Extracting wav from ./S0247.tar.gz\n",
            "Extracting wav from ./S0248.tar.gz\n",
            "Extracting wav from ./S0249.tar.gz\n",
            "Extracting wav from ./S0250.tar.gz\n",
            "Extracting wav from ./S0251.tar.gz\n",
            "Extracting wav from ./S0252.tar.gz\n",
            "Extracting wav from ./S0334.tar.gz\n",
            "Extracting wav from ./S0335.tar.gz\n",
            "Extracting wav from ./S0336.tar.gz\n",
            "Extracting wav from ./S0337.tar.gz\n",
            "Extracting wav from ./S0338.tar.gz\n",
            "Extracting wav from ./S0339.tar.gz\n",
            "Extracting wav from ./S0340.tar.gz\n",
            "Extracting wav from ./S0341.tar.gz\n",
            "Extracting wav from ./S0342.tar.gz\n",
            "Extracting wav from ./S0343.tar.gz\n",
            "Extracting wav from ./S0344.tar.gz\n",
            "Extracting wav from ./S0345.tar.gz\n",
            "Extracting wav from ./S0346.tar.gz\n",
            "Extracting wav from ./S0347.tar.gz\n",
            "Extracting wav from ./S0348.tar.gz\n",
            "Extracting wav from ./S0349.tar.gz\n",
            "Extracting wav from ./S0350.tar.gz\n",
            "Extracting wav from ./S0351.tar.gz\n",
            "Extracting wav from ./S0352.tar.gz\n",
            "Extracting wav from ./S0353.tar.gz\n",
            "Extracting wav from ./S0354.tar.gz\n",
            "Extracting wav from ./S0355.tar.gz\n",
            "Extracting wav from ./S0356.tar.gz\n",
            "Extracting wav from ./S0357.tar.gz\n",
            "Extracting wav from ./S0358.tar.gz\n",
            "Extracting wav from ./S0359.tar.gz\n",
            "Extracting wav from ./S0360.tar.gz\n",
            "Extracting wav from ./S0361.tar.gz\n",
            "Extracting wav from ./S0362.tar.gz\n",
            "Extracting wav from ./S0363.tar.gz\n",
            "Extracting wav from ./S0421.tar.gz\n",
            "Extracting wav from ./S0422.tar.gz\n",
            "Extracting wav from ./S0423.tar.gz\n",
            "Extracting wav from ./S0509.tar.gz\n",
            "Extracting wav from ./S0510.tar.gz\n",
            "Extracting wav from ./S0511.tar.gz\n",
            "Extracting wav from ./S0512.tar.gz\n",
            "Extracting wav from ./S0513.tar.gz\n",
            "Extracting wav from ./S0514.tar.gz\n",
            "Extracting wav from ./S0515.tar.gz\n",
            "Extracting wav from ./S0516.tar.gz\n",
            "Extracting wav from ./S0517.tar.gz\n",
            "Extracting wav from ./S0518.tar.gz\n",
            "Extracting wav from ./S0519.tar.gz\n",
            "Extracting wav from ./S0520.tar.gz\n",
            "Extracting wav from ./S0521.tar.gz\n",
            "Extracting wav from ./S0522.tar.gz\n",
            "Extracting wav from ./S0523.tar.gz\n",
            "Extracting wav from ./S0596.tar.gz\n",
            "Extracting wav from ./S0597.tar.gz\n",
            "Extracting wav from ./S0598.tar.gz\n",
            "Extracting wav from ./S0599.tar.gz\n",
            "Extracting wav from ./S0600.tar.gz\n",
            "Extracting wav from ./S0601.tar.gz\n",
            "Extracting wav from ./S0655.tar.gz\n",
            "Extracting wav from ./S0656.tar.gz\n",
            "Extracting wav from ./S0657.tar.gz\n",
            "Extracting wav from ./S0658.tar.gz\n",
            "Extracting wav from ./S0659.tar.gz\n",
            "Extracting wav from ./S0660.tar.gz\n",
            "Extracting wav from ./S0661.tar.gz\n",
            "Extracting wav from ./S0662.tar.gz\n",
            "Extracting wav from ./S0663.tar.gz\n",
            "Extracting wav from ./S0664.tar.gz\n",
            "Extracting wav from ./S0665.tar.gz\n",
            "Extracting wav from ./S0666.tar.gz\n",
            "Extracting wav from ./S0701.tar.gz\n",
            "Extracting wav from ./S0702.tar.gz\n",
            "Extracting wav from ./S0703.tar.gz\n",
            "Extracting wav from ./S0704.tar.gz\n",
            "Extracting wav from ./S0705.tar.gz\n",
            "Extracting wav from ./S0706.tar.gz\n",
            "Extracting wav from ./S0707.tar.gz\n",
            "Extracting wav from ./S0708.tar.gz\n",
            "Extracting wav from ./S0709.tar.gz\n",
            "Extracting wav from ./S0710.tar.gz\n",
            "Extracting wav from ./S0711.tar.gz\n",
            "Extracting wav from ./S0712.tar.gz\n",
            "Extracting wav from ./S0713.tar.gz\n",
            "Extracting wav from ./S0714.tar.gz\n",
            "Extracting wav from ./S0715.tar.gz\n",
            "Extracting wav from ./S0716.tar.gz\n",
            "Extracting wav from ./S0717.tar.gz\n",
            "Extracting wav from ./S0718.tar.gz\n",
            "Extracting wav from ./S0719.tar.gz\n",
            "Extracting wav from ./S0720.tar.gz\n",
            "Extracting wav from ./S0721.tar.gz\n",
            "Extracting wav from ./S0722.tar.gz\n",
            "Extracting wav from ./S0723.tar.gz\n",
            "Extracting wav from ./S0724.tar.gz\n",
            "Extracting wav from ./S0725.tar.gz\n",
            "Extracting wav from ./S0726.tar.gz\n",
            "Extracting wav from ./S0727.tar.gz\n",
            "Extracting wav from ./S0728.tar.gz\n",
            "Extracting wav from ./S0729.tar.gz\n",
            "Extracting wav from ./S0730.tar.gz\n",
            "Extracting wav from ./S0731.tar.gz\n",
            "Extracting wav from ./S0732.tar.gz\n",
            "Extracting wav from ./S0733.tar.gz\n",
            "Extracting wav from ./S0734.tar.gz\n",
            "Extracting wav from ./S0735.tar.gz\n",
            "Extracting wav from ./S0736.tar.gz\n",
            "Extracting wav from ./S0737.tar.gz\n",
            "Extracting wav from ./S0738.tar.gz\n",
            "Extracting wav from ./S0739.tar.gz\n",
            "Extracting wav from ./S0740.tar.gz\n",
            "Extracting wav from ./S0741.tar.gz\n",
            "Extracting wav from ./S0742.tar.gz\n",
            "Extracting wav from ./S0743.tar.gz\n",
            "Extracting wav from ./S0744.tar.gz\n",
            "Extracting wav from ./S0745.tar.gz\n",
            "Extracting wav from ./S0746.tar.gz\n",
            "Extracting wav from ./S0747.tar.gz\n",
            "Extracting wav from ./S0748.tar.gz\n",
            "Extracting wav from ./S0749.tar.gz\n",
            "Extracting wav from ./S0750.tar.gz\n",
            "Extracting wav from ./S0751.tar.gz\n",
            "Extracting wav from ./S0752.tar.gz\n",
            "Extracting wav from ./S0753.tar.gz\n",
            "Extracting wav from ./S0754.tar.gz\n",
            "Extracting wav from ./S0755.tar.gz\n",
            "Extracting wav from ./S0756.tar.gz\n",
            "Extracting wav from ./S0757.tar.gz\n",
            "Extracting wav from ./S0758.tar.gz\n",
            "Extracting wav from ./S0759.tar.gz\n",
            "Extracting wav from ./S0760.tar.gz\n",
            "Extracting wav from ./S0761.tar.gz\n",
            "Extracting wav from ./S0762.tar.gz\n",
            "Extracting wav from ./S0763.tar.gz\n",
            "Extracting wav from ./S0764.tar.gz\n",
            "Extracting wav from ./S0765.tar.gz\n",
            "Extracting wav from ./S0766.tar.gz\n",
            "Extracting wav from ./S0767.tar.gz\n",
            "Extracting wav from ./S0768.tar.gz\n",
            "Extracting wav from ./S0769.tar.gz\n",
            "Extracting wav from ./S0770.tar.gz\n",
            "Extracting wav from ./S0901.tar.gz\n",
            "Extracting wav from ./S0902.tar.gz\n",
            "Extracting wav from ./S0903.tar.gz\n",
            "Extracting wav from ./S0904.tar.gz\n",
            "Extracting wav from ./S0905.tar.gz\n",
            "Extracting wav from ./S0906.tar.gz\n",
            "Extracting wav from ./S0907.tar.gz\n",
            "Extracting wav from ./S0908.tar.gz\n",
            "Extracting wav from ./S0912.tar.gz\n",
            "Extracting wav from ./S0913.tar.gz\n",
            "Extracting wav from ./S0914.tar.gz\n",
            "Extracting wav from ./S0915.tar.gz\n",
            "Extracting wav from ./S0916.tar.gz\n",
            "local/download_and_untar.sh: Successfully downloaded and un-tarred /content/dataset/aishell/data_aishell.tgz\n",
            "local/download_and_untar.sh: downloading data from www.openslr.org/resources/33/resource_aishell.tgz.  This may take some time, please be patient.\n",
            "--2022-05-23 07:57:47--  http://www.openslr.org/resources/33/resource_aishell.tgz\n",
            "Resolving www.openslr.org (www.openslr.org)... 46.101.158.64\n",
            "Connecting to www.openslr.org (www.openslr.org)|46.101.158.64|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://us.openslr.org/resources/33/resource_aishell.tgz [following]\n",
            "--2022-05-23 07:57:47--  https://us.openslr.org/resources/33/resource_aishell.tgz\n",
            "Resolving us.openslr.org (us.openslr.org)... 46.101.158.64\n",
            "Connecting to us.openslr.org (us.openslr.org)|46.101.158.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1246920 (1.2M) [application/x-gzip]\n",
            "Saving to: ‘resource_aishell.tgz’\n",
            "\n",
            "resource_aishell.tg 100%[===================>]   1.19M   989KB/s    in 1.2s    \n",
            "\n",
            "2022-05-23 07:57:49 (989 KB/s) - ‘resource_aishell.tgz’ saved [1246920/1246920]\n",
            "\n",
            "resource_aishell/\n",
            "resource_aishell/lexicon.txt\n",
            "resource_aishell/speaker.info\n",
            "local/download_and_untar.sh: Successfully downloaded and un-tarred /content/dataset/aishell/resource_aishell.tgz\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "#0\n",
        "bash run.sh"
      ],
      "metadata": {
        "id": "tRJqL0AaL3LO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fcd77de-cd44-4436-8de2-216fbd652567"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing data/local/train transcriptions\n",
            "Preparing data/local/dev transcriptions\n",
            "Preparing data/local/test transcriptions\n",
            "local/aishell_data_prep.sh: AISHELL data preparation succeeded\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "#1\n",
        "bash run.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47TT4KYa98UO",
        "outputId": "09020e20-2f99-4613-a720-ad78b690a469"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using resample and new sample rate is 16000\n",
            "/usr/local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "processed 1000 wavs, 443300 frames\n",
            "processed 2000 wavs, 892847 frames\n",
            "processed 3000 wavs, 1344011 frames\n",
            "processed 4000 wavs, 1797562 frames\n",
            "processed 5000 wavs, 2253575 frames\n",
            "processed 6000 wavs, 2707495 frames\n",
            "processed 7000 wavs, 3158345 frames\n",
            "processed 8000 wavs, 3615328 frames\n",
            "processed 9000 wavs, 4060797 frames\n",
            "processed 10000 wavs, 4515448 frames\n",
            "processed 11000 wavs, 4970775 frames\n",
            "processed 12000 wavs, 5419242 frames\n",
            "processed 13000 wavs, 5869983 frames\n",
            "processed 14000 wavs, 6319367 frames\n",
            "processed 15000 wavs, 6770030 frames\n",
            "processed 16000 wavs, 7217263 frames\n",
            "processed 17000 wavs, 7658070 frames\n",
            "processed 18000 wavs, 8105804 frames\n",
            "processed 19000 wavs, 8564491 frames\n",
            "processed 20000 wavs, 9010668 frames\n",
            "processed 21000 wavs, 9457968 frames\n",
            "processed 22000 wavs, 9911168 frames\n",
            "processed 23000 wavs, 10364940 frames\n",
            "processed 24000 wavs, 10811631 frames\n",
            "processed 25000 wavs, 11255285 frames\n",
            "processed 26000 wavs, 11707984 frames\n",
            "processed 27000 wavs, 12155935 frames\n",
            "processed 28000 wavs, 12602949 frames\n",
            "processed 29000 wavs, 13052622 frames\n",
            "processed 30000 wavs, 13502788 frames\n",
            "processed 31000 wavs, 13946081 frames\n",
            "processed 32000 wavs, 14401833 frames\n",
            "processed 33000 wavs, 14843456 frames\n",
            "processed 34000 wavs, 15293649 frames\n",
            "processed 35000 wavs, 15746491 frames\n",
            "processed 36000 wavs, 16199004 frames\n",
            "processed 37000 wavs, 16651593 frames\n",
            "processed 38000 wavs, 17100917 frames\n",
            "processed 39000 wavs, 17549104 frames\n",
            "processed 40000 wavs, 17996318 frames\n",
            "processed 41000 wavs, 18449951 frames\n",
            "processed 42000 wavs, 18901051 frames\n",
            "processed 43000 wavs, 19345066 frames\n",
            "processed 44000 wavs, 19798959 frames\n",
            "processed 45000 wavs, 20245964 frames\n",
            "processed 46000 wavs, 20697011 frames\n",
            "processed 47000 wavs, 21149003 frames\n",
            "processed 48000 wavs, 21599117 frames\n",
            "processed 49000 wavs, 22049953 frames\n",
            "processed 50000 wavs, 22498383 frames\n",
            "processed 51000 wavs, 22944351 frames\n",
            "processed 52000 wavs, 23391559 frames\n",
            "processed 53000 wavs, 23839728 frames\n",
            "processed 54000 wavs, 24296329 frames\n",
            "processed 55000 wavs, 24746599 frames\n",
            "processed 56000 wavs, 25203346 frames\n",
            "processed 57000 wavs, 25649745 frames\n",
            "processed 58000 wavs, 26097441 frames\n",
            "processed 59000 wavs, 26557795 frames\n",
            "processed 60000 wavs, 27000368 frames\n",
            "processed 61000 wavs, 27446320 frames\n",
            "processed 62000 wavs, 27899726 frames\n",
            "processed 63000 wavs, 28352690 frames\n",
            "processed 64000 wavs, 28806085 frames\n",
            "processed 65000 wavs, 29251050 frames\n",
            "processed 66000 wavs, 29706029 frames\n",
            "processed 67000 wavs, 30156134 frames\n",
            "processed 68000 wavs, 30607710 frames\n",
            "processed 69000 wavs, 31057694 frames\n",
            "processed 70000 wavs, 31504297 frames\n",
            "processed 71000 wavs, 31947556 frames\n",
            "processed 72000 wavs, 32403302 frames\n",
            "processed 73000 wavs, 32849514 frames\n",
            "processed 74000 wavs, 33310694 frames\n",
            "processed 75000 wavs, 33761631 frames\n",
            "processed 76000 wavs, 34219035 frames\n",
            "processed 77000 wavs, 34665942 frames\n",
            "processed 78000 wavs, 35116912 frames\n",
            "processed 79000 wavs, 35560042 frames\n",
            "processed 80000 wavs, 36014939 frames\n",
            "processed 81000 wavs, 36468734 frames\n",
            "processed 82000 wavs, 36915113 frames\n",
            "processed 83000 wavs, 37372960 frames\n",
            "processed 84000 wavs, 37824726 frames\n",
            "processed 85000 wavs, 38271024 frames\n",
            "processed 86000 wavs, 38722721 frames\n",
            "processed 87000 wavs, 39179937 frames\n",
            "processed 88000 wavs, 39634228 frames\n",
            "processed 89000 wavs, 40086987 frames\n",
            "processed 90000 wavs, 40536078 frames\n",
            "processed 91000 wavs, 40980504 frames\n",
            "processed 92000 wavs, 41429205 frames\n",
            "processed 93000 wavs, 41884146 frames\n",
            "processed 94000 wavs, 42328898 frames\n",
            "processed 95000 wavs, 42783065 frames\n",
            "processed 96000 wavs, 43235207 frames\n",
            "processed 97000 wavs, 43681372 frames\n",
            "processed 98000 wavs, 44132194 frames\n",
            "processed 99000 wavs, 44582046 frames\n",
            "processed 100000 wavs, 45032122 frames\n",
            "processed 101000 wavs, 45478134 frames\n",
            "processed 102000 wavs, 45924737 frames\n",
            "processed 103000 wavs, 46376105 frames\n",
            "processed 104000 wavs, 46824529 frames\n",
            "processed 105000 wavs, 47278545 frames\n",
            "processed 106000 wavs, 47725732 frames\n",
            "processed 107000 wavs, 48175836 frames\n",
            "processed 108000 wavs, 48622547 frames\n",
            "processed 109000 wavs, 49074209 frames\n",
            "processed 110000 wavs, 49526496 frames\n",
            "processed 111000 wavs, 49973931 frames\n",
            "processed 112000 wavs, 50420426 frames\n",
            "processed 113000 wavs, 50867136 frames\n",
            "processed 114000 wavs, 51317116 frames\n",
            "processed 115000 wavs, 51775573 frames\n",
            "processed 116000 wavs, 52224178 frames\n",
            "processed 117000 wavs, 52677564 frames\n",
            "processed 118000 wavs, 53123261 frames\n",
            "processed 119000 wavs, 53567309 frames\n",
            "processed 120000 wavs, 54025127 frames\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "#2\n",
        "bash run.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gky5dhEg_cBf",
        "outputId": "c023d856-0088-45f7-c7eb-6a240039f0cb"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Make a dictionary\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "#3\n",
        "bash run.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gay92LWXADi4",
        "outputId": "79288819-d351-46eb-f420-8dd962caf4c9"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepare data, prepare requried format\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "#4\n",
        "bash run.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTRrov-UAVwP",
        "outputId": "8f30b26e-c40b-44a1-deb3-8155996c6fcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run.sh: init method is file:///content/wenet/examples/aishell/s0/exp/conformer/ddp_init\n",
            "total gpus is: 1\n",
            "ASRModel(\n",
            "  (encoder): ConformerEncoder(\n",
            "    (global_cmvn): GlobalCMVN()\n",
            "    (embed): Conv2dSubsampling4(\n",
            "      (conv): Sequential(\n",
            "        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))\n",
            "        (1): ReLU()\n",
            "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
            "        (3): ReLU()\n",
            "      )\n",
            "      (out): Sequential(\n",
            "        (0): Linear(in_features=4864, out_features=256, bias=True)\n",
            "      )\n",
            "      (pos_enc): RelPositionalEncoding(\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "    (encoders): ModuleList(\n",
            "      (0): ConformerEncoderLayer(\n",
            "        (self_attn): RelPositionMultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (activation): SiLU()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "        )\n",
            "        (feed_forward_macaron): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (activation): SiLU()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "        )\n",
            "        (conv_module): ConvolutionModule(\n",
            "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
            "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
            "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
            "          (activation): SiLU()\n",
            "        )\n",
            "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (concat_linear): Identity()\n",
            "      )\n",
            "      (1): ConformerEncoderLayer(\n",
            "        (self_attn): RelPositionMultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (activation): SiLU()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "        )\n",
            "        (feed_forward_macaron): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (activation): SiLU()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "        )\n",
            "        (conv_module): ConvolutionModule(\n",
            "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
            "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
            "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
            "          (activation): SiLU()\n",
            "        )\n",
            "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (concat_linear): Identity()\n",
            "      )\n",
            "      (2): ConformerEncoderLayer(\n",
            "        (self_attn): RelPositionMultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (activation): SiLU()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "        )\n",
            "        (feed_forward_macaron): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (activation): SiLU()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "        )\n",
            "        (conv_module): ConvolutionModule(\n",
            "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
            "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
            "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
            "          (activation): SiLU()\n",
            "        )\n",
            "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (concat_linear): Identity()\n",
            "      )\n",
            "      (3): ConformerEncoderLayer(\n",
            "        (self_attn): RelPositionMultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (activation): SiLU()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "        )\n",
            "        (feed_forward_macaron): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (activation): SiLU()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "        )\n",
            "        (conv_module): ConvolutionModule(\n",
            "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
            "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
            "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
            "          (activation): SiLU()\n",
            "        )\n",
            "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (concat_linear): Identity()\n",
            "      )\n",
            "      (4): ConformerEncoderLayer(\n",
            "        (self_attn): RelPositionMultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (activation): SiLU()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "        )\n",
            "        (feed_forward_macaron): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (activation): SiLU()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "        )\n",
            "        (conv_module): ConvolutionModule(\n",
            "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
            "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
            "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
            "          (activation): SiLU()\n",
            "        )\n",
            "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (concat_linear): Identity()\n",
            "      )\n",
            "      (5): ConformerEncoderLayer(\n",
            "        (self_attn): RelPositionMultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (activation): SiLU()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "        )\n",
            "        (feed_forward_macaron): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (activation): SiLU()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "        )\n",
            "        (conv_module): ConvolutionModule(\n",
            "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
            "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
            "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
            "          (activation): SiLU()\n",
            "        )\n",
            "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (concat_linear): Identity()\n",
            "      )\n",
            "      (6): ConformerEncoderLayer(\n",
            "        (self_attn): RelPositionMultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (activation): SiLU()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "        )\n",
            "        (feed_forward_macaron): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (activation): SiLU()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "        )\n",
            "        (conv_module): ConvolutionModule(\n",
            "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
            "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
            "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
            "          (activation): SiLU()\n",
            "        )\n",
            "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (concat_linear): Identity()\n",
            "      )\n",
            "      (7): ConformerEncoderLayer(\n",
            "        (self_attn): RelPositionMultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (activation): SiLU()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "        )\n",
            "        (feed_forward_macaron): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (activation): SiLU()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "        )\n",
            "        (conv_module): ConvolutionModule(\n",
            "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
            "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
            "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
            "          (activation): SiLU()\n",
            "        )\n",
            "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (concat_linear): Identity()\n",
            "      )\n",
            "      (8): ConformerEncoderLayer(\n",
            "        (self_attn): RelPositionMultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (activation): SiLU()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "        )\n",
            "        (feed_forward_macaron): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (activation): SiLU()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "        )\n",
            "        (conv_module): ConvolutionModule(\n",
            "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
            "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
            "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
            "          (activation): SiLU()\n",
            "        )\n",
            "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (concat_linear): Identity()\n",
            "      )\n",
            "      (9): ConformerEncoderLayer(\n",
            "        (self_attn): RelPositionMultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (activation): SiLU()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "        )\n",
            "        (feed_forward_macaron): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (activation): SiLU()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "        )\n",
            "        (conv_module): ConvolutionModule(\n",
            "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
            "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
            "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
            "          (activation): SiLU()\n",
            "        )\n",
            "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (concat_linear): Identity()\n",
            "      )\n",
            "      (10): ConformerEncoderLayer(\n",
            "        (self_attn): RelPositionMultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (activation): SiLU()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "        )\n",
            "        (feed_forward_macaron): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (activation): SiLU()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "        )\n",
            "        (conv_module): ConvolutionModule(\n",
            "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
            "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
            "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
            "          (activation): SiLU()\n",
            "        )\n",
            "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (concat_linear): Identity()\n",
            "      )\n",
            "      (11): ConformerEncoderLayer(\n",
            "        (self_attn): RelPositionMultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (activation): SiLU()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "        )\n",
            "        (feed_forward_macaron): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (activation): SiLU()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "        )\n",
            "        (conv_module): ConvolutionModule(\n",
            "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
            "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
            "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
            "          (activation): SiLU()\n",
            "        )\n",
            "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (concat_linear): Identity()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (embed): Sequential(\n",
            "      (0): Embedding(4233, 256)\n",
            "      (1): PositionalEncoding(\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "    (output_layer): Linear(in_features=256, out_features=4233, bias=True)\n",
            "    (decoders): ModuleList(\n",
            "      (0): DecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (src_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (activation): ReLU()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "        )\n",
            "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (concat_linear1): Identity()\n",
            "        (concat_linear2): Identity()\n",
            "      )\n",
            "      (1): DecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (src_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (activation): ReLU()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "        )\n",
            "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (concat_linear1): Identity()\n",
            "        (concat_linear2): Identity()\n",
            "      )\n",
            "      (2): DecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (src_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (activation): ReLU()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "        )\n",
            "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (concat_linear1): Identity()\n",
            "        (concat_linear2): Identity()\n",
            "      )\n",
            "      (3): DecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (src_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (activation): ReLU()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "        )\n",
            "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (concat_linear1): Identity()\n",
            "        (concat_linear2): Identity()\n",
            "      )\n",
            "      (4): DecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (src_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (activation): ReLU()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "        )\n",
            "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (concat_linear1): Identity()\n",
            "        (concat_linear2): Identity()\n",
            "      )\n",
            "      (5): DecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (src_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (activation): ReLU()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "        )\n",
            "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (concat_linear1): Identity()\n",
            "        (concat_linear2): Identity()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (ctc): CTC(\n",
            "    (ctc_lo): Linear(in_features=256, out_features=4233, bias=True)\n",
            "    (ctc_loss): CTCLoss()\n",
            "  )\n",
            "  (criterion_att): LabelSmoothingLoss(\n",
            "    (criterion): KLDivLoss()\n",
            "  )\n",
            ")\n",
            "the number of model params: 46197266\n",
            "2022-05-23 08:21:31,099 INFO Checkpoint: save to checkpoint exp/conformer/init.pt\n",
            "2022-05-23 08:21:31,466 INFO Epoch 0 TRAIN info lr 8e-08\n",
            "2022-05-23 08:21:31,468 INFO using accumulate grad, new batch size is 4 times larger than before\n",
            "2022-05-23 08:21:59,966 DEBUG TRAIN Batch 0/0 loss 183.087341 loss_att 66.840126 loss_ctc 454.330841 lr 0.00000008 rank 0\n",
            "2022-05-23 08:22:27,427 DEBUG TRAIN Batch 0/100 loss 230.735474 loss_att 85.051140 loss_ctc 570.665588 lr 0.00000208 rank 0\n",
            "2022-05-23 08:22:55,332 DEBUG TRAIN Batch 0/200 loss 194.002579 loss_att 108.153656 loss_ctc 394.316711 lr 0.00000408 rank 0\n",
            "2022-05-23 08:23:24,371 DEBUG TRAIN Batch 0/300 loss 164.309082 loss_att 127.111748 loss_ctc 251.102875 lr 0.00000608 rank 0\n",
            "2022-05-23 08:23:54,652 DEBUG TRAIN Batch 0/400 loss 159.847610 loss_att 139.761688 loss_ctc 206.714752 lr 0.00000808 rank 0\n",
            "2022-05-23 08:24:26,170 DEBUG TRAIN Batch 0/500 loss 69.406197 loss_att 65.454376 loss_ctc 78.627121 lr 0.00001008 rank 0\n",
            "2022-05-23 08:24:55,762 DEBUG TRAIN Batch 0/600 loss 84.658958 loss_att 79.910126 loss_ctc 95.739563 lr 0.00001208 rank 0\n",
            "2022-05-23 08:25:25,689 DEBUG TRAIN Batch 0/700 loss 106.422943 loss_att 101.769241 loss_ctc 117.281578 lr 0.00001408 rank 0\n",
            "2022-05-23 08:25:55,608 DEBUG TRAIN Batch 0/800 loss 127.779320 loss_att 122.712906 loss_ctc 139.600952 lr 0.00001608 rank 0\n",
            "2022-05-23 08:26:25,730 DEBUG TRAIN Batch 0/900 loss 134.033752 loss_att 127.743813 loss_ctc 148.710266 lr 0.00001808 rank 0\n",
            "2022-05-23 08:26:56,958 DEBUG TRAIN Batch 0/1000 loss 54.728676 loss_att 52.331364 loss_ctc 60.322407 lr 0.00002008 rank 0\n",
            "2022-05-23 08:27:27,362 DEBUG TRAIN Batch 0/1100 loss 74.051277 loss_att 70.693001 loss_ctc 81.887245 lr 0.00002208 rank 0\n",
            "2022-05-23 08:27:57,225 DEBUG TRAIN Batch 0/1200 loss 81.250351 loss_att 77.378464 loss_ctc 90.284752 lr 0.00002408 rank 0\n",
            "2022-05-23 08:28:27,293 DEBUG TRAIN Batch 0/1300 loss 105.004135 loss_att 100.481812 loss_ctc 115.556244 lr 0.00002608 rank 0\n",
            "2022-05-23 08:28:57,911 DEBUG TRAIN Batch 0/1400 loss 118.204941 loss_att 113.087288 loss_ctc 130.146133 lr 0.00002808 rank 0\n",
            "2022-05-23 08:29:29,457 DEBUG TRAIN Batch 0/1500 loss 51.283539 loss_att 49.396496 loss_ctc 55.686638 lr 0.00003008 rank 0\n",
            "2022-05-23 08:29:59,668 DEBUG TRAIN Batch 0/1600 loss 66.698364 loss_att 63.999718 loss_ctc 72.995193 lr 0.00003208 rank 0\n",
            "2022-05-23 08:30:29,734 DEBUG TRAIN Batch 0/1700 loss 74.731476 loss_att 71.773521 loss_ctc 81.633369 lr 0.00003408 rank 0\n",
            "2022-05-23 08:30:59,494 DEBUG TRAIN Batch 0/1800 loss 96.532021 loss_att 92.462341 loss_ctc 106.027924 lr 0.00003608 rank 0\n",
            "2022-05-23 08:31:29,924 DEBUG TRAIN Batch 0/1900 loss 110.638947 loss_att 105.915436 loss_ctc 121.660477 lr 0.00003808 rank 0\n",
            "2022-05-23 08:32:01,014 DEBUG TRAIN Batch 0/2000 loss 51.286400 loss_att 49.131947 loss_ctc 56.313454 lr 0.00004008 rank 0\n",
            "2022-05-23 08:32:30,571 DEBUG TRAIN Batch 0/2100 loss 69.772461 loss_att 66.764145 loss_ctc 76.791855 lr 0.00004208 rank 0\n",
            "2022-05-23 08:33:00,359 DEBUG TRAIN Batch 0/2200 loss 76.361961 loss_att 72.975098 loss_ctc 84.264641 lr 0.00004408 rank 0\n",
            "2022-05-23 08:33:30,004 DEBUG TRAIN Batch 0/2300 loss 93.195282 loss_att 89.073280 loss_ctc 102.813278 lr 0.00004608 rank 0\n",
            "2022-05-23 08:34:00,266 DEBUG TRAIN Batch 0/2400 loss 115.873985 loss_att 110.775787 loss_ctc 127.769768 lr 0.00004808 rank 0\n",
            "2022-05-23 08:34:31,229 DEBUG TRAIN Batch 0/2500 loss 48.505226 loss_att 46.333038 loss_ctc 53.573662 lr 0.00005008 rank 0\n",
            "2022-05-23 08:35:00,836 DEBUG TRAIN Batch 0/2600 loss 67.488060 loss_att 64.320488 loss_ctc 74.879059 lr 0.00005208 rank 0\n",
            "2022-05-23 08:35:30,770 DEBUG TRAIN Batch 0/2700 loss 77.149109 loss_att 73.505356 loss_ctc 85.651215 lr 0.00005408 rank 0\n",
            "2022-05-23 08:36:01,142 DEBUG TRAIN Batch 0/2800 loss 96.597473 loss_att 92.038437 loss_ctc 107.235245 lr 0.00005608 rank 0\n",
            "2022-05-23 08:36:31,249 DEBUG TRAIN Batch 0/2900 loss 108.162918 loss_att 102.810555 loss_ctc 120.651779 lr 0.00005808 rank 0\n",
            "2022-05-23 08:37:02,531 DEBUG TRAIN Batch 0/3000 loss 54.442890 loss_att 51.795212 loss_ctc 60.620804 lr 0.00006008 rank 0\n",
            "2022-05-23 08:37:32,176 DEBUG TRAIN Batch 0/3100 loss 66.667732 loss_att 63.163078 loss_ctc 74.845268 lr 0.00006208 rank 0\n",
            "2022-05-23 08:38:01,980 DEBUG TRAIN Batch 0/3200 loss 78.144104 loss_att 74.111137 loss_ctc 87.554352 lr 0.00006408 rank 0\n",
            "2022-05-23 08:38:31,829 DEBUG TRAIN Batch 0/3300 loss 96.637329 loss_att 91.526634 loss_ctc 108.562286 lr 0.00006608 rank 0\n",
            "2022-05-23 08:39:01,713 DEBUG TRAIN Batch 0/3400 loss 103.082809 loss_att 97.179001 loss_ctc 116.858376 lr 0.00006808 rank 0\n",
            "2022-05-23 08:39:33,302 DEBUG TRAIN Batch 0/3500 loss 49.059532 loss_att 45.983303 loss_ctc 56.237396 lr 0.00007008 rank 0\n",
            "2022-05-23 08:40:03,248 DEBUG TRAIN Batch 0/3600 loss 61.622009 loss_att 57.643856 loss_ctc 70.904358 lr 0.00007208 rank 0\n",
            "2022-05-23 08:40:32,837 DEBUG TRAIN Batch 0/3700 loss 76.647018 loss_att 72.559418 loss_ctc 86.184738 lr 0.00007408 rank 0\n",
            "2022-05-23 08:41:03,043 DEBUG TRAIN Batch 0/3800 loss 92.466606 loss_att 87.225212 loss_ctc 104.696533 lr 0.00007608 rank 0\n",
            "2022-05-23 08:41:33,286 DEBUG TRAIN Batch 0/3900 loss 110.445129 loss_att 104.816650 loss_ctc 123.578247 lr 0.00007808 rank 0\n",
            "2022-05-23 08:42:03,896 DEBUG TRAIN Batch 0/4000 loss 46.991531 loss_att 44.191444 loss_ctc 53.525063 lr 0.00008008 rank 0\n",
            "2022-05-23 08:42:33,682 DEBUG TRAIN Batch 0/4100 loss 60.709373 loss_att 56.355110 loss_ctc 70.869324 lr 0.00008208 rank 0\n",
            "2022-05-23 08:43:03,720 DEBUG TRAIN Batch 0/4200 loss 76.592766 loss_att 71.129959 loss_ctc 89.339325 lr 0.00008408 rank 0\n",
            "2022-05-23 08:43:33,773 DEBUG TRAIN Batch 0/4300 loss 88.097031 loss_att 81.720726 loss_ctc 102.975067 lr 0.00008608 rank 0\n",
            "2022-05-23 08:44:03,761 DEBUG TRAIN Batch 0/4400 loss 103.209839 loss_att 96.038239 loss_ctc 119.943581 lr 0.00008808 rank 0\n",
            "2022-05-23 08:44:34,597 DEBUG TRAIN Batch 0/4500 loss 44.065666 loss_att 40.252533 loss_ctc 52.962982 lr 0.00009008 rank 0\n",
            "2022-05-23 08:45:03,915 DEBUG TRAIN Batch 0/4600 loss 68.442276 loss_att 63.624859 loss_ctc 79.682930 lr 0.00009208 rank 0\n",
            "2022-05-23 08:45:33,337 DEBUG TRAIN Batch 0/4700 loss 75.624954 loss_att 70.819321 loss_ctc 86.838089 lr 0.00009408 rank 0\n",
            "2022-05-23 08:46:03,623 DEBUG TRAIN Batch 0/4800 loss 99.051270 loss_att 92.790924 loss_ctc 113.658752 lr 0.00009608 rank 0\n",
            "2022-05-23 08:46:34,085 DEBUG TRAIN Batch 0/4900 loss 95.912453 loss_att 88.824913 loss_ctc 112.450043 lr 0.00009808 rank 0\n",
            "2022-05-23 08:47:04,828 DEBUG TRAIN Batch 0/5000 loss 39.117241 loss_att 35.789062 loss_ctc 46.882988 lr 0.00010008 rank 0\n",
            "2022-05-23 08:47:34,186 DEBUG TRAIN Batch 0/5100 loss 62.287926 loss_att 58.316769 loss_ctc 71.553947 lr 0.00010208 rank 0\n",
            "2022-05-23 08:48:03,909 DEBUG TRAIN Batch 0/5200 loss 65.350044 loss_att 59.394482 loss_ctc 79.246353 lr 0.00010408 rank 0\n",
            "2022-05-23 08:48:33,639 DEBUG TRAIN Batch 0/5300 loss 87.622620 loss_att 80.936012 loss_ctc 103.224701 lr 0.00010608 rank 0\n",
            "2022-05-23 08:49:03,882 DEBUG TRAIN Batch 0/5400 loss 104.702881 loss_att 97.957626 loss_ctc 120.441803 lr 0.00010808 rank 0\n",
            "2022-05-23 08:49:35,062 DEBUG TRAIN Batch 0/5500 loss 43.200817 loss_att 39.939865 loss_ctc 50.809708 lr 0.00011008 rank 0\n",
            "2022-05-23 08:50:04,453 DEBUG TRAIN Batch 0/5600 loss 61.985916 loss_att 57.462807 loss_ctc 72.539841 lr 0.00011208 rank 0\n",
            "2022-05-23 08:50:34,146 DEBUG TRAIN Batch 0/5700 loss 76.297760 loss_att 70.651428 loss_ctc 89.472534 lr 0.00011408 rank 0\n",
            "2022-05-23 08:51:03,968 DEBUG TRAIN Batch 0/5800 loss 85.925278 loss_att 79.210495 loss_ctc 101.593102 lr 0.00011608 rank 0\n",
            "2022-05-23 08:51:33,951 DEBUG TRAIN Batch 0/5900 loss 106.463303 loss_att 97.241653 loss_ctc 127.980476 lr 0.00011808 rank 0\n",
            "2022-05-23 08:52:04,912 DEBUG TRAIN Batch 0/6000 loss 46.305313 loss_att 43.049141 loss_ctc 53.903053 lr 0.00012008 rank 0\n",
            "2022-05-23 08:52:35,037 DEBUG TRAIN Batch 0/6100 loss 59.392155 loss_att 54.798260 loss_ctc 70.111244 lr 0.00012208 rank 0\n",
            "2022-05-23 08:53:04,222 DEBUG TRAIN Batch 0/6200 loss 70.318604 loss_att 64.928940 loss_ctc 82.894478 lr 0.00012408 rank 0\n",
            "2022-05-23 08:53:33,830 DEBUG TRAIN Batch 0/6300 loss 86.979889 loss_att 80.883392 loss_ctc 101.205040 lr 0.00012608 rank 0\n",
            "2022-05-23 08:54:03,593 DEBUG TRAIN Batch 0/6400 loss 103.002922 loss_att 95.042534 loss_ctc 121.577171 lr 0.00012808 rank 0\n",
            "2022-05-23 08:54:34,650 DEBUG TRAIN Batch 0/6500 loss 46.426460 loss_att 43.187813 loss_ctc 53.983303 lr 0.00013008 rank 0\n",
            "2022-05-23 08:55:04,085 DEBUG TRAIN Batch 0/6600 loss 57.219810 loss_att 53.614624 loss_ctc 65.631912 lr 0.00013208 rank 0\n",
            "2022-05-23 08:55:33,834 DEBUG TRAIN Batch 0/6700 loss 68.459312 loss_att 63.914101 loss_ctc 79.064819 lr 0.00013408 rank 0\n",
            "2022-05-23 08:56:03,780 DEBUG TRAIN Batch 0/6800 loss 87.404411 loss_att 82.302711 loss_ctc 99.308380 lr 0.00013608 rank 0\n",
            "2022-05-23 08:56:33,481 DEBUG TRAIN Batch 0/6900 loss 91.286728 loss_att 85.568550 loss_ctc 104.629150 lr 0.00013808 rank 0\n",
            "2022-05-23 08:57:04,727 DEBUG TRAIN Batch 0/7000 loss 44.177345 loss_att 40.919903 loss_ctc 51.778034 lr 0.00014008 rank 0\n",
            "2022-05-23 08:57:34,173 DEBUG TRAIN Batch 0/7100 loss 56.671436 loss_att 54.253223 loss_ctc 62.313927 lr 0.00014208 rank 0\n",
            "2022-05-23 08:58:03,959 DEBUG TRAIN Batch 0/7200 loss 59.409027 loss_att 57.055225 loss_ctc 64.901230 lr 0.00014408 rank 0\n",
            "2022-05-23 08:58:33,572 DEBUG TRAIN Batch 0/7300 loss 76.625214 loss_att 73.772865 loss_ctc 83.280685 lr 0.00014608 rank 0\n",
            "2022-05-23 08:59:03,687 DEBUG TRAIN Batch 0/7400 loss 87.635376 loss_att 85.861008 loss_ctc 91.775574 lr 0.00014808 rank 0\n",
            "2022-05-23 08:59:34,710 DEBUG TRAIN Batch 0/7500 loss 43.047729 loss_att 40.893513 loss_ctc 48.074230 lr 0.00015008 rank 0\n",
            "2022-05-23 08:59:40,781 DEBUG CV Batch 0/0 loss 35.011009 loss_att 34.476326 loss_ctc 36.258602 history loss 32.951538 rank 0\n",
            "2022-05-23 08:59:52,451 DEBUG CV Batch 0/100 loss 52.304398 loss_att 51.681633 loss_ctc 53.757507 history loss 69.750211 rank 0\n",
            "2022-05-23 09:00:02,940 DEBUG CV Batch 0/200 loss 64.512108 loss_att 64.206757 loss_ctc 65.224594 history loss 70.194188 rank 0\n",
            "2022-05-23 09:00:13,841 DEBUG CV Batch 0/300 loss 74.278435 loss_att 74.160728 loss_ctc 74.553078 history loss 70.491735 rank 0\n",
            "2022-05-23 09:00:25,619 DEBUG CV Batch 0/400 loss 101.755051 loss_att 102.215668 loss_ctc 100.680283 history loss 69.986544 rank 0\n",
            "2022-05-23 09:00:37,833 DEBUG CV Batch 0/500 loss 41.552830 loss_att 41.333557 loss_ctc 42.064472 history loss 70.403358 rank 0\n",
            "2022-05-23 09:00:49,929 DEBUG CV Batch 0/600 loss 54.443924 loss_att 53.918850 loss_ctc 55.669102 history loss 70.458624 rank 0\n",
            "2022-05-23 09:01:01,198 DEBUG CV Batch 0/700 loss 64.757706 loss_att 65.037384 loss_ctc 64.105118 history loss 70.299582 rank 0\n",
            "2022-05-23 09:01:12,665 DEBUG CV Batch 0/800 loss 70.861160 loss_att 71.282852 loss_ctc 69.877213 history loss 70.486782 rank 0\n",
            "2022-05-23 09:01:24,241 INFO Epoch 0 CV info cv_loss 70.90523560205075\n",
            "2022-05-23 09:01:24,241 INFO Checkpoint: save to checkpoint exp/conformer/0.pt\n",
            "2022-05-23 09:01:24,575 INFO Epoch 1 TRAIN info lr 0.00015015999999999999\n",
            "2022-05-23 09:01:24,577 INFO using accumulate grad, new batch size is 4 times larger than before\n",
            "2022-05-23 09:01:50,626 DEBUG TRAIN Batch 1/0 loss 44.040150 loss_att 41.964394 loss_ctc 48.883583 lr 0.00015024 rank 0\n",
            "2022-05-23 09:02:20,406 DEBUG TRAIN Batch 1/100 loss 53.002594 loss_att 51.564644 loss_ctc 56.357822 lr 0.00015224 rank 0\n",
            "2022-05-23 09:02:50,572 DEBUG TRAIN Batch 1/200 loss 60.724152 loss_att 59.548016 loss_ctc 63.468468 lr 0.00015424 rank 0\n",
            "2022-05-23 09:03:19,898 DEBUG TRAIN Batch 1/300 loss 69.887009 loss_att 67.655396 loss_ctc 75.094101 lr 0.00015624 rank 0\n",
            "2022-05-23 09:03:50,175 DEBUG TRAIN Batch 1/400 loss 90.151543 loss_att 89.879837 loss_ctc 90.785522 lr 0.00015824 rank 0\n",
            "2022-05-23 09:04:21,365 DEBUG TRAIN Batch 1/500 loss 37.857742 loss_att 37.408329 loss_ctc 38.906372 lr 0.00016024 rank 0\n",
            "2022-05-23 09:04:50,732 DEBUG TRAIN Batch 1/600 loss 54.835434 loss_att 53.674656 loss_ctc 57.543922 lr 0.00016224 rank 0\n",
            "2022-05-23 09:05:20,451 DEBUG TRAIN Batch 1/700 loss 63.258904 loss_att 63.637917 loss_ctc 62.374550 lr 0.00016424 rank 0\n",
            "2022-05-23 09:05:50,453 DEBUG TRAIN Batch 1/800 loss 68.973923 loss_att 68.846764 loss_ctc 69.270615 lr 0.00016624 rank 0\n",
            "2022-05-23 09:06:20,655 DEBUG TRAIN Batch 1/900 loss 86.058784 loss_att 86.947960 loss_ctc 83.984047 lr 0.00016824 rank 0\n",
            "2022-05-23 09:06:51,805 DEBUG TRAIN Batch 1/1000 loss 39.871769 loss_att 38.887280 loss_ctc 42.168907 lr 0.00017024 rank 0\n",
            "2022-05-23 09:07:21,292 DEBUG TRAIN Batch 1/1100 loss 55.217834 loss_att 56.089268 loss_ctc 53.184502 lr 0.00017224 rank 0\n",
            "2022-05-23 09:07:51,414 DEBUG TRAIN Batch 1/1200 loss 64.669914 loss_att 65.258263 loss_ctc 63.297100 lr 0.00017424 rank 0\n",
            "2022-05-23 09:08:21,368 DEBUG TRAIN Batch 1/1300 loss 71.644989 loss_att 72.691483 loss_ctc 69.203163 lr 0.00017624 rank 0\n",
            "2022-05-23 09:08:51,789 DEBUG TRAIN Batch 1/1400 loss 87.109695 loss_att 89.714378 loss_ctc 81.032104 lr 0.00017824 rank 0\n",
            "2022-05-23 09:09:22,172 DEBUG TRAIN Batch 1/1500 loss 37.587109 loss_att 37.786068 loss_ctc 37.122871 lr 0.00018024 rank 0\n",
            "2022-05-23 09:09:51,516 DEBUG TRAIN Batch 1/1600 loss 47.658134 loss_att 48.149338 loss_ctc 46.511986 lr 0.00018224 rank 0\n",
            "2022-05-23 09:10:21,131 DEBUG TRAIN Batch 1/1700 loss 63.648033 loss_att 64.028183 loss_ctc 62.761017 lr 0.00018424 rank 0\n",
            "2022-05-23 09:10:51,151 DEBUG TRAIN Batch 1/1800 loss 72.308098 loss_att 75.025742 loss_ctc 65.966934 lr 0.00018624 rank 0\n",
            "2022-05-23 09:11:21,266 DEBUG TRAIN Batch 1/1900 loss 79.932709 loss_att 84.198288 loss_ctc 69.979675 lr 0.00018824 rank 0\n",
            "2022-05-23 09:11:51,914 DEBUG TRAIN Batch 1/2000 loss 34.724606 loss_att 36.247921 loss_ctc 31.170202 lr 0.00019024 rank 0\n",
            "2022-05-23 09:12:21,051 DEBUG TRAIN Batch 1/2100 loss 48.752956 loss_att 48.929840 loss_ctc 48.340225 lr 0.00019224 rank 0\n",
            "2022-05-23 09:12:50,597 DEBUG TRAIN Batch 1/2200 loss 60.397537 loss_att 62.501736 loss_ctc 55.487740 lr 0.00019424 rank 0\n",
            "2022-05-23 09:13:20,600 DEBUG TRAIN Batch 1/2300 loss 67.057480 loss_att 69.577354 loss_ctc 61.177773 lr 0.00019624 rank 0\n",
            "2022-05-23 09:13:50,360 DEBUG TRAIN Batch 1/2400 loss 84.259682 loss_att 87.543411 loss_ctc 76.597641 lr 0.00019824 rank 0\n",
            "2022-05-23 09:14:20,927 DEBUG TRAIN Batch 1/2500 loss 35.684174 loss_att 35.883160 loss_ctc 35.219872 lr 0.00020024 rank 0\n",
            "2022-05-23 09:14:50,642 DEBUG TRAIN Batch 1/2600 loss 45.336235 loss_att 47.202103 loss_ctc 40.982544 lr 0.00020224 rank 0\n",
            "2022-05-23 09:15:21,006 DEBUG TRAIN Batch 1/2700 loss 56.897205 loss_att 58.729424 loss_ctc 52.622032 lr 0.00020424 rank 0\n",
            "2022-05-23 09:15:51,065 DEBUG TRAIN Batch 1/2800 loss 67.671638 loss_att 70.575668 loss_ctc 60.895576 lr 0.00020624 rank 0\n",
            "2022-05-23 09:16:21,141 DEBUG TRAIN Batch 1/2900 loss 79.202103 loss_att 83.610733 loss_ctc 68.915298 lr 0.00020824 rank 0\n",
            "2022-05-23 09:16:52,305 DEBUG TRAIN Batch 1/3000 loss 36.017464 loss_att 36.655006 loss_ctc 34.529865 lr 0.00021024 rank 0\n",
            "2022-05-23 09:17:21,814 DEBUG TRAIN Batch 1/3100 loss 51.307281 loss_att 53.400406 loss_ctc 46.423328 lr 0.00021224 rank 0\n",
            "2022-05-23 09:17:51,437 DEBUG TRAIN Batch 1/3200 loss 55.279240 loss_att 58.242157 loss_ctc 48.365768 lr 0.00021424 rank 0\n",
            "2022-05-23 09:18:21,630 DEBUG TRAIN Batch 1/3300 loss 68.092369 loss_att 71.763474 loss_ctc 59.526451 lr 0.00021624 rank 0\n",
            "2022-05-23 09:18:51,916 DEBUG TRAIN Batch 1/3400 loss 86.886299 loss_att 91.550377 loss_ctc 76.003433 lr 0.00021824 rank 0\n",
            "2022-05-23 09:19:22,758 DEBUG TRAIN Batch 1/3500 loss 33.828178 loss_att 35.516129 loss_ctc 29.889631 lr 0.00022024 rank 0\n",
            "2022-05-23 09:19:52,322 DEBUG TRAIN Batch 1/3600 loss 47.849796 loss_att 49.223522 loss_ctc 44.644432 lr 0.00022224 rank 0\n",
            "2022-05-23 09:20:22,008 DEBUG TRAIN Batch 1/3700 loss 52.668480 loss_att 55.257767 loss_ctc 46.626816 lr 0.00022424 rank 0\n",
            "2022-05-23 09:20:51,897 DEBUG TRAIN Batch 1/3800 loss 59.821838 loss_att 63.584503 loss_ctc 51.042290 lr 0.00022624 rank 0\n",
            "2022-05-23 09:21:22,362 DEBUG TRAIN Batch 1/3900 loss 72.506668 loss_att 77.962036 loss_ctc 59.777489 lr 0.00022824 rank 0\n",
            "2022-05-23 09:21:53,194 DEBUG TRAIN Batch 1/4000 loss 38.973591 loss_att 39.511295 loss_ctc 37.718948 lr 0.00023024 rank 0\n",
            "2022-05-23 09:22:23,062 DEBUG TRAIN Batch 1/4100 loss 45.061840 loss_att 46.726849 loss_ctc 41.176819 lr 0.00023224 rank 0\n",
            "2022-05-23 09:22:52,708 DEBUG TRAIN Batch 1/4200 loss 56.689693 loss_att 59.848366 loss_ctc 49.319458 lr 0.00023424 rank 0\n",
            "2022-05-23 09:23:22,040 DEBUG TRAIN Batch 1/4300 loss 63.382854 loss_att 67.391495 loss_ctc 54.029358 lr 0.00023624 rank 0\n",
            "2022-05-23 09:23:52,044 DEBUG TRAIN Batch 1/4400 loss 74.177277 loss_att 79.282043 loss_ctc 62.266163 lr 0.00023824 rank 0\n",
            "2022-05-23 09:24:22,846 DEBUG TRAIN Batch 1/4500 loss 35.875343 loss_att 36.737679 loss_ctc 33.863228 lr 0.00024024 rank 0\n",
            "2022-05-23 09:24:52,280 DEBUG TRAIN Batch 1/4600 loss 42.039692 loss_att 43.595695 loss_ctc 38.409012 lr 0.00024224 rank 0\n",
            "2022-05-23 09:25:21,998 DEBUG TRAIN Batch 1/4700 loss 49.758987 loss_att 52.546177 loss_ctc 43.255554 lr 0.00024424 rank 0\n",
            "2022-05-23 09:25:51,677 DEBUG TRAIN Batch 1/4800 loss 71.056419 loss_att 73.324982 loss_ctc 65.763100 lr 0.00024624 rank 0\n",
            "2022-05-23 09:26:22,453 DEBUG TRAIN Batch 1/4900 loss 75.423103 loss_att 80.717667 loss_ctc 63.069126 lr 0.00024824 rank 0\n",
            "2022-05-23 09:26:53,622 DEBUG TRAIN Batch 1/5000 loss 31.815868 loss_att 32.109528 loss_ctc 31.130667 lr 0.00025024 rank 0\n",
            "2022-05-23 09:27:23,346 DEBUG TRAIN Batch 1/5100 loss 43.022785 loss_att 44.801155 loss_ctc 38.873257 lr 0.00025224 rank 0\n",
            "2022-05-23 09:27:52,599 DEBUG TRAIN Batch 1/5200 loss 53.265724 loss_att 54.456268 loss_ctc 50.487778 lr 0.00025424 rank 0\n",
            "2022-05-23 09:28:22,427 DEBUG TRAIN Batch 1/5300 loss 62.359474 loss_att 64.776871 loss_ctc 56.718887 lr 0.00025624 rank 0\n",
            "2022-05-23 09:28:52,465 DEBUG TRAIN Batch 1/5400 loss 71.019394 loss_att 75.868446 loss_ctc 59.704948 lr 0.00025824 rank 0\n",
            "2022-05-23 09:29:23,289 DEBUG TRAIN Batch 1/5500 loss 33.465847 loss_att 34.413445 loss_ctc 31.254787 lr 0.00026024 rank 0\n",
            "2022-05-23 09:29:53,155 DEBUG TRAIN Batch 1/5600 loss 41.812851 loss_att 42.025948 loss_ctc 41.315620 lr 0.00026224 rank 0\n",
            "2022-05-23 09:30:23,038 DEBUG TRAIN Batch 1/5700 loss 50.784294 loss_att 51.833549 loss_ctc 48.336021 lr 0.00026424 rank 0\n",
            "2022-05-23 09:30:52,660 DEBUG TRAIN Batch 1/5800 loss 52.434612 loss_att 55.924202 loss_ctc 44.292233 lr 0.00026624 rank 0\n",
            "2022-05-23 09:31:23,132 DEBUG TRAIN Batch 1/5900 loss 68.041809 loss_att 72.104599 loss_ctc 58.561981 lr 0.00026824 rank 0\n",
            "2022-05-23 09:31:54,032 DEBUG TRAIN Batch 1/6000 loss 28.244568 loss_att 27.796928 loss_ctc 29.289061 lr 0.00027024 rank 0\n",
            "2022-05-23 09:32:23,490 DEBUG TRAIN Batch 1/6100 loss 40.375412 loss_att 40.411049 loss_ctc 40.292267 lr 0.00027224 rank 0\n",
            "2022-05-23 09:32:53,192 DEBUG TRAIN Batch 1/6200 loss 46.472073 loss_att 47.594612 loss_ctc 43.852821 lr 0.00027424 rank 0\n",
            "2022-05-23 09:33:22,916 DEBUG TRAIN Batch 1/6300 loss 55.786812 loss_att 58.305519 loss_ctc 49.909828 lr 0.00027624 rank 0\n",
            "2022-05-23 09:33:53,044 DEBUG TRAIN Batch 1/6400 loss 68.722641 loss_att 72.673988 loss_ctc 59.502823 lr 0.00027824 rank 0\n",
            "2022-05-23 09:34:23,407 DEBUG TRAIN Batch 1/6500 loss 24.837133 loss_att 24.440090 loss_ctc 25.763567 lr 0.00028024 rank 0\n",
            "2022-05-23 09:34:53,454 DEBUG TRAIN Batch 1/6600 loss 41.927261 loss_att 41.666115 loss_ctc 42.536606 lr 0.00028224 rank 0\n",
            "2022-05-23 09:35:23,044 DEBUG TRAIN Batch 1/6700 loss 42.935184 loss_att 43.612320 loss_ctc 41.355202 lr 0.00028424 rank 0\n",
            "2022-05-23 09:35:52,631 DEBUG TRAIN Batch 1/6800 loss 51.917812 loss_att 53.388393 loss_ctc 48.486458 lr 0.00028624 rank 0\n",
            "2022-05-23 09:36:23,103 DEBUG TRAIN Batch 1/6900 loss 60.558231 loss_att 61.977230 loss_ctc 57.247238 lr 0.00028824 rank 0\n",
            "2022-05-23 09:36:54,124 DEBUG TRAIN Batch 1/7000 loss 27.779606 loss_att 27.394638 loss_ctc 28.677864 lr 0.00029024 rank 0\n",
            "2022-05-23 09:37:23,234 DEBUG TRAIN Batch 1/7100 loss 34.219875 loss_att 34.550735 loss_ctc 33.447865 lr 0.00029224 rank 0\n",
            "2022-05-23 09:37:52,944 DEBUG TRAIN Batch 1/7200 loss 38.703838 loss_att 38.580849 loss_ctc 38.990814 lr 0.00029424 rank 0\n",
            "2022-05-23 09:38:22,243 DEBUG TRAIN Batch 1/7300 loss 39.120930 loss_att 40.641304 loss_ctc 35.573395 lr 0.00029624 rank 0\n",
            "2022-05-23 09:38:51,692 DEBUG TRAIN Batch 1/7400 loss 61.336426 loss_att 63.161457 loss_ctc 57.078007 lr 0.00029824 rank 0\n",
            "2022-05-23 09:39:22,567 DEBUG TRAIN Batch 1/7500 loss 31.806469 loss_att 32.502232 loss_ctc 30.183018 lr 0.00030024 rank 0\n",
            "2022-05-23 09:39:28,388 DEBUG CV Batch 1/0 loss 22.049383 loss_att 23.286709 loss_ctc 19.162292 history loss 20.752361 rank 0\n",
            "2022-05-23 09:39:39,955 DEBUG CV Batch 1/100 loss 30.031345 loss_att 31.664223 loss_ctc 26.221298 history loss 43.376209 rank 0\n",
            "2022-05-23 09:39:50,503 DEBUG CV Batch 1/200 loss 38.161957 loss_att 41.067482 loss_ctc 31.382399 history loss 43.748930 rank 0\n",
            "2022-05-23 09:40:01,406 DEBUG CV Batch 1/300 loss 41.333923 loss_att 45.021873 loss_ctc 32.728703 history loss 43.116195 rank 0\n",
            "2022-05-23 09:40:13,049 DEBUG CV Batch 1/400 loss 65.254593 loss_att 72.339493 loss_ctc 48.723164 history loss 42.029874 rank 0\n",
            "2022-05-23 09:40:24,988 DEBUG CV Batch 1/500 loss 25.092678 loss_att 26.698690 loss_ctc 21.345316 history loss 42.064368 rank 0\n",
            "2022-05-23 09:40:36,568 DEBUG CV Batch 1/600 loss 32.965290 loss_att 35.338287 loss_ctc 27.428295 history loss 42.078431 rank 0\n",
            "2022-05-23 09:40:47,497 DEBUG CV Batch 1/700 loss 34.875134 loss_att 38.139519 loss_ctc 27.258238 history loss 41.881842 rank 0\n",
            "2022-05-23 09:40:58,850 DEBUG CV Batch 1/800 loss 40.479828 loss_att 44.304939 loss_ctc 31.554571 history loss 41.966305 rank 0\n",
            "2022-05-23 09:41:10,386 INFO Epoch 1 CV info cv_loss 42.298866882771534\n",
            "2022-05-23 09:41:10,386 INFO Checkpoint: save to checkpoint exp/conformer/1.pt\n",
            "2022-05-23 09:41:10,725 INFO Epoch 2 TRAIN info lr 0.00030031999999999997\n",
            "2022-05-23 09:41:10,728 INFO using accumulate grad, new batch size is 4 times larger than before\n",
            "2022-05-23 09:41:36,033 DEBUG TRAIN Batch 2/0 loss 31.022114 loss_att 31.132084 loss_ctc 30.765518 lr 0.00030040 rank 0\n",
            "2022-05-23 09:42:05,923 DEBUG TRAIN Batch 2/100 loss 30.601284 loss_att 29.824072 loss_ctc 32.414780 lr 0.00030240 rank 0\n",
            "2022-05-23 09:42:35,792 DEBUG TRAIN Batch 2/200 loss 31.993855 loss_att 30.772961 loss_ctc 34.842606 lr 0.00030440 rank 0\n",
            "2022-05-23 09:43:05,086 DEBUG TRAIN Batch 2/300 loss 47.126930 loss_att 47.174282 loss_ctc 47.016441 lr 0.00030640 rank 0\n",
            "2022-05-23 09:43:35,761 DEBUG TRAIN Batch 2/400 loss 52.882362 loss_att 53.951469 loss_ctc 50.387772 lr 0.00030840 rank 0\n",
            "2022-05-23 09:44:06,709 DEBUG TRAIN Batch 2/500 loss 26.366417 loss_att 26.364441 loss_ctc 26.371029 lr 0.00031040 rank 0\n",
            "2022-05-23 09:44:36,767 DEBUG TRAIN Batch 2/600 loss 31.830074 loss_att 30.985325 loss_ctc 33.801155 lr 0.00031240 rank 0\n",
            "2022-05-23 09:45:06,899 DEBUG TRAIN Batch 2/700 loss 39.514839 loss_att 39.107719 loss_ctc 40.464787 lr 0.00031440 rank 0\n",
            "2022-05-23 09:45:36,438 DEBUG TRAIN Batch 2/800 loss 42.501877 loss_att 42.249123 loss_ctc 43.091637 lr 0.00031640 rank 0\n",
            "2022-05-23 09:46:05,778 DEBUG TRAIN Batch 2/900 loss 50.502750 loss_att 50.423641 loss_ctc 50.687347 lr 0.00031840 rank 0\n",
            "2022-05-23 09:46:36,276 DEBUG TRAIN Batch 2/1000 loss 21.874083 loss_att 20.591251 loss_ctc 24.867357 lr 0.00032040 rank 0\n",
            "2022-05-23 09:47:05,746 DEBUG TRAIN Batch 2/1100 loss 29.376478 loss_att 28.533882 loss_ctc 31.342537 lr 0.00032240 rank 0\n",
            "2022-05-23 09:47:35,698 DEBUG TRAIN Batch 2/1200 loss 31.791195 loss_att 30.637632 loss_ctc 34.482841 lr 0.00032440 rank 0\n",
            "2022-05-23 09:48:05,437 DEBUG TRAIN Batch 2/1300 loss 36.098976 loss_att 35.432259 loss_ctc 37.654652 lr 0.00032640 rank 0\n",
            "2022-05-23 09:48:35,811 DEBUG TRAIN Batch 2/1400 loss 45.591034 loss_att 44.643921 loss_ctc 47.800964 lr 0.00032840 rank 0\n",
            "2022-05-23 09:49:06,520 DEBUG TRAIN Batch 2/1500 loss 22.352015 loss_att 20.572197 loss_ctc 26.504925 lr 0.00033040 rank 0\n",
            "2022-05-23 09:49:36,073 DEBUG TRAIN Batch 2/1600 loss 25.537094 loss_att 24.102535 loss_ctc 28.884399 lr 0.00033240 rank 0\n",
            "2022-05-23 09:50:05,750 DEBUG TRAIN Batch 2/1700 loss 32.608177 loss_att 31.811928 loss_ctc 34.466095 lr 0.00033440 rank 0\n",
            "2022-05-23 09:50:35,744 DEBUG TRAIN Batch 2/1800 loss 33.739220 loss_att 31.948296 loss_ctc 37.918041 lr 0.00033640 rank 0\n",
            "2022-05-23 09:51:05,971 DEBUG TRAIN Batch 2/1900 loss 41.582386 loss_att 41.949211 loss_ctc 40.726467 lr 0.00033840 rank 0\n",
            "2022-05-23 09:51:37,059 DEBUG TRAIN Batch 2/2000 loss 16.900642 loss_att 15.778652 loss_ctc 19.518621 lr 0.00034040 rank 0\n",
            "2022-05-23 09:52:06,961 DEBUG TRAIN Batch 2/2100 loss 26.139029 loss_att 25.077061 loss_ctc 28.616955 lr 0.00034240 rank 0\n",
            "2022-05-23 09:52:36,620 DEBUG TRAIN Batch 2/2200 loss 29.462944 loss_att 28.632858 loss_ctc 31.399811 lr 0.00034440 rank 0\n",
            "2022-05-23 09:53:06,503 DEBUG TRAIN Batch 2/2300 loss 34.488716 loss_att 32.524490 loss_ctc 39.071915 lr 0.00034640 rank 0\n",
            "2022-05-23 09:53:36,682 DEBUG TRAIN Batch 2/2400 loss 44.273670 loss_att 43.894634 loss_ctc 45.158081 lr 0.00034840 rank 0\n",
            "2022-05-23 09:54:07,900 DEBUG TRAIN Batch 2/2500 loss 19.857162 loss_att 18.473320 loss_ctc 23.086128 lr 0.00035040 rank 0\n",
            "2022-05-23 09:54:37,443 DEBUG TRAIN Batch 2/2600 loss 31.761644 loss_att 30.413116 loss_ctc 34.908211 lr 0.00035240 rank 0\n",
            "2022-05-23 09:55:07,132 DEBUG TRAIN Batch 2/2700 loss 27.823925 loss_att 27.621181 loss_ctc 28.296993 lr 0.00035440 rank 0\n",
            "2022-05-23 09:55:37,042 DEBUG TRAIN Batch 2/2800 loss 39.637524 loss_att 38.347519 loss_ctc 42.647530 lr 0.00035640 rank 0\n",
            "2022-05-23 09:56:06,761 DEBUG TRAIN Batch 2/2900 loss 42.697811 loss_att 42.902550 loss_ctc 42.220093 lr 0.00035840 rank 0\n",
            "2022-05-23 09:56:37,766 DEBUG TRAIN Batch 2/3000 loss 21.806265 loss_att 20.155968 loss_ctc 25.656958 lr 0.00036040 rank 0\n",
            "2022-05-23 09:57:07,514 DEBUG TRAIN Batch 2/3100 loss 27.765213 loss_att 26.914227 loss_ctc 29.750851 lr 0.00036240 rank 0\n",
            "2022-05-23 09:57:36,937 DEBUG TRAIN Batch 2/3200 loss 25.445583 loss_att 24.000711 loss_ctc 28.816954 lr 0.00036440 rank 0\n",
            "2022-05-23 09:58:07,136 DEBUG TRAIN Batch 2/3300 loss 34.625015 loss_att 33.184776 loss_ctc 37.985573 lr 0.00036640 rank 0\n",
            "2022-05-23 09:58:37,108 DEBUG TRAIN Batch 2/3400 loss 39.523117 loss_att 38.165264 loss_ctc 42.691433 lr 0.00036840 rank 0\n",
            "2022-05-23 09:59:07,689 DEBUG TRAIN Batch 2/3500 loss 22.403675 loss_att 20.924793 loss_ctc 25.854403 lr 0.00037040 rank 0\n",
            "2022-05-23 09:59:36,892 DEBUG TRAIN Batch 2/3600 loss 24.853134 loss_att 23.051182 loss_ctc 29.057693 lr 0.00037240 rank 0\n",
            "2022-05-23 10:00:06,019 DEBUG TRAIN Batch 2/3700 loss 30.732189 loss_att 29.071419 loss_ctc 34.607319 lr 0.00037440 rank 0\n",
            "2022-05-23 10:00:36,024 DEBUG TRAIN Batch 2/3800 loss 30.897999 loss_att 29.705074 loss_ctc 33.681488 lr 0.00037640 rank 0\n",
            "2022-05-23 10:01:05,971 DEBUG TRAIN Batch 2/3900 loss 30.563583 loss_att 29.662743 loss_ctc 32.665543 lr 0.00037840 rank 0\n",
            "2022-05-23 10:01:36,387 DEBUG TRAIN Batch 2/4000 loss 23.457315 loss_att 21.041552 loss_ctc 29.094097 lr 0.00038040 rank 0\n",
            "2022-05-23 10:02:05,799 DEBUG TRAIN Batch 2/4100 loss 22.596821 loss_att 20.605675 loss_ctc 27.242825 lr 0.00038240 rank 0\n",
            "2022-05-23 10:02:35,147 DEBUG TRAIN Batch 2/4200 loss 32.942154 loss_att 31.312618 loss_ctc 36.744400 lr 0.00038440 rank 0\n",
            "2022-05-23 10:03:04,722 DEBUG TRAIN Batch 2/4300 loss 33.138992 loss_att 31.443436 loss_ctc 37.095284 lr 0.00038640 rank 0\n",
            "2022-05-23 10:03:34,803 DEBUG TRAIN Batch 2/4400 loss 33.970093 loss_att 32.371376 loss_ctc 37.700436 lr 0.00038840 rank 0\n",
            "2022-05-23 10:04:05,648 DEBUG TRAIN Batch 2/4500 loss 22.638241 loss_att 21.061136 loss_ctc 26.318150 lr 0.00039040 rank 0\n",
            "2022-05-23 10:04:35,761 DEBUG TRAIN Batch 2/4600 loss 23.688334 loss_att 22.301239 loss_ctc 26.924887 lr 0.00039240 rank 0\n",
            "2022-05-23 10:05:05,170 DEBUG TRAIN Batch 2/4700 loss 25.326805 loss_att 23.240494 loss_ctc 30.194866 lr 0.00039440 rank 0\n",
            "2022-05-23 10:05:35,216 DEBUG TRAIN Batch 2/4800 loss 31.833553 loss_att 30.724598 loss_ctc 34.421120 lr 0.00039640 rank 0\n",
            "2022-05-23 10:06:05,507 DEBUG TRAIN Batch 2/4900 loss 39.222527 loss_att 38.597488 loss_ctc 40.680943 lr 0.00039840 rank 0\n",
            "2022-05-23 10:06:36,105 DEBUG TRAIN Batch 2/5000 loss 13.973012 loss_att 12.411219 loss_ctc 17.617197 lr 0.00040040 rank 0\n",
            "2022-05-23 10:07:05,642 DEBUG TRAIN Batch 2/5100 loss 23.161060 loss_att 21.338770 loss_ctc 27.413074 lr 0.00040240 rank 0\n",
            "2022-05-23 10:07:35,106 DEBUG TRAIN Batch 2/5200 loss 25.918455 loss_att 24.118692 loss_ctc 30.117897 lr 0.00040440 rank 0\n",
            "2022-05-23 10:08:05,056 DEBUG TRAIN Batch 2/5300 loss 32.325592 loss_att 30.394770 loss_ctc 36.830845 lr 0.00040640 rank 0\n",
            "2022-05-23 10:08:35,662 DEBUG TRAIN Batch 2/5400 loss 36.708508 loss_att 35.405258 loss_ctc 39.749428 lr 0.00040840 rank 0\n",
            "2022-05-23 10:09:06,548 DEBUG TRAIN Batch 2/5500 loss 19.294697 loss_att 17.829723 loss_ctc 22.712965 lr 0.00041040 rank 0\n",
            "2022-05-23 10:09:35,857 DEBUG TRAIN Batch 2/5600 loss 24.567909 loss_att 22.492683 loss_ctc 29.410107 lr 0.00041240 rank 0\n",
            "2022-05-23 10:10:05,355 DEBUG TRAIN Batch 2/5700 loss 25.244095 loss_att 23.214245 loss_ctc 29.980410 lr 0.00041440 rank 0\n",
            "2022-05-23 10:10:35,517 DEBUG TRAIN Batch 2/5800 loss 28.167835 loss_att 26.059784 loss_ctc 33.086624 lr 0.00041640 rank 0\n",
            "2022-05-23 10:11:05,502 DEBUG TRAIN Batch 2/5900 loss 33.986420 loss_att 32.203041 loss_ctc 38.147644 lr 0.00041840 rank 0\n",
            "2022-05-23 10:11:36,410 DEBUG TRAIN Batch 2/6000 loss 17.911591 loss_att 16.177227 loss_ctc 21.958437 lr 0.00042040 rank 0\n",
            "2022-05-23 10:12:06,218 DEBUG TRAIN Batch 2/6100 loss 21.799900 loss_att 19.474667 loss_ctc 27.225441 lr 0.00042240 rank 0\n",
            "2022-05-23 10:12:35,883 DEBUG TRAIN Batch 2/6200 loss 24.680939 loss_att 23.640186 loss_ctc 27.109364 lr 0.00042440 rank 0\n",
            "2022-05-23 10:13:06,142 DEBUG TRAIN Batch 2/6300 loss 26.155796 loss_att 24.580219 loss_ctc 29.832136 lr 0.00042640 rank 0\n",
            "2022-05-23 10:13:36,201 DEBUG TRAIN Batch 2/6400 loss 36.476570 loss_att 35.655685 loss_ctc 38.391968 lr 0.00042840 rank 0\n",
            "2022-05-23 10:14:06,859 DEBUG TRAIN Batch 2/6500 loss 22.320627 loss_att 20.984432 loss_ctc 25.438416 lr 0.00043040 rank 0\n",
            "2022-05-23 10:14:36,586 DEBUG TRAIN Batch 2/6600 loss 18.627592 loss_att 17.598850 loss_ctc 21.027988 lr 0.00043240 rank 0\n",
            "2022-05-23 10:15:06,297 DEBUG TRAIN Batch 2/6700 loss 23.576241 loss_att 21.853107 loss_ctc 27.596886 lr 0.00043440 rank 0\n",
            "2022-05-23 10:15:36,451 DEBUG TRAIN Batch 2/6800 loss 30.705460 loss_att 28.639065 loss_ctc 35.527046 lr 0.00043640 rank 0\n",
            "2022-05-23 10:16:06,746 DEBUG TRAIN Batch 2/6900 loss 36.103424 loss_att 35.589420 loss_ctc 37.302773 lr 0.00043840 rank 0\n",
            "2022-05-23 10:16:38,036 DEBUG TRAIN Batch 2/7000 loss 17.594177 loss_att 15.901539 loss_ctc 21.543667 lr 0.00044040 rank 0\n",
            "2022-05-23 10:17:07,443 DEBUG TRAIN Batch 2/7100 loss 19.614103 loss_att 18.289310 loss_ctc 22.705288 lr 0.00044240 rank 0\n",
            "2022-05-23 10:17:37,458 DEBUG TRAIN Batch 2/7200 loss 19.418539 loss_att 18.340832 loss_ctc 21.933191 lr 0.00044440 rank 0\n",
            "2022-05-23 10:18:07,739 DEBUG TRAIN Batch 2/7300 loss 30.267826 loss_att 29.145575 loss_ctc 32.886417 lr 0.00044640 rank 0\n",
            "2022-05-23 10:18:38,450 DEBUG TRAIN Batch 2/7400 loss 28.360077 loss_att 26.327950 loss_ctc 33.101707 lr 0.00044840 rank 0\n",
            "2022-05-23 10:19:09,602 DEBUG TRAIN Batch 2/7500 loss 19.722885 loss_att 17.762169 loss_ctc 24.297890 lr 0.00045040 rank 0\n",
            "2022-05-23 10:19:15,630 DEBUG CV Batch 2/0 loss 9.192777 loss_att 8.202751 loss_ctc 11.502837 history loss 8.652025 rank 0\n",
            "2022-05-23 10:19:27,454 DEBUG CV Batch 2/100 loss 12.656967 loss_att 11.809614 loss_ctc 14.634128 history loss 20.579055 rank 0\n",
            "2022-05-23 10:19:38,214 DEBUG CV Batch 2/200 loss 19.414743 loss_att 18.894165 loss_ctc 20.629429 history loss 20.808648 rank 0\n",
            "2022-05-23 10:19:49,303 DEBUG CV Batch 2/300 loss 17.321030 loss_att 16.892815 loss_ctc 18.320194 history loss 20.171196 rank 0\n",
            "2022-05-23 10:20:01,340 DEBUG CV Batch 2/400 loss 29.700264 loss_att 29.604401 loss_ctc 29.923946 history loss 19.211725 rank 0\n",
            "2022-05-23 10:20:13,725 DEBUG CV Batch 2/500 loss 12.984428 loss_att 11.977940 loss_ctc 15.332903 history loss 19.146025 rank 0\n",
            "2022-05-23 10:20:25,738 DEBUG CV Batch 2/600 loss 15.375286 loss_att 14.589935 loss_ctc 17.207771 history loss 19.138933 rank 0\n",
            "2022-05-23 10:20:37,089 DEBUG CV Batch 2/700 loss 14.477290 loss_att 13.762943 loss_ctc 16.144100 history loss 18.959266 rank 0\n",
            "2022-05-23 10:20:48,895 DEBUG CV Batch 2/800 loss 16.567898 loss_att 16.213863 loss_ctc 17.393978 history loss 18.936901 rank 0\n",
            "2022-05-23 10:21:00,622 INFO Epoch 2 CV info cv_loss 19.050819897540176\n",
            "2022-05-23 10:21:00,622 INFO Checkpoint: save to checkpoint exp/conformer/2.pt\n",
            "2022-05-23 10:21:00,968 INFO Epoch 3 TRAIN info lr 0.00045047999999999996\n",
            "2022-05-23 10:21:00,970 INFO using accumulate grad, new batch size is 4 times larger than before\n",
            "2022-05-23 10:21:27,122 DEBUG TRAIN Batch 3/0 loss 18.702871 loss_att 16.377186 loss_ctc 24.129467 lr 0.00045056 rank 0\n",
            "2022-05-23 10:21:57,067 DEBUG TRAIN Batch 3/100 loss 27.842049 loss_att 25.256241 loss_ctc 33.875595 lr 0.00045256 rank 0\n",
            "2022-05-23 10:22:27,046 DEBUG TRAIN Batch 3/200 loss 28.240269 loss_att 25.745115 loss_ctc 34.062294 lr 0.00045456 rank 0\n",
            "2022-05-23 10:22:56,865 DEBUG TRAIN Batch 3/300 loss 31.345472 loss_att 29.472466 loss_ctc 35.715820 lr 0.00045656 rank 0\n",
            "2022-05-23 10:23:27,133 DEBUG TRAIN Batch 3/400 loss 38.844952 loss_att 36.384392 loss_ctc 44.586258 lr 0.00045856 rank 0\n",
            "2022-05-23 10:23:58,878 DEBUG TRAIN Batch 3/500 loss 23.900297 loss_att 22.180260 loss_ctc 27.913717 lr 0.00046056 rank 0\n",
            "2022-05-23 10:24:28,073 DEBUG TRAIN Batch 3/600 loss 21.541485 loss_att 20.009645 loss_ctc 25.115778 lr 0.00046256 rank 0\n",
            "2022-05-23 10:24:57,830 DEBUG TRAIN Batch 3/700 loss 23.096060 loss_att 21.069750 loss_ctc 27.824114 lr 0.00046456 rank 0\n",
            "2022-05-23 10:25:27,592 DEBUG TRAIN Batch 3/800 loss 26.028427 loss_att 24.795418 loss_ctc 28.905449 lr 0.00046656 rank 0\n",
            "2022-05-23 10:25:57,697 DEBUG TRAIN Batch 3/900 loss 32.352127 loss_att 30.319592 loss_ctc 37.094707 lr 0.00046856 rank 0\n",
            "2022-05-23 10:26:28,666 DEBUG TRAIN Batch 3/1000 loss 16.318005 loss_att 15.182524 loss_ctc 18.967459 lr 0.00047056 rank 0\n",
            "2022-05-23 10:26:57,893 DEBUG TRAIN Batch 3/1100 loss 20.372177 loss_att 18.483315 loss_ctc 24.779526 lr 0.00047256 rank 0\n",
            "2022-05-23 10:27:27,846 DEBUG TRAIN Batch 3/1200 loss 21.728403 loss_att 20.302963 loss_ctc 25.054426 lr 0.00047456 rank 0\n",
            "2022-05-23 10:27:57,517 DEBUG TRAIN Batch 3/1300 loss 24.662170 loss_att 23.621269 loss_ctc 27.090946 lr 0.00047656 rank 0\n",
            "2022-05-23 10:28:27,719 DEBUG TRAIN Batch 3/1400 loss 27.201969 loss_att 24.961067 loss_ctc 32.430744 lr 0.00047856 rank 0\n",
            "2022-05-23 10:28:58,557 DEBUG TRAIN Batch 3/1500 loss 18.521002 loss_att 16.465342 loss_ctc 23.317543 lr 0.00048056 rank 0\n",
            "2022-05-23 10:29:28,214 DEBUG TRAIN Batch 3/1600 loss 19.443251 loss_att 17.927109 loss_ctc 22.980915 lr 0.00048256 rank 0\n",
            "2022-05-23 10:29:57,662 DEBUG TRAIN Batch 3/1700 loss 21.547310 loss_att 19.869610 loss_ctc 25.461941 lr 0.00048456 rank 0\n",
            "2022-05-23 10:30:27,422 DEBUG TRAIN Batch 3/1800 loss 23.091866 loss_att 20.722649 loss_ctc 28.620039 lr 0.00048656 rank 0\n",
            "2022-05-23 10:30:57,212 DEBUG TRAIN Batch 3/1900 loss 34.399384 loss_att 32.790211 loss_ctc 38.154110 lr 0.00048856 rank 0\n",
            "2022-05-23 10:31:28,605 DEBUG TRAIN Batch 3/2000 loss 22.283699 loss_att 20.733122 loss_ctc 25.901712 lr 0.00049056 rank 0\n",
            "2022-05-23 10:31:58,200 DEBUG TRAIN Batch 3/2100 loss 21.131649 loss_att 20.232424 loss_ctc 23.229843 lr 0.00049256 rank 0\n",
            "2022-05-23 10:32:27,853 DEBUG TRAIN Batch 3/2200 loss 23.400309 loss_att 22.159351 loss_ctc 26.295876 lr 0.00049456 rank 0\n",
            "2022-05-23 10:32:58,222 DEBUG TRAIN Batch 3/2300 loss 23.658041 loss_att 22.379623 loss_ctc 26.641014 lr 0.00049656 rank 0\n",
            "2022-05-23 10:33:28,055 DEBUG TRAIN Batch 3/2400 loss 26.390200 loss_att 24.917028 loss_ctc 29.827599 lr 0.00049856 rank 0\n",
            "2022-05-23 10:33:59,076 DEBUG TRAIN Batch 3/2500 loss 17.038647 loss_att 15.627727 loss_ctc 20.330791 lr 0.00050056 rank 0\n",
            "2022-05-23 10:34:28,577 DEBUG TRAIN Batch 3/2600 loss 16.166563 loss_att 14.686824 loss_ctc 19.619287 lr 0.00050256 rank 0\n",
            "2022-05-23 10:34:58,351 DEBUG TRAIN Batch 3/2700 loss 23.585220 loss_att 21.741482 loss_ctc 27.887280 lr 0.00050456 rank 0\n",
            "2022-05-23 10:35:28,293 DEBUG TRAIN Batch 3/2800 loss 24.358938 loss_att 22.704887 loss_ctc 28.218388 lr 0.00050656 rank 0\n",
            "2022-05-23 10:35:58,156 DEBUG TRAIN Batch 3/2900 loss 28.625645 loss_att 27.206188 loss_ctc 31.937714 lr 0.00050856 rank 0\n",
            "2022-05-23 10:36:29,096 DEBUG TRAIN Batch 3/3000 loss 16.456070 loss_att 15.310284 loss_ctc 19.129572 lr 0.00051056 rank 0\n",
            "2022-05-23 10:36:58,507 DEBUG TRAIN Batch 3/3100 loss 18.425018 loss_att 17.100460 loss_ctc 21.515657 lr 0.00051256 rank 0\n",
            "2022-05-23 10:37:28,302 DEBUG TRAIN Batch 3/3200 loss 16.255663 loss_att 14.969260 loss_ctc 19.257267 lr 0.00051456 rank 0\n",
            "2022-05-23 10:37:58,147 DEBUG TRAIN Batch 3/3300 loss 24.574730 loss_att 23.184635 loss_ctc 27.818283 lr 0.00051656 rank 0\n",
            "2022-05-23 10:38:28,485 DEBUG TRAIN Batch 3/3400 loss 27.965897 loss_att 25.392319 loss_ctc 33.970917 lr 0.00051856 rank 0\n",
            "2022-05-23 10:38:58,759 DEBUG TRAIN Batch 3/3500 loss 14.348732 loss_att 12.855232 loss_ctc 17.833565 lr 0.00052056 rank 0\n",
            "2022-05-23 10:39:28,382 DEBUG TRAIN Batch 3/3600 loss 18.356361 loss_att 17.130772 loss_ctc 21.216070 lr 0.00052256 rank 0\n",
            "2022-05-23 10:39:57,929 DEBUG TRAIN Batch 3/3700 loss 19.597534 loss_att 18.028713 loss_ctc 23.258114 lr 0.00052456 rank 0\n",
            "2022-05-23 10:40:27,630 DEBUG TRAIN Batch 3/3800 loss 21.318367 loss_att 20.700794 loss_ctc 22.759373 lr 0.00052656 rank 0\n",
            "2022-05-23 10:40:58,082 DEBUG TRAIN Batch 3/3900 loss 25.187878 loss_att 23.450874 loss_ctc 29.240889 lr 0.00052856 rank 0\n",
            "2022-05-23 10:41:29,029 DEBUG TRAIN Batch 3/4000 loss 18.953741 loss_att 16.875355 loss_ctc 23.803308 lr 0.00053056 rank 0\n",
            "2022-05-23 10:41:58,816 DEBUG TRAIN Batch 3/4100 loss 17.087452 loss_att 15.604966 loss_ctc 20.546585 lr 0.00053256 rank 0\n",
            "2022-05-23 10:42:28,294 DEBUG TRAIN Batch 3/4200 loss 19.946760 loss_att 18.239456 loss_ctc 23.930470 lr 0.00053456 rank 0\n",
            "2022-05-23 10:42:58,003 DEBUG TRAIN Batch 3/4300 loss 25.609131 loss_att 24.743267 loss_ctc 27.629482 lr 0.00053656 rank 0\n",
            "2022-05-23 10:43:28,077 DEBUG TRAIN Batch 3/4400 loss 26.125397 loss_att 25.590248 loss_ctc 27.374071 lr 0.00053856 rank 0\n",
            "2022-05-23 10:43:59,399 DEBUG TRAIN Batch 3/4500 loss 14.065619 loss_att 12.835025 loss_ctc 16.937008 lr 0.00054056 rank 0\n",
            "2022-05-23 10:44:28,886 DEBUG TRAIN Batch 3/4600 loss 15.539087 loss_att 14.306890 loss_ctc 18.414215 lr 0.00054256 rank 0\n",
            "2022-05-23 10:44:58,533 DEBUG TRAIN Batch 3/4700 loss 18.939957 loss_att 16.483341 loss_ctc 24.672060 lr 0.00054456 rank 0\n",
            "2022-05-23 10:45:28,537 DEBUG TRAIN Batch 3/4800 loss 26.260981 loss_att 24.799656 loss_ctc 29.670738 lr 0.00054656 rank 0\n",
            "2022-05-23 10:45:58,776 DEBUG TRAIN Batch 3/4900 loss 29.610683 loss_att 27.612041 loss_ctc 34.274181 lr 0.00054856 rank 0\n",
            "2022-05-23 10:46:29,989 DEBUG TRAIN Batch 3/5000 loss 12.344797 loss_att 11.161840 loss_ctc 15.105030 lr 0.00055056 rank 0\n",
            "2022-05-23 10:46:59,556 DEBUG TRAIN Batch 3/5100 loss 16.230038 loss_att 14.954686 loss_ctc 19.205856 lr 0.00055256 rank 0\n",
            "2022-05-23 10:47:29,481 DEBUG TRAIN Batch 3/5200 loss 17.912968 loss_att 16.418468 loss_ctc 21.400129 lr 0.00055456 rank 0\n",
            "2022-05-23 10:47:59,342 DEBUG TRAIN Batch 3/5300 loss 23.000862 loss_att 21.698261 loss_ctc 26.040260 lr 0.00055656 rank 0\n",
            "2022-05-23 10:48:29,624 DEBUG TRAIN Batch 3/5400 loss 28.549751 loss_att 26.914574 loss_ctc 32.365166 lr 0.00055856 rank 0\n",
            "2022-05-23 10:49:00,299 DEBUG TRAIN Batch 3/5500 loss 20.962574 loss_att 19.639511 loss_ctc 24.049717 lr 0.00056056 rank 0\n",
            "2022-05-23 10:49:30,259 DEBUG TRAIN Batch 3/5600 loss 19.830637 loss_att 18.528000 loss_ctc 22.870121 lr 0.00056256 rank 0\n",
            "2022-05-23 10:49:59,924 DEBUG TRAIN Batch 3/5700 loss 17.451145 loss_att 15.564318 loss_ctc 21.853745 lr 0.00056456 rank 0\n",
            "2022-05-23 10:50:30,213 DEBUG TRAIN Batch 3/5800 loss 23.108784 loss_att 21.476545 loss_ctc 26.917343 lr 0.00056656 rank 0\n",
            "2022-05-23 10:51:00,522 DEBUG TRAIN Batch 3/5900 loss 29.891460 loss_att 28.295795 loss_ctc 33.614677 lr 0.00056856 rank 0\n",
            "2022-05-23 10:51:31,427 DEBUG TRAIN Batch 3/6000 loss 17.938435 loss_att 16.475370 loss_ctc 21.352255 lr 0.00057056 rank 0\n",
            "2022-05-23 10:52:00,745 DEBUG TRAIN Batch 3/6100 loss 19.309166 loss_att 17.484734 loss_ctc 23.566177 lr 0.00057256 rank 0\n",
            "2022-05-23 10:52:30,653 DEBUG TRAIN Batch 3/6200 loss 15.672536 loss_att 14.623731 loss_ctc 18.119747 lr 0.00057456 rank 0\n",
            "2022-05-23 10:53:00,438 DEBUG TRAIN Batch 3/6300 loss 23.205971 loss_att 22.233206 loss_ctc 25.475754 lr 0.00057656 rank 0\n",
            "2022-05-23 10:53:30,410 DEBUG TRAIN Batch 3/6400 loss 26.602993 loss_att 25.200350 loss_ctc 29.875828 lr 0.00057856 rank 0\n",
            "2022-05-23 10:54:00,900 DEBUG TRAIN Batch 3/6500 loss 17.708277 loss_att 15.702995 loss_ctc 22.387264 lr 0.00058056 rank 0\n",
            "2022-05-23 10:54:31,123 DEBUG TRAIN Batch 3/6600 loss 15.100514 loss_att 13.343895 loss_ctc 19.199295 lr 0.00058256 rank 0\n",
            "2022-05-23 10:55:00,889 DEBUG TRAIN Batch 3/6700 loss 22.651617 loss_att 21.055378 loss_ctc 26.376171 lr 0.00058456 rank 0\n",
            "2022-05-23 10:55:30,842 DEBUG TRAIN Batch 3/6800 loss 31.225378 loss_att 29.331211 loss_ctc 35.645103 lr 0.00058656 rank 0\n",
            "2022-05-23 10:56:01,061 DEBUG TRAIN Batch 3/6900 loss 34.483223 loss_att 32.645683 loss_ctc 38.770813 lr 0.00058856 rank 0\n",
            "2022-05-23 10:56:31,600 DEBUG TRAIN Batch 3/7000 loss 17.363415 loss_att 15.919762 loss_ctc 20.731937 lr 0.00059056 rank 0\n",
            "2022-05-23 10:57:00,797 DEBUG TRAIN Batch 3/7100 loss 22.741333 loss_att 20.396494 loss_ctc 28.212620 lr 0.00059256 rank 0\n",
            "2022-05-23 10:57:30,568 DEBUG TRAIN Batch 3/7200 loss 21.452757 loss_att 19.318840 loss_ctc 26.431894 lr 0.00059456 rank 0\n",
            "2022-05-23 10:58:00,402 DEBUG TRAIN Batch 3/7300 loss 21.210985 loss_att 19.916042 loss_ctc 24.232517 lr 0.00059656 rank 0\n",
            "2022-05-23 10:58:30,756 DEBUG TRAIN Batch 3/7400 loss 23.198473 loss_att 23.137695 loss_ctc 23.340286 lr 0.00059856 rank 0\n",
            "2022-05-23 10:59:01,363 DEBUG TRAIN Batch 3/7500 loss 17.967468 loss_att 16.287588 loss_ctc 21.887188 lr 0.00060056 rank 0\n",
            "2022-05-23 10:59:07,649 DEBUG CV Batch 3/0 loss 8.977388 loss_att 8.271323 loss_ctc 10.624872 history loss 8.449307 rank 0\n",
            "2022-05-23 10:59:19,084 DEBUG CV Batch 3/100 loss 10.272280 loss_att 9.810381 loss_ctc 11.350044 history loss 15.726612 rank 0\n",
            "2022-05-23 10:59:29,585 DEBUG CV Batch 3/200 loss 13.125334 loss_att 12.923732 loss_ctc 13.595739 history loss 16.048388 rank 0\n",
            "2022-05-23 10:59:40,815 DEBUG CV Batch 3/300 loss 12.969641 loss_att 12.502645 loss_ctc 14.059294 history loss 15.310236 rank 0\n",
            "2022-05-23 10:59:52,857 DEBUG CV Batch 3/400 loss 20.797829 loss_att 20.991695 loss_ctc 20.345470 history loss 14.299743 rank 0\n",
            "2022-05-23 11:00:05,083 DEBUG CV Batch 3/500 loss 9.638660 loss_att 8.684500 loss_ctc 11.865037 history loss 14.144740 rank 0\n",
            "2022-05-23 11:00:16,943 DEBUG CV Batch 3/600 loss 11.504749 loss_att 11.124122 loss_ctc 12.392879 history loss 14.071685 rank 0\n",
            "2022-05-23 11:00:28,151 DEBUG CV Batch 3/700 loss 11.042061 loss_att 10.771561 loss_ctc 11.673229 history loss 13.875090 rank 0\n",
            "2022-05-23 11:00:39,777 DEBUG CV Batch 3/800 loss 11.686233 loss_att 11.526419 loss_ctc 12.059131 history loss 13.828444 rank 0\n",
            "2022-05-23 11:00:51,410 INFO Epoch 3 CV info cv_loss 13.851862598748552\n",
            "2022-05-23 11:00:51,410 INFO Checkpoint: save to checkpoint exp/conformer/3.pt\n",
            "2022-05-23 11:00:51,791 INFO Epoch 4 TRAIN info lr 0.0006006399999999999\n",
            "2022-05-23 11:00:51,794 INFO using accumulate grad, new batch size is 4 times larger than before\n",
            "2022-05-23 11:01:17,439 DEBUG TRAIN Batch 4/0 loss 15.916878 loss_att 14.640510 loss_ctc 18.895069 lr 0.00060072 rank 0\n",
            "2022-05-23 11:01:47,690 DEBUG TRAIN Batch 4/100 loss 14.660342 loss_att 13.147165 loss_ctc 18.191090 lr 0.00060272 rank 0\n",
            "2022-05-23 11:02:17,432 DEBUG TRAIN Batch 4/200 loss 19.905373 loss_att 17.676178 loss_ctc 25.106825 lr 0.00060472 rank 0\n",
            "2022-05-23 11:02:46,736 DEBUG TRAIN Batch 4/300 loss 16.647625 loss_att 15.333605 loss_ctc 19.713675 lr 0.00060672 rank 0\n",
            "2022-05-23 11:03:17,118 DEBUG TRAIN Batch 4/400 loss 24.889761 loss_att 23.165112 loss_ctc 28.913942 lr 0.00060872 rank 0\n",
            "2022-05-23 11:03:47,707 DEBUG TRAIN Batch 4/500 loss 17.432447 loss_att 15.113071 loss_ctc 22.844322 lr 0.00061072 rank 0\n",
            "2022-05-23 11:04:16,554 DEBUG TRAIN Batch 4/600 loss 13.104055 loss_att 11.899127 loss_ctc 15.915557 lr 0.00061272 rank 0\n",
            "2022-05-23 11:04:46,325 DEBUG TRAIN Batch 4/700 loss 23.895458 loss_att 21.784618 loss_ctc 28.820753 lr 0.00061472 rank 0\n",
            "2022-05-23 11:05:16,302 DEBUG TRAIN Batch 4/800 loss 21.866356 loss_att 20.895546 loss_ctc 24.131577 lr 0.00061672 rank 0\n",
            "2022-05-23 11:05:46,402 DEBUG TRAIN Batch 4/900 loss 26.433701 loss_att 25.436983 loss_ctc 28.759373 lr 0.00061872 rank 0\n",
            "2022-05-23 11:06:17,587 DEBUG TRAIN Batch 4/1000 loss 17.848211 loss_att 16.543076 loss_ctc 20.893528 lr 0.00062072 rank 0\n",
            "2022-05-23 11:06:47,240 DEBUG TRAIN Batch 4/1100 loss 19.260864 loss_att 17.673397 loss_ctc 22.964952 lr 0.00062272 rank 0\n",
            "2022-05-23 11:07:16,666 DEBUG TRAIN Batch 4/1200 loss 16.939787 loss_att 15.737871 loss_ctc 19.744257 lr 0.00062472 rank 0\n",
            "2022-05-23 11:07:46,192 DEBUG TRAIN Batch 4/1300 loss 14.438139 loss_att 13.937386 loss_ctc 15.606566 lr 0.00062672 rank 0\n",
            "2022-05-23 11:08:15,729 DEBUG TRAIN Batch 4/1400 loss 24.113806 loss_att 22.317520 loss_ctc 28.305138 lr 0.00062872 rank 0\n",
            "2022-05-23 11:08:46,369 DEBUG TRAIN Batch 4/1500 loss 18.938652 loss_att 16.777596 loss_ctc 23.981113 lr 0.00063072 rank 0\n",
            "2022-05-23 11:09:16,031 DEBUG TRAIN Batch 4/1600 loss 12.263247 loss_att 11.528090 loss_ctc 13.978613 lr 0.00063272 rank 0\n",
            "2022-05-23 11:09:45,582 DEBUG TRAIN Batch 4/1700 loss 17.489098 loss_att 16.441273 loss_ctc 19.934025 lr 0.00063472 rank 0\n",
            "2022-05-23 11:10:15,201 DEBUG TRAIN Batch 4/1800 loss 22.451078 loss_att 20.742878 loss_ctc 26.436876 lr 0.00063672 rank 0\n",
            "2022-05-23 11:10:45,285 DEBUG TRAIN Batch 4/1900 loss 27.448868 loss_att 26.150517 loss_ctc 30.478352 lr 0.00063872 rank 0\n",
            "2022-05-23 11:11:16,487 DEBUG TRAIN Batch 4/2000 loss 13.987798 loss_att 12.540990 loss_ctc 17.363682 lr 0.00064072 rank 0\n",
            "2022-05-23 11:11:45,688 DEBUG TRAIN Batch 4/2100 loss 14.831396 loss_att 13.905618 loss_ctc 16.991543 lr 0.00064272 rank 0\n",
            "2022-05-23 11:12:15,217 DEBUG TRAIN Batch 4/2200 loss 14.142927 loss_att 12.515087 loss_ctc 17.941219 lr 0.00064472 rank 0\n",
            "2022-05-23 11:12:45,229 DEBUG TRAIN Batch 4/2300 loss 20.233238 loss_att 18.846119 loss_ctc 23.469852 lr 0.00064672 rank 0\n",
            "2022-05-23 11:13:15,589 DEBUG TRAIN Batch 4/2400 loss 19.629311 loss_att 18.702087 loss_ctc 21.792831 lr 0.00064872 rank 0\n",
            "2022-05-23 11:13:45,822 DEBUG TRAIN Batch 4/2500 loss 15.684342 loss_att 14.691499 loss_ctc 18.000977 lr 0.00065072 rank 0\n",
            "2022-05-23 11:14:15,547 DEBUG TRAIN Batch 4/2600 loss 16.879660 loss_att 15.875639 loss_ctc 19.222374 lr 0.00065272 rank 0\n",
            "2022-05-23 11:14:45,267 DEBUG TRAIN Batch 4/2700 loss 18.122881 loss_att 16.654772 loss_ctc 21.548466 lr 0.00065472 rank 0\n",
            "2022-05-23 11:15:15,061 DEBUG TRAIN Batch 4/2800 loss 20.586494 loss_att 19.558270 loss_ctc 22.985687 lr 0.00065672 rank 0\n",
            "2022-05-23 11:15:44,863 DEBUG TRAIN Batch 4/2900 loss 22.443811 loss_att 20.793352 loss_ctc 26.294884 lr 0.00065872 rank 0\n",
            "2022-05-23 11:16:16,223 DEBUG TRAIN Batch 4/3000 loss 14.178658 loss_att 12.676210 loss_ctc 17.684368 lr 0.00066072 rank 0\n",
            "2022-05-23 11:16:45,925 DEBUG TRAIN Batch 4/3100 loss 17.786974 loss_att 15.897788 loss_ctc 22.195072 lr 0.00066272 rank 0\n",
            "2022-05-23 11:17:15,526 DEBUG TRAIN Batch 4/3200 loss 17.973593 loss_att 16.570871 loss_ctc 21.246613 lr 0.00066472 rank 0\n",
            "2022-05-23 11:17:45,053 DEBUG TRAIN Batch 4/3300 loss 21.191771 loss_att 19.597982 loss_ctc 24.910610 lr 0.00066672 rank 0\n",
            "2022-05-23 11:18:15,486 DEBUG TRAIN Batch 4/3400 loss 19.737228 loss_att 18.210396 loss_ctc 23.299835 lr 0.00066872 rank 0\n",
            "2022-05-23 11:18:46,708 DEBUG TRAIN Batch 4/3500 loss 13.996656 loss_att 12.508373 loss_ctc 17.469315 lr 0.00067072 rank 0\n",
            "2022-05-23 11:19:16,213 DEBUG TRAIN Batch 4/3600 loss 12.568097 loss_att 11.392431 loss_ctc 15.311316 lr 0.00067272 rank 0\n",
            "2022-05-23 11:19:45,556 DEBUG TRAIN Batch 4/3700 loss 16.988558 loss_att 15.979662 loss_ctc 19.342648 lr 0.00067472 rank 0\n",
            "2022-05-23 11:20:15,696 DEBUG TRAIN Batch 4/3800 loss 16.935884 loss_att 15.964822 loss_ctc 19.201698 lr 0.00067672 rank 0\n",
            "2022-05-23 11:20:45,456 DEBUG TRAIN Batch 4/3900 loss 20.298182 loss_att 18.977566 loss_ctc 23.379616 lr 0.00067872 rank 0\n",
            "2022-05-23 11:21:16,787 DEBUG TRAIN Batch 4/4000 loss 13.650620 loss_att 12.634627 loss_ctc 16.021271 lr 0.00068072 rank 0\n",
            "2022-05-23 11:21:46,555 DEBUG TRAIN Batch 4/4100 loss 15.967905 loss_att 14.186485 loss_ctc 20.124554 lr 0.00068272 rank 0\n",
            "2022-05-23 11:22:15,997 DEBUG TRAIN Batch 4/4200 loss 14.040788 loss_att 12.062881 loss_ctc 18.655903 lr 0.00068472 rank 0\n",
            "2022-05-23 11:22:45,938 DEBUG TRAIN Batch 4/4300 loss 19.532345 loss_att 18.234217 loss_ctc 22.561314 lr 0.00068672 rank 0\n",
            "2022-05-23 11:23:16,349 DEBUG TRAIN Batch 4/4400 loss 23.490269 loss_att 21.970451 loss_ctc 27.036507 lr 0.00068872 rank 0\n",
            "2022-05-23 11:23:47,150 DEBUG TRAIN Batch 4/4500 loss 13.681816 loss_att 12.058400 loss_ctc 17.469788 lr 0.00069072 rank 0\n",
            "2022-05-23 11:24:16,490 DEBUG TRAIN Batch 4/4600 loss 14.494067 loss_att 12.957022 loss_ctc 18.080507 lr 0.00069272 rank 0\n",
            "2022-05-23 11:24:46,254 DEBUG TRAIN Batch 4/4700 loss 16.833736 loss_att 15.805368 loss_ctc 19.233259 lr 0.00069472 rank 0\n",
            "2022-05-23 11:25:16,060 DEBUG TRAIN Batch 4/4800 loss 16.327263 loss_att 15.544824 loss_ctc 18.152954 lr 0.00069672 rank 0\n",
            "2022-05-23 11:25:46,558 DEBUG TRAIN Batch 4/4900 loss 25.202408 loss_att 24.891563 loss_ctc 25.927715 lr 0.00069872 rank 0\n",
            "2022-05-23 11:26:17,364 DEBUG TRAIN Batch 4/5000 loss 11.579554 loss_att 10.680019 loss_ctc 13.678466 lr 0.00070072 rank 0\n",
            "2022-05-23 11:26:46,564 DEBUG TRAIN Batch 4/5100 loss 16.654440 loss_att 15.048678 loss_ctc 20.401215 lr 0.00070272 rank 0\n",
            "2022-05-23 11:27:16,521 DEBUG TRAIN Batch 4/5200 loss 14.357623 loss_att 13.663065 loss_ctc 15.978259 lr 0.00070472 rank 0\n",
            "2022-05-23 11:27:46,504 DEBUG TRAIN Batch 4/5300 loss 18.725285 loss_att 17.163126 loss_ctc 22.370319 lr 0.00070672 rank 0\n",
            "2022-05-23 11:28:16,462 DEBUG TRAIN Batch 4/5400 loss 22.233961 loss_att 21.654636 loss_ctc 23.585720 lr 0.00070872 rank 0\n",
            "2022-05-23 11:28:46,998 DEBUG TRAIN Batch 4/5500 loss 11.087234 loss_att 9.696955 loss_ctc 14.331223 lr 0.00071072 rank 0\n",
            "2022-05-23 11:29:16,854 DEBUG TRAIN Batch 4/5600 loss 12.468163 loss_att 11.140294 loss_ctc 15.566522 lr 0.00071272 rank 0\n",
            "2022-05-23 11:29:46,763 DEBUG TRAIN Batch 4/5700 loss 12.301560 loss_att 11.615547 loss_ctc 13.902257 lr 0.00071472 rank 0\n",
            "2022-05-23 11:30:17,266 DEBUG TRAIN Batch 4/5800 loss 16.184521 loss_att 14.747142 loss_ctc 19.538406 lr 0.00071672 rank 0\n",
            "2022-05-23 11:30:47,399 DEBUG TRAIN Batch 4/5900 loss 21.607887 loss_att 21.174267 loss_ctc 22.619667 lr 0.00071872 rank 0\n",
            "2022-05-23 11:31:18,305 DEBUG TRAIN Batch 4/6000 loss 13.421612 loss_att 12.219818 loss_ctc 16.225798 lr 0.00072072 rank 0\n",
            "2022-05-23 11:31:47,799 DEBUG TRAIN Batch 4/6100 loss 14.954445 loss_att 13.900075 loss_ctc 17.414642 lr 0.00072272 rank 0\n",
            "2022-05-23 11:32:17,569 DEBUG TRAIN Batch 4/6200 loss 17.721939 loss_att 16.091766 loss_ctc 21.525675 lr 0.00072472 rank 0\n",
            "2022-05-23 11:32:47,390 DEBUG TRAIN Batch 4/6300 loss 20.726116 loss_att 19.153015 loss_ctc 24.396687 lr 0.00072672 rank 0\n",
            "2022-05-23 11:33:17,207 DEBUG TRAIN Batch 4/6400 loss 24.114319 loss_att 23.747078 loss_ctc 24.971214 lr 0.00072872 rank 0\n",
            "2022-05-23 11:33:48,528 DEBUG TRAIN Batch 4/6500 loss 13.925100 loss_att 12.166353 loss_ctc 18.028845 lr 0.00073072 rank 0\n",
            "2022-05-23 11:34:18,398 DEBUG TRAIN Batch 4/6600 loss 14.881945 loss_att 13.889268 loss_ctc 17.198189 lr 0.00073272 rank 0\n",
            "2022-05-23 11:34:48,284 DEBUG TRAIN Batch 4/6700 loss 13.783668 loss_att 13.065057 loss_ctc 15.460426 lr 0.00073472 rank 0\n",
            "2022-05-23 11:35:18,068 DEBUG TRAIN Batch 4/6800 loss 19.554607 loss_att 18.189844 loss_ctc 22.739056 lr 0.00073672 rank 0\n",
            "2022-05-23 11:35:48,197 DEBUG TRAIN Batch 4/6900 loss 23.319733 loss_att 22.354122 loss_ctc 25.572823 lr 0.00073872 rank 0\n",
            "2022-05-23 11:36:19,775 DEBUG TRAIN Batch 4/7000 loss 11.676610 loss_att 10.807222 loss_ctc 13.705181 lr 0.00074072 rank 0\n",
            "2022-05-23 11:36:48,708 DEBUG TRAIN Batch 4/7100 loss 14.562822 loss_att 13.281044 loss_ctc 17.553637 lr 0.00074272 rank 0\n",
            "2022-05-23 11:37:18,382 DEBUG TRAIN Batch 4/7200 loss 14.548559 loss_att 13.271698 loss_ctc 17.527903 lr 0.00074472 rank 0\n",
            "2022-05-23 11:37:47,935 DEBUG TRAIN Batch 4/7300 loss 20.327425 loss_att 18.876579 loss_ctc 23.712732 lr 0.00074672 rank 0\n",
            "2022-05-23 11:38:18,010 DEBUG TRAIN Batch 4/7400 loss 20.754101 loss_att 19.135929 loss_ctc 24.529831 lr 0.00074872 rank 0\n",
            "2022-05-23 11:38:48,813 DEBUG TRAIN Batch 4/7500 loss 16.067986 loss_att 14.789778 loss_ctc 19.050472 lr 0.00075072 rank 0\n",
            "2022-05-23 11:38:54,916 DEBUG CV Batch 4/0 loss 7.167280 loss_att 6.904002 loss_ctc 7.781594 history loss 6.745675 rank 0\n",
            "2022-05-23 11:39:06,595 DEBUG CV Batch 4/100 loss 7.461928 loss_att 7.093407 loss_ctc 8.321814 history loss 12.655732 rank 0\n",
            "2022-05-23 11:39:17,370 DEBUG CV Batch 4/200 loss 12.100970 loss_att 11.913180 loss_ctc 12.539148 history loss 12.818880 rank 0\n",
            "2022-05-23 11:39:28,654 DEBUG CV Batch 4/300 loss 9.845589 loss_att 9.721626 loss_ctc 10.134833 history loss 12.312352 rank 0\n",
            "2022-05-23 11:39:40,674 DEBUG CV Batch 4/400 loss 18.505814 loss_att 18.564484 loss_ctc 18.368919 history loss 11.593803 rank 0\n",
            "2022-05-23 11:39:52,907 DEBUG CV Batch 4/500 loss 7.477470 loss_att 7.129235 loss_ctc 8.290020 history loss 11.511738 rank 0\n",
            "2022-05-23 11:40:04,691 DEBUG CV Batch 4/600 loss 8.707110 loss_att 8.427487 loss_ctc 9.359564 history loss 11.457509 rank 0\n",
            "2022-05-23 11:40:15,721 DEBUG CV Batch 4/700 loss 8.961914 loss_att 8.491768 loss_ctc 10.058923 history loss 11.328131 rank 0\n",
            "2022-05-23 11:40:27,289 DEBUG CV Batch 4/800 loss 9.669957 loss_att 9.350117 loss_ctc 10.416250 history loss 11.266879 rank 0\n",
            "2022-05-23 11:40:38,912 INFO Epoch 4 CV info cv_loss 11.33827665175481\n",
            "2022-05-23 11:40:38,912 INFO Checkpoint: save to checkpoint exp/conformer/4.pt\n",
            "2022-05-23 11:40:39,272 INFO Epoch 5 TRAIN info lr 0.0007507999999999999\n",
            "2022-05-23 11:40:39,274 INFO using accumulate grad, new batch size is 4 times larger than before\n",
            "2022-05-23 11:41:04,621 DEBUG TRAIN Batch 5/0 loss 11.983603 loss_att 10.404465 loss_ctc 15.668255 lr 0.00075088 rank 0\n",
            "2022-05-23 11:41:34,478 DEBUG TRAIN Batch 5/100 loss 17.852875 loss_att 15.337112 loss_ctc 23.722990 lr 0.00075288 rank 0\n",
            "2022-05-23 11:42:04,520 DEBUG TRAIN Batch 5/200 loss 17.244034 loss_att 15.942924 loss_ctc 20.279953 lr 0.00075488 rank 0\n",
            "2022-05-23 11:42:33,904 DEBUG TRAIN Batch 5/300 loss 16.989437 loss_att 15.672715 loss_ctc 20.061785 lr 0.00075688 rank 0\n",
            "2022-05-23 11:43:03,870 DEBUG TRAIN Batch 5/400 loss 22.676851 loss_att 21.885429 loss_ctc 24.523502 lr 0.00075888 rank 0\n",
            "2022-05-23 11:43:34,649 DEBUG TRAIN Batch 5/500 loss 11.563356 loss_att 10.453894 loss_ctc 14.152100 lr 0.00076088 rank 0\n",
            "2022-05-23 11:44:03,922 DEBUG TRAIN Batch 5/600 loss 13.433815 loss_att 12.486437 loss_ctc 15.644367 lr 0.00076288 rank 0\n",
            "2022-05-23 11:44:33,962 DEBUG TRAIN Batch 5/700 loss 19.360334 loss_att 17.224213 loss_ctc 24.344616 lr 0.00076488 rank 0\n",
            "2022-05-23 11:45:03,852 DEBUG TRAIN Batch 5/800 loss 19.819422 loss_att 18.720634 loss_ctc 22.383259 lr 0.00076688 rank 0\n",
            "2022-05-23 11:45:33,784 DEBUG TRAIN Batch 5/900 loss 26.474617 loss_att 25.023598 loss_ctc 29.860332 lr 0.00076888 rank 0\n",
            "2022-05-23 11:46:04,736 DEBUG TRAIN Batch 5/1000 loss 11.946938 loss_att 10.434648 loss_ctc 15.475613 lr 0.00077088 rank 0\n",
            "2022-05-23 11:46:33,998 DEBUG TRAIN Batch 5/1100 loss 11.029065 loss_att 9.905095 loss_ctc 13.651661 lr 0.00077288 rank 0\n",
            "2022-05-23 11:47:03,450 DEBUG TRAIN Batch 5/1200 loss 13.253139 loss_att 12.250570 loss_ctc 15.592463 lr 0.00077488 rank 0\n",
            "2022-05-23 11:47:33,061 DEBUG TRAIN Batch 5/1300 loss 14.600110 loss_att 13.834997 loss_ctc 16.385374 lr 0.00077688 rank 0\n",
            "2022-05-23 11:48:02,851 DEBUG TRAIN Batch 5/1400 loss 23.068338 loss_att 21.696529 loss_ctc 26.269226 lr 0.00077888 rank 0\n",
            "2022-05-23 11:48:33,509 DEBUG TRAIN Batch 5/1500 loss 13.022535 loss_att 11.360910 loss_ctc 16.899662 lr 0.00078088 rank 0\n",
            "2022-05-23 11:49:02,582 DEBUG TRAIN Batch 5/1600 loss 15.958361 loss_att 14.832388 loss_ctc 18.585632 lr 0.00078288 rank 0\n",
            "2022-05-23 11:49:32,344 DEBUG TRAIN Batch 5/1700 loss 19.656778 loss_att 18.181423 loss_ctc 23.099270 lr 0.00078488 rank 0\n",
            "2022-05-23 11:50:02,405 DEBUG TRAIN Batch 5/1800 loss 20.180845 loss_att 17.929680 loss_ctc 25.433563 lr 0.00078688 rank 0\n",
            "2022-05-23 11:50:32,350 DEBUG TRAIN Batch 5/1900 loss 17.905996 loss_att 17.081251 loss_ctc 19.830402 lr 0.00078888 rank 0\n",
            "2022-05-23 11:51:02,978 DEBUG TRAIN Batch 5/2000 loss 13.466043 loss_att 11.661784 loss_ctc 17.675982 lr 0.00079088 rank 0\n",
            "2022-05-23 11:51:32,891 DEBUG TRAIN Batch 5/2100 loss 13.126753 loss_att 12.738397 loss_ctc 14.032917 lr 0.00079288 rank 0\n",
            "2022-05-23 11:52:03,069 DEBUG TRAIN Batch 5/2200 loss 12.706723 loss_att 12.119015 loss_ctc 14.078045 lr 0.00079488 rank 0\n",
            "2022-05-23 11:52:33,101 DEBUG TRAIN Batch 5/2300 loss 15.448986 loss_att 14.487345 loss_ctc 17.692816 lr 0.00079688 rank 0\n",
            "2022-05-23 11:53:03,580 DEBUG TRAIN Batch 5/2400 loss 20.055248 loss_att 19.559620 loss_ctc 21.211714 lr 0.00079888 rank 0\n",
            "2022-05-23 11:53:34,471 DEBUG TRAIN Batch 5/2500 loss 15.962069 loss_att 13.894684 loss_ctc 20.785967 lr 0.00080088 rank 0\n",
            "2022-05-23 11:54:03,621 DEBUG TRAIN Batch 5/2600 loss 17.669882 loss_att 16.448856 loss_ctc 20.518940 lr 0.00080288 rank 0\n",
            "2022-05-23 11:54:33,078 DEBUG TRAIN Batch 5/2700 loss 16.060093 loss_att 14.577050 loss_ctc 19.520523 lr 0.00080488 rank 0\n",
            "2022-05-23 11:55:02,602 DEBUG TRAIN Batch 5/2800 loss 13.988047 loss_att 12.841751 loss_ctc 16.662739 lr 0.00080688 rank 0\n",
            "2022-05-23 11:55:32,206 DEBUG TRAIN Batch 5/2900 loss 20.913757 loss_att 19.114658 loss_ctc 25.111658 lr 0.00080888 rank 0\n",
            "2022-05-23 11:56:03,316 DEBUG TRAIN Batch 5/3000 loss 11.501332 loss_att 10.345112 loss_ctc 14.199180 lr 0.00081088 rank 0\n",
            "2022-05-23 11:56:32,198 DEBUG TRAIN Batch 5/3100 loss 12.772129 loss_att 11.872128 loss_ctc 14.872135 lr 0.00081288 rank 0\n",
            "2022-05-23 11:57:02,087 DEBUG TRAIN Batch 5/3200 loss 16.900204 loss_att 16.117653 loss_ctc 18.726158 lr 0.00081488 rank 0\n",
            "2022-05-23 11:57:32,044 DEBUG TRAIN Batch 5/3300 loss 16.043139 loss_att 14.463495 loss_ctc 19.728971 lr 0.00081688 rank 0\n",
            "2022-05-23 11:58:02,714 DEBUG TRAIN Batch 5/3400 loss 22.694452 loss_att 21.484520 loss_ctc 25.517626 lr 0.00081888 rank 0\n",
            "2022-05-23 11:58:33,798 DEBUG TRAIN Batch 5/3500 loss 16.890867 loss_att 15.357329 loss_ctc 20.469122 lr 0.00082088 rank 0\n",
            "2022-05-23 11:59:03,352 DEBUG TRAIN Batch 5/3600 loss 11.627213 loss_att 10.577555 loss_ctc 14.076417 lr 0.00082288 rank 0\n",
            "2022-05-23 11:59:32,801 DEBUG TRAIN Batch 5/3700 loss 14.941457 loss_att 14.100142 loss_ctc 16.904522 lr 0.00082488 rank 0\n",
            "2022-05-23 12:00:02,760 DEBUG TRAIN Batch 5/3800 loss 18.886305 loss_att 17.836555 loss_ctc 21.335720 lr 0.00082688 rank 0\n",
            "2022-05-23 12:00:32,935 DEBUG TRAIN Batch 5/3900 loss 22.543642 loss_att 21.962704 loss_ctc 23.899162 lr 0.00082888 rank 0\n",
            "2022-05-23 12:01:03,451 DEBUG TRAIN Batch 5/4000 loss 11.764275 loss_att 10.735338 loss_ctc 14.165127 lr 0.00083088 rank 0\n",
            "2022-05-23 12:01:33,185 DEBUG TRAIN Batch 5/4100 loss 15.636229 loss_att 14.685007 loss_ctc 17.855743 lr 0.00083288 rank 0\n",
            "2022-05-23 12:02:03,147 DEBUG TRAIN Batch 5/4200 loss 13.220129 loss_att 11.865210 loss_ctc 16.381607 lr 0.00083488 rank 0\n",
            "2022-05-23 12:02:32,633 DEBUG TRAIN Batch 5/4300 loss 16.409948 loss_att 15.025999 loss_ctc 19.639164 lr 0.00083688 rank 0\n",
            "2022-05-23 12:03:02,911 DEBUG TRAIN Batch 5/4400 loss 17.892185 loss_att 16.415854 loss_ctc 21.336958 lr 0.00083888 rank 0\n",
            "2022-05-23 12:03:33,454 DEBUG TRAIN Batch 5/4500 loss 12.224148 loss_att 10.893859 loss_ctc 15.328157 lr 0.00084088 rank 0\n",
            "2022-05-23 12:04:02,955 DEBUG TRAIN Batch 5/4600 loss 12.875393 loss_att 12.213930 loss_ctc 14.418806 lr 0.00084288 rank 0\n",
            "2022-05-23 12:04:32,604 DEBUG TRAIN Batch 5/4700 loss 15.485213 loss_att 13.924625 loss_ctc 19.126587 lr 0.00084488 rank 0\n",
            "2022-05-23 12:05:02,322 DEBUG TRAIN Batch 5/4800 loss 19.712543 loss_att 17.771248 loss_ctc 24.242233 lr 0.00084688 rank 0\n",
            "2022-05-23 12:05:32,583 DEBUG TRAIN Batch 5/4900 loss 18.189077 loss_att 17.592070 loss_ctc 19.582094 lr 0.00084888 rank 0\n",
            "2022-05-23 12:06:03,765 DEBUG TRAIN Batch 5/5000 loss 10.982012 loss_att 10.195317 loss_ctc 12.817633 lr 0.00085088 rank 0\n",
            "2022-05-23 12:06:33,471 DEBUG TRAIN Batch 5/5100 loss 10.834573 loss_att 9.937630 loss_ctc 12.927441 lr 0.00085288 rank 0\n",
            "2022-05-23 12:07:03,318 DEBUG TRAIN Batch 5/5200 loss 22.293741 loss_att 21.093594 loss_ctc 25.094086 lr 0.00085488 rank 0\n",
            "2022-05-23 12:07:33,300 DEBUG TRAIN Batch 5/5300 loss 18.032131 loss_att 16.267778 loss_ctc 22.148952 lr 0.00085688 rank 0\n",
            "2022-05-23 12:08:03,786 DEBUG TRAIN Batch 5/5400 loss 17.216251 loss_att 16.759441 loss_ctc 18.282139 lr 0.00085888 rank 0\n",
            "2022-05-23 12:08:34,234 DEBUG TRAIN Batch 5/5500 loss 8.899095 loss_att 7.906649 loss_ctc 11.214802 lr 0.00086088 rank 0\n",
            "2022-05-23 12:09:03,509 DEBUG TRAIN Batch 5/5600 loss 18.563450 loss_att 17.446123 loss_ctc 21.170547 lr 0.00086288 rank 0\n",
            "2022-05-23 12:09:33,338 DEBUG TRAIN Batch 5/5700 loss 18.802887 loss_att 17.072065 loss_ctc 22.841469 lr 0.00086488 rank 0\n",
            "2022-05-23 12:10:03,129 DEBUG TRAIN Batch 5/5800 loss 16.360044 loss_att 15.354490 loss_ctc 18.706335 lr 0.00086688 rank 0\n",
            "2022-05-23 12:10:33,259 DEBUG TRAIN Batch 5/5900 loss 19.581278 loss_att 18.425594 loss_ctc 22.277872 lr 0.00086888 rank 0\n",
            "2022-05-23 12:11:04,129 DEBUG TRAIN Batch 5/6000 loss 13.785516 loss_att 12.219397 loss_ctc 17.439795 lr 0.00087088 rank 0\n",
            "2022-05-23 12:11:33,596 DEBUG TRAIN Batch 5/6100 loss 12.166681 loss_att 11.510098 loss_ctc 13.698711 lr 0.00087288 rank 0\n",
            "2022-05-23 12:12:02,725 DEBUG TRAIN Batch 5/6200 loss 14.726758 loss_att 13.426463 loss_ctc 17.760778 lr 0.00087488 rank 0\n",
            "2022-05-23 12:12:32,151 DEBUG TRAIN Batch 5/6300 loss 14.467375 loss_att 13.598396 loss_ctc 16.494991 lr 0.00087688 rank 0\n",
            "2022-05-23 12:13:02,231 DEBUG TRAIN Batch 5/6400 loss 20.618700 loss_att 19.300039 loss_ctc 23.695578 lr 0.00087888 rank 0\n",
            "2022-05-23 12:13:33,361 DEBUG TRAIN Batch 5/6500 loss 14.736458 loss_att 13.530609 loss_ctc 17.550106 lr 0.00088088 rank 0\n",
            "2022-05-23 12:14:03,085 DEBUG TRAIN Batch 5/6600 loss 15.468671 loss_att 13.806927 loss_ctc 19.346075 lr 0.00088288 rank 0\n",
            "2022-05-23 12:14:32,856 DEBUG TRAIN Batch 5/6700 loss 14.892043 loss_att 13.898279 loss_ctc 17.210827 lr 0.00088488 rank 0\n",
            "2022-05-23 12:15:03,119 DEBUG TRAIN Batch 5/6800 loss 20.943081 loss_att 19.226576 loss_ctc 24.948257 lr 0.00088688 rank 0\n",
            "2022-05-23 12:15:33,438 DEBUG TRAIN Batch 5/6900 loss 17.724663 loss_att 16.294989 loss_ctc 21.060566 lr 0.00088888 rank 0\n",
            "2022-05-23 12:16:04,209 DEBUG TRAIN Batch 5/7000 loss 11.101774 loss_att 9.660734 loss_ctc 14.464199 lr 0.00089088 rank 0\n",
            "2022-05-23 12:16:33,928 DEBUG TRAIN Batch 5/7100 loss 12.478709 loss_att 12.127745 loss_ctc 13.297626 lr 0.00089288 rank 0\n",
            "2022-05-23 12:17:03,891 DEBUG TRAIN Batch 5/7200 loss 14.333941 loss_att 13.342078 loss_ctc 16.648287 lr 0.00089488 rank 0\n",
            "2022-05-23 12:17:34,193 DEBUG TRAIN Batch 5/7300 loss 17.391268 loss_att 16.150932 loss_ctc 20.285385 lr 0.00089688 rank 0\n",
            "2022-05-23 12:18:04,448 DEBUG TRAIN Batch 5/7400 loss 19.170517 loss_att 18.780323 loss_ctc 20.080967 lr 0.00089888 rank 0\n",
            "2022-05-23 12:18:35,408 DEBUG TRAIN Batch 5/7500 loss 13.589174 loss_att 12.706224 loss_ctc 15.649389 lr 0.00090088 rank 0\n",
            "2022-05-23 12:18:41,412 DEBUG CV Batch 5/0 loss 6.856106 loss_att 6.345922 loss_ctc 8.046535 history loss 6.452806 rank 0\n",
            "2022-05-23 12:18:53,381 DEBUG CV Batch 5/100 loss 6.488142 loss_att 6.378761 loss_ctc 6.743362 history loss 11.783289 rank 0\n",
            "2022-05-23 12:19:04,478 DEBUG CV Batch 5/200 loss 10.580506 loss_att 10.304177 loss_ctc 11.225275 history loss 11.907147 rank 0\n",
            "2022-05-23 12:19:16,097 DEBUG CV Batch 5/300 loss 9.149590 loss_att 9.469250 loss_ctc 8.403720 history loss 11.225528 rank 0\n",
            "2022-05-23 12:19:28,610 DEBUG CV Batch 5/400 loss 15.084577 loss_att 15.014955 loss_ctc 15.247026 history loss 10.482247 rank 0\n",
            "2022-05-23 12:19:41,713 DEBUG CV Batch 5/500 loss 6.298535 loss_att 5.831464 loss_ctc 7.388370 history loss 10.407897 rank 0\n",
            "2022-05-23 12:19:54,241 DEBUG CV Batch 5/600 loss 8.230081 loss_att 8.003201 loss_ctc 8.759468 history loss 10.344541 rank 0\n",
            "2022-05-23 12:20:05,739 DEBUG CV Batch 5/700 loss 7.520368 loss_att 7.173846 loss_ctc 8.328918 history loss 10.179350 rank 0\n",
            "2022-05-23 12:20:17,967 DEBUG CV Batch 5/800 loss 8.609917 loss_att 8.736280 loss_ctc 8.315068 history loss 10.117774 rank 0\n",
            "2022-05-23 12:20:30,048 INFO Epoch 5 CV info cv_loss 10.129995198694802\n",
            "2022-05-23 12:20:30,048 INFO Checkpoint: save to checkpoint exp/conformer/5.pt\n",
            "2022-05-23 12:20:30,413 INFO Epoch 6 TRAIN info lr 0.0009009599999999999\n",
            "2022-05-23 12:20:30,415 INFO using accumulate grad, new batch size is 4 times larger than before\n",
            "2022-05-23 12:20:56,516 DEBUG TRAIN Batch 6/0 loss 13.587406 loss_att 12.514841 loss_ctc 16.090059 lr 0.00090104 rank 0\n",
            "2022-05-23 12:21:26,417 DEBUG TRAIN Batch 6/100 loss 14.860149 loss_att 13.110981 loss_ctc 18.941540 lr 0.00090304 rank 0\n",
            "2022-05-23 12:21:57,155 DEBUG TRAIN Batch 6/200 loss 13.332305 loss_att 12.712659 loss_ctc 14.778147 lr 0.00090504 rank 0\n",
            "2022-05-23 12:22:26,667 DEBUG TRAIN Batch 6/300 loss 16.424467 loss_att 15.175692 loss_ctc 19.338278 lr 0.00090704 rank 0\n",
            "2022-05-23 12:22:57,154 DEBUG TRAIN Batch 6/400 loss 18.401552 loss_att 17.591415 loss_ctc 20.291870 lr 0.00090904 rank 0\n",
            "2022-05-23 12:23:28,580 DEBUG TRAIN Batch 6/500 loss 10.123739 loss_att 8.726683 loss_ctc 13.383535 lr 0.00091104 rank 0\n",
            "2022-05-23 12:23:57,892 DEBUG TRAIN Batch 6/600 loss 15.783710 loss_att 14.042849 loss_ctc 19.845716 lr 0.00091304 rank 0\n",
            "2022-05-23 12:24:27,567 DEBUG TRAIN Batch 6/700 loss 17.155241 loss_att 15.533419 loss_ctc 20.939491 lr 0.00091504 rank 0\n",
            "2022-05-23 12:24:57,516 DEBUG TRAIN Batch 6/800 loss 14.673689 loss_att 13.795183 loss_ctc 16.723537 lr 0.00091704 rank 0\n",
            "2022-05-23 12:25:27,416 DEBUG TRAIN Batch 6/900 loss 14.070697 loss_att 13.611362 loss_ctc 15.142478 lr 0.00091904 rank 0\n",
            "2022-05-23 12:25:58,275 DEBUG TRAIN Batch 6/1000 loss 13.097549 loss_att 12.199724 loss_ctc 15.192474 lr 0.00092104 rank 0\n",
            "2022-05-23 12:26:27,363 DEBUG TRAIN Batch 6/1100 loss 10.839321 loss_att 10.289377 loss_ctc 12.122522 lr 0.00092304 rank 0\n",
            "2022-05-23 12:26:56,699 DEBUG TRAIN Batch 6/1200 loss 12.803358 loss_att 12.447573 loss_ctc 13.633522 lr 0.00092504 rank 0\n",
            "2022-05-23 12:27:26,620 DEBUG TRAIN Batch 6/1300 loss 18.458593 loss_att 17.161133 loss_ctc 21.486004 lr 0.00092704 rank 0\n",
            "2022-05-23 12:27:57,005 DEBUG TRAIN Batch 6/1400 loss 20.081545 loss_att 18.929430 loss_ctc 22.769814 lr 0.00092904 rank 0\n",
            "2022-05-23 12:28:27,386 DEBUG TRAIN Batch 6/1500 loss 14.350145 loss_att 13.332850 loss_ctc 16.723831 lr 0.00093104 rank 0\n",
            "2022-05-23 12:28:56,996 DEBUG TRAIN Batch 6/1600 loss 10.865942 loss_att 9.597754 loss_ctc 13.825050 lr 0.00093304 rank 0\n",
            "2022-05-23 12:29:26,316 DEBUG TRAIN Batch 6/1700 loss 12.747689 loss_att 11.783173 loss_ctc 14.998228 lr 0.00093504 rank 0\n",
            "2022-05-23 12:29:56,413 DEBUG TRAIN Batch 6/1800 loss 11.875149 loss_att 10.691705 loss_ctc 14.636518 lr 0.00093704 rank 0\n",
            "2022-05-23 12:30:26,476 DEBUG TRAIN Batch 6/1900 loss 20.170864 loss_att 18.916306 loss_ctc 23.098167 lr 0.00093904 rank 0\n",
            "2022-05-23 12:30:57,258 DEBUG TRAIN Batch 6/2000 loss 14.788351 loss_att 13.467024 loss_ctc 17.871449 lr 0.00094104 rank 0\n",
            "2022-05-23 12:31:26,685 DEBUG TRAIN Batch 6/2100 loss 16.847878 loss_att 15.800776 loss_ctc 19.291113 lr 0.00094304 rank 0\n",
            "2022-05-23 12:31:56,505 DEBUG TRAIN Batch 6/2200 loss 16.165014 loss_att 14.912241 loss_ctc 19.088154 lr 0.00094504 rank 0\n",
            "2022-05-23 12:32:26,490 DEBUG TRAIN Batch 6/2300 loss 20.371910 loss_att 19.305180 loss_ctc 22.860950 lr 0.00094704 rank 0\n",
            "2022-05-23 12:32:56,744 DEBUG TRAIN Batch 6/2400 loss 14.478248 loss_att 14.299340 loss_ctc 14.895700 lr 0.00094904 rank 0\n",
            "2022-05-23 12:33:27,367 DEBUG TRAIN Batch 6/2500 loss 15.138069 loss_att 13.713459 loss_ctc 18.462158 lr 0.00095104 rank 0\n",
            "2022-05-23 12:33:56,992 DEBUG TRAIN Batch 6/2600 loss 10.598505 loss_att 9.481547 loss_ctc 13.204739 lr 0.00095304 rank 0\n",
            "2022-05-23 12:34:27,202 DEBUG TRAIN Batch 6/2700 loss 17.516123 loss_att 16.446573 loss_ctc 20.011738 lr 0.00095504 rank 0\n",
            "2022-05-23 12:34:57,179 DEBUG TRAIN Batch 6/2800 loss 14.781306 loss_att 14.103691 loss_ctc 16.362410 lr 0.00095704 rank 0\n",
            "2022-05-23 12:35:27,347 DEBUG TRAIN Batch 6/2900 loss 14.938698 loss_att 13.908118 loss_ctc 17.343382 lr 0.00095904 rank 0\n",
            "2022-05-23 12:35:57,806 DEBUG TRAIN Batch 6/3000 loss 13.366659 loss_att 12.208784 loss_ctc 16.068371 lr 0.00096104 rank 0\n",
            "2022-05-23 12:36:27,615 DEBUG TRAIN Batch 6/3100 loss 13.617859 loss_att 12.180968 loss_ctc 16.970600 lr 0.00096304 rank 0\n",
            "2022-05-23 12:36:56,853 DEBUG TRAIN Batch 6/3200 loss 11.373598 loss_att 10.659485 loss_ctc 13.039862 lr 0.00096504 rank 0\n",
            "2022-05-23 12:37:26,931 DEBUG TRAIN Batch 6/3300 loss 16.110661 loss_att 15.172299 loss_ctc 18.300167 lr 0.00096704 rank 0\n",
            "2022-05-23 12:37:57,143 DEBUG TRAIN Batch 6/3400 loss 13.686260 loss_att 12.989891 loss_ctc 15.311125 lr 0.00096904 rank 0\n",
            "2022-05-23 12:38:28,007 DEBUG TRAIN Batch 6/3500 loss 12.401211 loss_att 11.111411 loss_ctc 15.410746 lr 0.00097104 rank 0\n",
            "2022-05-23 12:38:57,393 DEBUG TRAIN Batch 6/3600 loss 18.566856 loss_att 17.636028 loss_ctc 20.738785 lr 0.00097304 rank 0\n",
            "2022-05-23 12:39:26,928 DEBUG TRAIN Batch 6/3700 loss 12.751837 loss_att 12.110317 loss_ctc 14.248718 lr 0.00097504 rank 0\n",
            "2022-05-23 12:39:56,877 DEBUG TRAIN Batch 6/3800 loss 13.602997 loss_att 12.684309 loss_ctc 15.746602 lr 0.00097704 rank 0\n",
            "2022-05-23 12:40:26,762 DEBUG TRAIN Batch 6/3900 loss 18.687447 loss_att 17.699169 loss_ctc 20.993425 lr 0.00097904 rank 0\n",
            "2022-05-23 12:40:57,572 DEBUG TRAIN Batch 6/4000 loss 14.735207 loss_att 13.505454 loss_ctc 17.604630 lr 0.00098104 rank 0\n",
            "2022-05-23 12:41:27,061 DEBUG TRAIN Batch 6/4100 loss 10.610098 loss_att 9.221201 loss_ctc 13.850859 lr 0.00098304 rank 0\n",
            "2022-05-23 12:41:56,964 DEBUG TRAIN Batch 6/4200 loss 15.558918 loss_att 14.753159 loss_ctc 17.439026 lr 0.00098504 rank 0\n",
            "2022-05-23 12:42:26,575 DEBUG TRAIN Batch 6/4300 loss 17.355076 loss_att 16.769905 loss_ctc 18.720472 lr 0.00098704 rank 0\n",
            "2022-05-23 12:42:56,489 DEBUG TRAIN Batch 6/4400 loss 13.700218 loss_att 13.299969 loss_ctc 14.634136 lr 0.00098904 rank 0\n",
            "2022-05-23 12:43:27,224 DEBUG TRAIN Batch 6/4500 loss 12.881042 loss_att 12.155745 loss_ctc 14.573405 lr 0.00099104 rank 0\n",
            "2022-05-23 12:43:56,506 DEBUG TRAIN Batch 6/4600 loss 15.197722 loss_att 13.960459 loss_ctc 18.084671 lr 0.00099304 rank 0\n",
            "2022-05-23 12:44:26,514 DEBUG TRAIN Batch 6/4700 loss 13.420716 loss_att 12.785978 loss_ctc 14.901772 lr 0.00099504 rank 0\n",
            "2022-05-23 12:44:56,193 DEBUG TRAIN Batch 6/4800 loss 13.753019 loss_att 12.649410 loss_ctc 16.328110 lr 0.00099704 rank 0\n",
            "2022-05-23 12:45:26,376 DEBUG TRAIN Batch 6/4900 loss 16.091885 loss_att 15.103333 loss_ctc 18.398504 lr 0.00099904 rank 0\n",
            "2022-05-23 12:45:57,335 DEBUG TRAIN Batch 6/5000 loss 12.052256 loss_att 10.631742 loss_ctc 15.366788 lr 0.00100104 rank 0\n",
            "2022-05-23 12:46:27,504 DEBUG TRAIN Batch 6/5100 loss 16.487862 loss_att 14.912273 loss_ctc 20.164236 lr 0.00100304 rank 0\n",
            "2022-05-23 12:46:56,949 DEBUG TRAIN Batch 6/5200 loss 14.692563 loss_att 13.654324 loss_ctc 17.115122 lr 0.00100504 rank 0\n",
            "2022-05-23 12:47:26,701 DEBUG TRAIN Batch 6/5300 loss 18.572849 loss_att 17.144279 loss_ctc 21.906176 lr 0.00100704 rank 0\n",
            "2022-05-23 12:47:57,102 DEBUG TRAIN Batch 6/5400 loss 26.438658 loss_att 25.052038 loss_ctc 29.674103 lr 0.00100904 rank 0\n",
            "2022-05-23 12:48:28,028 DEBUG TRAIN Batch 6/5500 loss 16.022596 loss_att 14.334509 loss_ctc 19.961468 lr 0.00101104 rank 0\n",
            "2022-05-23 12:48:57,813 DEBUG TRAIN Batch 6/5600 loss 12.935831 loss_att 11.768047 loss_ctc 15.660660 lr 0.00101304 rank 0\n",
            "2022-05-23 12:49:27,157 DEBUG TRAIN Batch 6/5700 loss 13.689438 loss_att 12.709732 loss_ctc 15.975415 lr 0.00101504 rank 0\n",
            "2022-05-23 12:49:57,390 DEBUG TRAIN Batch 6/5800 loss 18.721916 loss_att 17.716093 loss_ctc 21.068836 lr 0.00101704 rank 0\n",
            "2022-05-23 12:50:27,562 DEBUG TRAIN Batch 6/5900 loss 16.709553 loss_att 15.578817 loss_ctc 19.347931 lr 0.00101904 rank 0\n",
            "2022-05-23 12:50:58,462 DEBUG TRAIN Batch 6/6000 loss 13.584808 loss_att 12.360474 loss_ctc 16.441589 lr 0.00102104 rank 0\n",
            "2022-05-23 12:51:28,283 DEBUG TRAIN Batch 6/6100 loss 8.242615 loss_att 7.387815 loss_ctc 10.237148 lr 0.00102304 rank 0\n",
            "2022-05-23 12:51:58,198 DEBUG TRAIN Batch 6/6200 loss 14.978992 loss_att 14.630337 loss_ctc 15.792520 lr 0.00102504 rank 0\n",
            "2022-05-23 12:52:27,942 DEBUG TRAIN Batch 6/6300 loss 13.764921 loss_att 13.110502 loss_ctc 15.291898 lr 0.00102704 rank 0\n",
            "2022-05-23 12:52:57,886 DEBUG TRAIN Batch 6/6400 loss 18.950138 loss_att 17.954378 loss_ctc 21.273581 lr 0.00102904 rank 0\n",
            "2022-05-23 12:53:28,747 DEBUG TRAIN Batch 6/6500 loss 12.482761 loss_att 10.706455 loss_ctc 16.627476 lr 0.00103104 rank 0\n",
            "2022-05-23 12:53:58,415 DEBUG TRAIN Batch 6/6600 loss 15.339796 loss_att 14.286521 loss_ctc 17.797438 lr 0.00103304 rank 0\n",
            "2022-05-23 12:54:28,460 DEBUG TRAIN Batch 6/6700 loss 15.021105 loss_att 13.552907 loss_ctc 18.446899 lr 0.00103504 rank 0\n",
            "2022-05-23 12:54:58,285 DEBUG TRAIN Batch 6/6800 loss 14.238079 loss_att 12.841723 loss_ctc 17.496243 lr 0.00103704 rank 0\n",
            "2022-05-23 12:55:28,045 DEBUG TRAIN Batch 6/6900 loss 18.366287 loss_att 17.565151 loss_ctc 20.235607 lr 0.00103904 rank 0\n",
            "2022-05-23 12:55:58,717 DEBUG TRAIN Batch 6/7000 loss 11.998663 loss_att 10.607029 loss_ctc 15.245809 lr 0.00104104 rank 0\n",
            "2022-05-23 12:56:28,238 DEBUG TRAIN Batch 6/7100 loss 12.990347 loss_att 12.213917 loss_ctc 14.802017 lr 0.00104304 rank 0\n",
            "2022-05-23 12:56:58,418 DEBUG TRAIN Batch 6/7200 loss 14.745614 loss_att 13.334909 loss_ctc 18.037258 lr 0.00104504 rank 0\n",
            "2022-05-23 12:57:28,413 DEBUG TRAIN Batch 6/7300 loss 15.815136 loss_att 14.590899 loss_ctc 18.671692 lr 0.00104704 rank 0\n",
            "2022-05-23 12:57:58,366 DEBUG TRAIN Batch 6/7400 loss 16.865459 loss_att 16.244020 loss_ctc 18.315487 lr 0.00104904 rank 0\n",
            "2022-05-23 12:58:29,338 DEBUG TRAIN Batch 6/7500 loss 13.522779 loss_att 12.448965 loss_ctc 16.028347 lr 0.00105104 rank 0\n",
            "2022-05-23 12:58:35,835 DEBUG CV Batch 6/0 loss 6.789756 loss_att 6.220798 loss_ctc 8.117327 history loss 6.390358 rank 0\n",
            "2022-05-23 12:58:47,561 DEBUG CV Batch 6/100 loss 5.993277 loss_att 6.240166 loss_ctc 5.417202 history loss 10.658084 rank 0\n",
            "2022-05-23 12:58:58,302 DEBUG CV Batch 6/200 loss 9.426565 loss_att 9.335369 loss_ctc 9.639359 history loss 10.614903 rank 0\n",
            "2022-05-23 12:59:09,692 DEBUG CV Batch 6/300 loss 8.279202 loss_att 8.249272 loss_ctc 8.349036 history loss 10.046355 rank 0\n",
            "2022-05-23 12:59:21,875 DEBUG CV Batch 6/400 loss 15.727064 loss_att 15.693478 loss_ctc 15.805433 history loss 9.396049 rank 0\n",
            "2022-05-23 12:59:34,625 DEBUG CV Batch 6/500 loss 5.824111 loss_att 5.651923 loss_ctc 6.225882 history loss 9.337795 rank 0\n",
            "2022-05-23 12:59:46,786 DEBUG CV Batch 6/600 loss 6.707948 loss_att 6.542921 loss_ctc 7.093009 history loss 9.283361 rank 0\n",
            "2022-05-23 12:59:58,155 DEBUG CV Batch 6/700 loss 6.700337 loss_att 6.676415 loss_ctc 6.756156 history loss 9.125301 rank 0\n",
            "2022-05-23 13:00:10,052 DEBUG CV Batch 6/800 loss 8.102226 loss_att 8.033619 loss_ctc 8.262308 history loss 9.097393 rank 0\n",
            "2022-05-23 13:00:21,904 INFO Epoch 6 CV info cv_loss 9.116571134704518\n",
            "2022-05-23 13:00:21,904 INFO Checkpoint: save to checkpoint exp/conformer/6.pt\n",
            "2022-05-23 13:00:22,242 INFO Epoch 7 TRAIN info lr 0.00105112\n",
            "2022-05-23 13:00:22,244 INFO using accumulate grad, new batch size is 4 times larger than before\n",
            "2022-05-23 13:00:48,454 DEBUG TRAIN Batch 7/0 loss 12.099405 loss_att 10.760645 loss_ctc 15.223181 lr 0.00105120 rank 0\n",
            "2022-05-23 13:01:16,888 DEBUG TRAIN Batch 7/100 loss 13.324728 loss_att 11.688004 loss_ctc 17.143749 lr 0.00105320 rank 0\n",
            "2022-05-23 13:01:45,776 DEBUG TRAIN Batch 7/200 loss 16.969759 loss_att 15.443121 loss_ctc 20.531914 lr 0.00105520 rank 0\n",
            "2022-05-23 13:02:14,763 DEBUG TRAIN Batch 7/300 loss 12.568209 loss_att 11.600528 loss_ctc 14.826132 lr 0.00105720 rank 0\n",
            "2022-05-23 13:02:44,570 DEBUG TRAIN Batch 7/400 loss 18.777061 loss_att 18.131779 loss_ctc 20.282719 lr 0.00105920 rank 0\n",
            "2022-05-23 13:03:15,061 DEBUG TRAIN Batch 7/500 loss 15.359529 loss_att 13.461767 loss_ctc 19.787640 lr 0.00106120 rank 0\n",
            "2022-05-23 13:03:44,375 DEBUG TRAIN Batch 7/600 loss 11.075520 loss_att 10.116610 loss_ctc 13.312975 lr 0.00106320 rank 0\n",
            "2022-05-23 13:04:14,026 DEBUG TRAIN Batch 7/700 loss 11.468441 loss_att 10.207280 loss_ctc 14.411150 lr 0.00106520 rank 0\n",
            "2022-05-23 13:04:43,677 DEBUG TRAIN Batch 7/800 loss 14.055027 loss_att 13.228091 loss_ctc 15.984543 lr 0.00106720 rank 0\n",
            "2022-05-23 13:05:13,703 DEBUG TRAIN Batch 7/900 loss 17.892208 loss_att 17.294119 loss_ctc 19.287750 lr 0.00106920 rank 0\n",
            "2022-05-23 13:05:44,591 DEBUG TRAIN Batch 7/1000 loss 8.576527 loss_att 7.705855 loss_ctc 10.608093 lr 0.00107120 rank 0\n",
            "2022-05-23 13:06:14,122 DEBUG TRAIN Batch 7/1100 loss 13.985867 loss_att 12.963035 loss_ctc 16.372475 lr 0.00107320 rank 0\n",
            "2022-05-23 13:06:44,055 DEBUG TRAIN Batch 7/1200 loss 10.152165 loss_att 9.570182 loss_ctc 11.510127 lr 0.00107520 rank 0\n",
            "2022-05-23 13:07:13,924 DEBUG TRAIN Batch 7/1300 loss 14.562695 loss_att 13.237063 loss_ctc 17.655832 lr 0.00107720 rank 0\n",
            "2022-05-23 13:07:44,374 DEBUG TRAIN Batch 7/1400 loss 15.353682 loss_att 14.921227 loss_ctc 16.362738 lr 0.00107920 rank 0\n",
            "2022-05-23 13:08:15,402 DEBUG TRAIN Batch 7/1500 loss 13.183861 loss_att 11.658045 loss_ctc 16.744099 lr 0.00108120 rank 0\n",
            "2022-05-23 13:08:45,112 DEBUG TRAIN Batch 7/1600 loss 12.876740 loss_att 11.721567 loss_ctc 15.572140 lr 0.00108320 rank 0\n",
            "2022-05-23 13:09:14,157 DEBUG TRAIN Batch 7/1700 loss 11.871716 loss_att 11.358396 loss_ctc 13.069464 lr 0.00108520 rank 0\n",
            "2022-05-23 13:09:43,981 DEBUG TRAIN Batch 7/1800 loss 13.938445 loss_att 13.417376 loss_ctc 15.154276 lr 0.00108720 rank 0\n",
            "2022-05-23 13:10:14,049 DEBUG TRAIN Batch 7/1900 loss 16.004524 loss_att 15.619090 loss_ctc 16.903873 lr 0.00108920 rank 0\n",
            "2022-05-23 13:10:44,809 DEBUG TRAIN Batch 7/2000 loss 11.403889 loss_att 9.819657 loss_ctc 15.100430 lr 0.00109120 rank 0\n",
            "2022-05-23 13:11:14,570 DEBUG TRAIN Batch 7/2100 loss 12.812325 loss_att 11.624577 loss_ctc 15.583739 lr 0.00109320 rank 0\n",
            "2022-05-23 13:11:44,483 DEBUG TRAIN Batch 7/2200 loss 11.307264 loss_att 10.133036 loss_ctc 14.047131 lr 0.00109520 rank 0\n",
            "2022-05-23 13:12:14,505 DEBUG TRAIN Batch 7/2300 loss 15.062379 loss_att 14.443533 loss_ctc 16.506353 lr 0.00109720 rank 0\n",
            "2022-05-23 13:12:44,537 DEBUG TRAIN Batch 7/2400 loss 19.055939 loss_att 17.928118 loss_ctc 21.687521 lr 0.00109920 rank 0\n",
            "2022-05-23 13:13:15,404 DEBUG TRAIN Batch 7/2500 loss 11.135084 loss_att 10.529535 loss_ctc 12.548030 lr 0.00110120 rank 0\n",
            "2022-05-23 13:13:45,253 DEBUG TRAIN Batch 7/2600 loss 14.544579 loss_att 13.268182 loss_ctc 17.522839 lr 0.00110320 rank 0\n",
            "2022-05-23 13:14:14,556 DEBUG TRAIN Batch 7/2700 loss 11.168486 loss_att 10.572172 loss_ctc 12.559885 lr 0.00110520 rank 0\n",
            "2022-05-23 13:14:44,650 DEBUG TRAIN Batch 7/2800 loss 11.208460 loss_att 10.514909 loss_ctc 12.826744 lr 0.00110720 rank 0\n",
            "2022-05-23 13:15:14,171 DEBUG TRAIN Batch 7/2900 loss 16.675894 loss_att 15.457336 loss_ctc 19.519194 lr 0.00110920 rank 0\n",
            "2022-05-23 13:15:45,573 DEBUG TRAIN Batch 7/3000 loss 10.842710 loss_att 10.077120 loss_ctc 12.629090 lr 0.00111120 rank 0\n",
            "2022-05-23 13:16:15,149 DEBUG TRAIN Batch 7/3100 loss 9.716354 loss_att 8.624260 loss_ctc 12.264572 lr 0.00111320 rank 0\n",
            "2022-05-23 13:16:44,990 DEBUG TRAIN Batch 7/3200 loss 13.121599 loss_att 12.079451 loss_ctc 15.553282 lr 0.00111520 rank 0\n",
            "2022-05-23 13:17:14,793 DEBUG TRAIN Batch 7/3300 loss 17.006548 loss_att 16.123734 loss_ctc 19.066448 lr 0.00111720 rank 0\n",
            "2022-05-23 13:17:45,339 DEBUG TRAIN Batch 7/3400 loss 17.365950 loss_att 15.883635 loss_ctc 20.824684 lr 0.00111920 rank 0\n",
            "2022-05-23 13:18:15,862 DEBUG TRAIN Batch 7/3500 loss 8.626592 loss_att 7.459787 loss_ctc 11.349134 lr 0.00112120 rank 0\n",
            "2022-05-23 13:18:45,515 DEBUG TRAIN Batch 7/3600 loss 16.262989 loss_att 14.430795 loss_ctc 20.538111 lr 0.00112320 rank 0\n",
            "2022-05-23 13:19:15,358 DEBUG TRAIN Batch 7/3700 loss 12.689450 loss_att 11.476245 loss_ctc 15.520264 lr 0.00112520 rank 0\n",
            "2022-05-23 13:19:45,481 DEBUG TRAIN Batch 7/3800 loss 15.106567 loss_att 13.980865 loss_ctc 17.733206 lr 0.00112720 rank 0\n",
            "2022-05-23 13:20:15,564 DEBUG TRAIN Batch 7/3900 loss 16.334837 loss_att 15.381598 loss_ctc 18.559063 lr 0.00112920 rank 0\n",
            "2022-05-23 13:20:46,685 DEBUG TRAIN Batch 7/4000 loss 10.963742 loss_att 10.117955 loss_ctc 12.937244 lr 0.00113120 rank 0\n",
            "2022-05-23 13:21:16,750 DEBUG TRAIN Batch 7/4100 loss 13.756083 loss_att 12.519206 loss_ctc 16.642132 lr 0.00113320 rank 0\n",
            "2022-05-23 13:21:46,407 DEBUG TRAIN Batch 7/4200 loss 13.736872 loss_att 13.068647 loss_ctc 15.296061 lr 0.00113520 rank 0\n",
            "2022-05-23 13:22:16,460 DEBUG TRAIN Batch 7/4300 loss 15.622316 loss_att 14.741076 loss_ctc 17.678547 lr 0.00113720 rank 0\n",
            "2022-05-23 13:22:46,348 DEBUG TRAIN Batch 7/4400 loss 16.835411 loss_att 15.794154 loss_ctc 19.265011 lr 0.00113920 rank 0\n",
            "2022-05-23 13:23:17,235 DEBUG TRAIN Batch 7/4500 loss 12.176673 loss_att 10.677531 loss_ctc 15.674670 lr 0.00114120 rank 0\n",
            "2022-05-23 13:23:46,786 DEBUG TRAIN Batch 7/4600 loss 10.018316 loss_att 9.095358 loss_ctc 12.171885 lr 0.00114320 rank 0\n",
            "2022-05-23 13:24:16,696 DEBUG TRAIN Batch 7/4700 loss 10.520034 loss_att 10.133919 loss_ctc 11.420969 lr 0.00114520 rank 0\n",
            "2022-05-23 13:24:46,789 DEBUG TRAIN Batch 7/4800 loss 12.658386 loss_att 12.026041 loss_ctc 14.133861 lr 0.00114720 rank 0\n",
            "2022-05-23 13:25:16,949 DEBUG TRAIN Batch 7/4900 loss 18.130190 loss_att 17.086918 loss_ctc 20.564491 lr 0.00114920 rank 0\n",
            "2022-05-23 13:25:47,901 DEBUG TRAIN Batch 7/5000 loss 10.542849 loss_att 9.018998 loss_ctc 14.098497 lr 0.00115120 rank 0\n",
            "2022-05-23 13:26:17,589 DEBUG TRAIN Batch 7/5100 loss 7.152818 loss_att 6.479397 loss_ctc 8.724134 lr 0.00115320 rank 0\n",
            "2022-05-23 13:26:47,089 DEBUG TRAIN Batch 7/5200 loss 11.313283 loss_att 10.471930 loss_ctc 13.276441 lr 0.00115520 rank 0\n",
            "2022-05-23 13:27:16,966 DEBUG TRAIN Batch 7/5300 loss 12.750412 loss_att 11.836651 loss_ctc 14.882523 lr 0.00115720 rank 0\n",
            "2022-05-23 13:27:47,042 DEBUG TRAIN Batch 7/5400 loss 18.126877 loss_att 17.231009 loss_ctc 20.217239 lr 0.00115920 rank 0\n",
            "2022-05-23 13:28:17,965 DEBUG TRAIN Batch 7/5500 loss 10.978447 loss_att 10.009464 loss_ctc 13.239406 lr 0.00116120 rank 0\n",
            "2022-05-23 13:28:47,680 DEBUG TRAIN Batch 7/5600 loss 14.809022 loss_att 13.891438 loss_ctc 16.950050 lr 0.00116320 rank 0\n",
            "2022-05-23 13:29:16,916 DEBUG TRAIN Batch 7/5700 loss 12.520147 loss_att 11.549210 loss_ctc 14.785667 lr 0.00116520 rank 0\n",
            "2022-05-23 13:29:46,824 DEBUG TRAIN Batch 7/5800 loss 14.106120 loss_att 12.938622 loss_ctc 16.830284 lr 0.00116720 rank 0\n",
            "2022-05-23 13:30:17,095 DEBUG TRAIN Batch 7/5900 loss 14.576464 loss_att 13.766080 loss_ctc 16.467358 lr 0.00116920 rank 0\n",
            "2022-05-23 13:30:48,272 DEBUG TRAIN Batch 7/6000 loss 7.180712 loss_att 6.458593 loss_ctc 8.865654 lr 0.00117120 rank 0\n",
            "2022-05-23 13:31:17,961 DEBUG TRAIN Batch 7/6100 loss 11.526148 loss_att 10.755560 loss_ctc 13.324186 lr 0.00117320 rank 0\n",
            "2022-05-23 13:31:47,610 DEBUG TRAIN Batch 7/6200 loss 11.233740 loss_att 10.353881 loss_ctc 13.286743 lr 0.00117520 rank 0\n",
            "2022-05-23 13:32:16,994 DEBUG TRAIN Batch 7/6300 loss 13.959803 loss_att 13.471052 loss_ctc 15.100220 lr 0.00117720 rank 0\n",
            "2022-05-23 13:32:47,368 DEBUG TRAIN Batch 7/6400 loss 15.642164 loss_att 14.858859 loss_ctc 17.469875 lr 0.00117920 rank 0\n",
            "2022-05-23 13:33:18,562 DEBUG TRAIN Batch 7/6500 loss 10.603428 loss_att 9.439184 loss_ctc 13.319997 lr 0.00118120 rank 0\n",
            "2022-05-23 13:33:48,131 DEBUG TRAIN Batch 7/6600 loss 15.502640 loss_att 13.879189 loss_ctc 19.290693 lr 0.00118320 rank 0\n",
            "2022-05-23 13:34:18,001 DEBUG TRAIN Batch 7/6700 loss 12.077571 loss_att 10.966347 loss_ctc 14.670427 lr 0.00118520 rank 0\n",
            "2022-05-23 13:34:47,747 DEBUG TRAIN Batch 7/6800 loss 12.525036 loss_att 12.192551 loss_ctc 13.300836 lr 0.00118720 rank 0\n",
            "2022-05-23 13:35:17,711 DEBUG TRAIN Batch 7/6900 loss 12.993387 loss_att 12.685074 loss_ctc 13.712784 lr 0.00118920 rank 0\n",
            "2022-05-23 13:35:48,844 DEBUG TRAIN Batch 7/7000 loss 9.903625 loss_att 8.892756 loss_ctc 12.262318 lr 0.00119120 rank 0\n",
            "2022-05-23 13:36:18,543 DEBUG TRAIN Batch 7/7100 loss 10.334476 loss_att 9.641994 loss_ctc 11.950272 lr 0.00119320 rank 0\n",
            "2022-05-23 13:36:48,043 DEBUG TRAIN Batch 7/7200 loss 13.524976 loss_att 12.478148 loss_ctc 15.967575 lr 0.00119520 rank 0\n",
            "2022-05-23 13:37:17,673 DEBUG TRAIN Batch 7/7300 loss 14.880110 loss_att 14.358624 loss_ctc 16.096910 lr 0.00119720 rank 0\n",
            "2022-05-23 13:37:48,405 DEBUG TRAIN Batch 7/7400 loss 15.193815 loss_att 15.042610 loss_ctc 15.546625 lr 0.00119920 rank 0\n",
            "2022-05-23 13:38:19,098 DEBUG TRAIN Batch 7/7500 loss 9.805264 loss_att 8.666775 loss_ctc 12.461735 lr 0.00120120 rank 0\n",
            "2022-05-23 13:38:25,207 DEBUG CV Batch 7/0 loss 6.750479 loss_att 6.267303 loss_ctc 7.877888 history loss 6.353392 rank 0\n",
            "2022-05-23 13:38:37,002 DEBUG CV Batch 7/100 loss 5.639268 loss_att 5.685903 loss_ctc 5.530454 history loss 9.882609 rank 0\n",
            "2022-05-23 13:38:47,889 DEBUG CV Batch 7/200 loss 7.945606 loss_att 7.810564 loss_ctc 8.260705 history loss 9.816384 rank 0\n",
            "2022-05-23 13:38:59,244 DEBUG CV Batch 7/300 loss 7.159047 loss_att 7.286778 loss_ctc 6.861008 history loss 9.267729 rank 0\n",
            "2022-05-23 13:39:11,383 DEBUG CV Batch 7/400 loss 13.071485 loss_att 13.342743 loss_ctc 12.438549 history loss 8.604756 rank 0\n",
            "2022-05-23 13:39:24,040 DEBUG CV Batch 7/500 loss 5.766064 loss_att 5.560105 loss_ctc 6.246635 history loss 8.516443 rank 0\n",
            "2022-05-23 13:39:36,121 DEBUG CV Batch 7/600 loss 6.806406 loss_att 6.696160 loss_ctc 7.063645 history loss 8.448173 rank 0\n",
            "2022-05-23 13:39:47,509 DEBUG CV Batch 7/700 loss 6.229635 loss_att 5.820166 loss_ctc 7.185063 history loss 8.291832 rank 0\n",
            "2022-05-23 13:39:59,526 DEBUG CV Batch 7/800 loss 8.106541 loss_att 7.854252 loss_ctc 8.695215 history loss 8.256760 rank 0\n",
            "2022-05-23 13:40:11,439 INFO Epoch 7 CV info cv_loss 8.274191972140516\n",
            "2022-05-23 13:40:11,439 INFO Checkpoint: save to checkpoint exp/conformer/7.pt\n",
            "2022-05-23 13:40:11,820 INFO Epoch 8 TRAIN info lr 0.0012012799999999999\n",
            "2022-05-23 13:40:11,822 INFO using accumulate grad, new batch size is 4 times larger than before\n",
            "2022-05-23 13:40:37,507 DEBUG TRAIN Batch 8/0 loss 8.710351 loss_att 7.774867 loss_ctc 10.893147 lr 0.00120136 rank 0\n",
            "2022-05-23 13:41:05,980 DEBUG TRAIN Batch 8/100 loss 10.553678 loss_att 9.513338 loss_ctc 12.981135 lr 0.00120336 rank 0\n",
            "2022-05-23 13:41:34,863 DEBUG TRAIN Batch 8/200 loss 8.372128 loss_att 7.690565 loss_ctc 9.962442 lr 0.00120536 rank 0\n",
            "2022-05-23 13:42:04,010 DEBUG TRAIN Batch 8/300 loss 13.197979 loss_att 12.081687 loss_ctc 15.802658 lr 0.00120736 rank 0\n",
            "2022-05-23 13:42:33,771 DEBUG TRAIN Batch 8/400 loss 16.798891 loss_att 15.333909 loss_ctc 20.217180 lr 0.00120936 rank 0\n",
            "2022-05-23 13:43:04,186 DEBUG TRAIN Batch 8/500 loss 11.627661 loss_att 10.360397 loss_ctc 14.584610 lr 0.00121136 rank 0\n",
            "2022-05-23 13:43:33,281 DEBUG TRAIN Batch 8/600 loss 8.363375 loss_att 7.619890 loss_ctc 10.098171 lr 0.00121336 rank 0\n",
            "2022-05-23 13:44:03,081 DEBUG TRAIN Batch 8/700 loss 12.430855 loss_att 11.878304 loss_ctc 13.720138 lr 0.00121536 rank 0\n",
            "2022-05-23 13:44:33,416 DEBUG TRAIN Batch 8/800 loss 14.002596 loss_att 13.125156 loss_ctc 16.049953 lr 0.00121736 rank 0\n",
            "2022-05-23 13:45:03,728 DEBUG TRAIN Batch 8/900 loss 19.618507 loss_att 19.219917 loss_ctc 20.548546 lr 0.00121936 rank 0\n",
            "2022-05-23 13:45:34,809 DEBUG TRAIN Batch 8/1000 loss 10.617908 loss_att 9.235789 loss_ctc 13.842852 lr 0.00122136 rank 0\n",
            "2022-05-23 13:46:04,439 DEBUG TRAIN Batch 8/1100 loss 14.001734 loss_att 11.942357 loss_ctc 18.806946 lr 0.00122336 rank 0\n",
            "2022-05-23 13:46:34,383 DEBUG TRAIN Batch 8/1200 loss 9.924210 loss_att 9.448892 loss_ctc 11.033285 lr 0.00122536 rank 0\n",
            "2022-05-23 13:47:04,684 DEBUG TRAIN Batch 8/1300 loss 8.855360 loss_att 8.499747 loss_ctc 9.685122 lr 0.00122736 rank 0\n",
            "2022-05-23 13:47:34,636 DEBUG TRAIN Batch 8/1400 loss 18.305353 loss_att 17.049303 loss_ctc 21.236134 lr 0.00122936 rank 0\n",
            "2022-05-23 13:48:05,323 DEBUG TRAIN Batch 8/1500 loss 9.531739 loss_att 8.107016 loss_ctc 12.856092 lr 0.00123136 rank 0\n",
            "2022-05-23 13:48:34,918 DEBUG TRAIN Batch 8/1600 loss 12.714796 loss_att 12.093845 loss_ctc 14.163679 lr 0.00123336 rank 0\n",
            "2022-05-23 13:49:05,059 DEBUG TRAIN Batch 8/1700 loss 13.598664 loss_att 12.841078 loss_ctc 15.366365 lr 0.00123536 rank 0\n",
            "2022-05-23 13:49:34,998 DEBUG TRAIN Batch 8/1800 loss 13.153762 loss_att 12.712767 loss_ctc 14.182751 lr 0.00123736 rank 0\n",
            "2022-05-23 13:50:05,366 DEBUG TRAIN Batch 8/1900 loss 13.009819 loss_att 12.276657 loss_ctc 14.720532 lr 0.00123936 rank 0\n",
            "2022-05-23 13:50:36,105 DEBUG TRAIN Batch 8/2000 loss 13.088829 loss_att 11.353775 loss_ctc 17.137287 lr 0.00124136 rank 0\n",
            "2022-05-23 13:51:05,315 DEBUG TRAIN Batch 8/2100 loss 11.073169 loss_att 10.407248 loss_ctc 12.626983 lr 0.00124336 rank 0\n",
            "2022-05-23 13:51:34,852 DEBUG TRAIN Batch 8/2200 loss 12.254377 loss_att 11.284931 loss_ctc 14.516418 lr 0.00124536 rank 0\n",
            "2022-05-23 13:52:04,533 DEBUG TRAIN Batch 8/2300 loss 15.543786 loss_att 14.686436 loss_ctc 17.544270 lr 0.00124736 rank 0\n",
            "2022-05-23 13:52:34,574 DEBUG TRAIN Batch 8/2400 loss 17.988449 loss_att 17.616909 loss_ctc 18.855379 lr 0.00124936 rank 0\n",
            "2022-05-23 13:53:05,685 DEBUG TRAIN Batch 8/2500 loss 12.495348 loss_att 11.385052 loss_ctc 15.086037 lr 0.00125136 rank 0\n",
            "2022-05-23 13:53:34,839 DEBUG TRAIN Batch 8/2600 loss 13.999895 loss_att 12.990988 loss_ctc 16.354013 lr 0.00125336 rank 0\n",
            "2022-05-23 13:54:04,253 DEBUG TRAIN Batch 8/2700 loss 13.601297 loss_att 12.902382 loss_ctc 15.232103 lr 0.00125536 rank 0\n",
            "2022-05-23 13:54:33,912 DEBUG TRAIN Batch 8/2800 loss 11.550861 loss_att 10.489798 loss_ctc 14.026676 lr 0.00125736 rank 0\n",
            "2022-05-23 13:55:04,531 DEBUG TRAIN Batch 8/2900 loss 16.196053 loss_att 15.331614 loss_ctc 18.213076 lr 0.00125936 rank 0\n",
            "2022-05-23 13:55:35,576 DEBUG TRAIN Batch 8/3000 loss 13.395211 loss_att 12.163940 loss_ctc 16.268177 lr 0.00126136 rank 0\n",
            "2022-05-23 13:56:05,544 DEBUG TRAIN Batch 8/3100 loss 10.228663 loss_att 9.063461 loss_ctc 12.947469 lr 0.00126336 rank 0\n",
            "2022-05-23 13:56:35,431 DEBUG TRAIN Batch 8/3200 loss 8.312382 loss_att 8.105630 loss_ctc 8.794803 lr 0.00126536 rank 0\n",
            "2022-05-23 13:57:05,147 DEBUG TRAIN Batch 8/3300 loss 12.306461 loss_att 11.487360 loss_ctc 14.217695 lr 0.00126736 rank 0\n",
            "2022-05-23 13:57:35,753 DEBUG TRAIN Batch 8/3400 loss 16.132530 loss_att 15.392288 loss_ctc 17.859756 lr 0.00126936 rank 0\n",
            "2022-05-23 13:58:06,070 DEBUG TRAIN Batch 8/3500 loss 12.411483 loss_att 10.661077 loss_ctc 16.495762 lr 0.00127136 rank 0\n",
            "2022-05-23 13:58:35,755 DEBUG TRAIN Batch 8/3600 loss 11.821096 loss_att 10.546738 loss_ctc 14.794601 lr 0.00127336 rank 0\n",
            "2022-05-23 13:59:05,427 DEBUG TRAIN Batch 8/3700 loss 19.077496 loss_att 17.164265 loss_ctc 23.541704 lr 0.00127536 rank 0\n",
            "2022-05-23 13:59:35,866 DEBUG TRAIN Batch 8/3800 loss 12.083857 loss_att 11.134349 loss_ctc 14.299374 lr 0.00127736 rank 0\n",
            "2022-05-23 14:00:06,125 DEBUG TRAIN Batch 8/3900 loss 17.623823 loss_att 16.230671 loss_ctc 20.874512 lr 0.00127936 rank 0\n",
            "2022-05-23 14:00:37,435 DEBUG TRAIN Batch 8/4000 loss 10.602688 loss_att 9.490583 loss_ctc 13.197598 lr 0.00128136 rank 0\n",
            "2022-05-23 14:01:07,290 DEBUG TRAIN Batch 8/4100 loss 10.000849 loss_att 9.035314 loss_ctc 12.253765 lr 0.00128336 rank 0\n",
            "2022-05-23 14:01:37,845 DEBUG TRAIN Batch 8/4200 loss 11.532103 loss_att 11.171736 loss_ctc 12.372959 lr 0.00128536 rank 0\n",
            "2022-05-23 14:02:07,572 DEBUG TRAIN Batch 8/4300 loss 15.457540 loss_att 15.024582 loss_ctc 16.467773 lr 0.00128736 rank 0\n",
            "2022-05-23 14:02:37,513 DEBUG TRAIN Batch 8/4400 loss 14.002365 loss_att 13.570967 loss_ctc 15.008961 lr 0.00128936 rank 0\n",
            "2022-05-23 14:03:08,308 DEBUG TRAIN Batch 8/4500 loss 12.671427 loss_att 11.216899 loss_ctc 16.065323 lr 0.00129136 rank 0\n",
            "2022-05-23 14:03:37,899 DEBUG TRAIN Batch 8/4600 loss 12.706413 loss_att 11.056812 loss_ctc 16.555481 lr 0.00129336 rank 0\n",
            "2022-05-23 14:04:08,024 DEBUG TRAIN Batch 8/4700 loss 11.685290 loss_att 10.465212 loss_ctc 14.532139 lr 0.00129536 rank 0\n",
            "2022-05-23 14:04:37,694 DEBUG TRAIN Batch 8/4800 loss 12.889231 loss_att 11.910226 loss_ctc 15.173574 lr 0.00129736 rank 0\n",
            "2022-05-23 14:05:07,868 DEBUG TRAIN Batch 8/4900 loss 19.002655 loss_att 17.795830 loss_ctc 21.818577 lr 0.00129936 rank 0\n",
            "2022-05-23 14:05:38,718 DEBUG TRAIN Batch 8/5000 loss 8.896071 loss_att 8.059403 loss_ctc 10.848296 lr 0.00130136 rank 0\n",
            "2022-05-23 14:06:08,213 DEBUG TRAIN Batch 8/5100 loss 11.870436 loss_att 11.218781 loss_ctc 13.390961 lr 0.00130336 rank 0\n",
            "2022-05-23 14:06:37,622 DEBUG TRAIN Batch 8/5200 loss 12.784552 loss_att 11.532435 loss_ctc 15.706156 lr 0.00130536 rank 0\n",
            "2022-05-23 14:07:07,985 DEBUG TRAIN Batch 8/5300 loss 15.323866 loss_att 14.076235 loss_ctc 18.235004 lr 0.00130736 rank 0\n",
            "2022-05-23 14:07:37,971 DEBUG TRAIN Batch 8/5400 loss 15.528091 loss_att 14.348024 loss_ctc 18.281580 lr 0.00130936 rank 0\n",
            "2022-05-23 14:08:08,991 DEBUG TRAIN Batch 8/5500 loss 16.531227 loss_att 14.680302 loss_ctc 20.850056 lr 0.00131136 rank 0\n",
            "2022-05-23 14:08:38,276 DEBUG TRAIN Batch 8/5600 loss 13.107768 loss_att 12.427700 loss_ctc 14.694595 lr 0.00131336 rank 0\n",
            "2022-05-23 14:09:07,978 DEBUG TRAIN Batch 8/5700 loss 13.441936 loss_att 12.157887 loss_ctc 16.438053 lr 0.00131536 rank 0\n",
            "2022-05-23 14:09:38,253 DEBUG TRAIN Batch 8/5800 loss 14.009810 loss_att 13.420432 loss_ctc 15.385026 lr 0.00131736 rank 0\n",
            "2022-05-23 14:10:09,015 DEBUG TRAIN Batch 8/5900 loss 13.272459 loss_att 12.453462 loss_ctc 15.183451 lr 0.00131936 rank 0\n",
            "2022-05-23 14:10:40,435 DEBUG TRAIN Batch 8/6000 loss 12.884474 loss_att 11.483818 loss_ctc 16.152672 lr 0.00132136 rank 0\n",
            "2022-05-23 14:11:10,121 DEBUG TRAIN Batch 8/6100 loss 15.854475 loss_att 15.296782 loss_ctc 17.155762 lr 0.00132336 rank 0\n",
            "2022-05-23 14:11:39,823 DEBUG TRAIN Batch 8/6200 loss 13.532721 loss_att 12.320690 loss_ctc 16.360792 lr 0.00132536 rank 0\n",
            "2022-05-23 14:12:09,527 DEBUG TRAIN Batch 8/6300 loss 13.973989 loss_att 12.846963 loss_ctc 16.603718 lr 0.00132736 rank 0\n",
            "2022-05-23 14:12:39,418 DEBUG TRAIN Batch 8/6400 loss 15.616264 loss_att 14.760170 loss_ctc 17.613815 lr 0.00132936 rank 0\n",
            "2022-05-23 14:13:10,489 DEBUG TRAIN Batch 8/6500 loss 12.615578 loss_att 11.832777 loss_ctc 14.442114 lr 0.00133136 rank 0\n",
            "2022-05-23 14:13:40,240 DEBUG TRAIN Batch 8/6600 loss 13.295106 loss_att 12.203835 loss_ctc 15.841408 lr 0.00133336 rank 0\n",
            "2022-05-23 14:14:09,966 DEBUG TRAIN Batch 8/6700 loss 6.543329 loss_att 6.075934 loss_ctc 7.633919 lr 0.00133536 rank 0\n",
            "2022-05-23 14:14:39,889 DEBUG TRAIN Batch 8/6800 loss 13.464912 loss_att 12.686378 loss_ctc 15.281494 lr 0.00133736 rank 0\n",
            "2022-05-23 14:15:10,212 DEBUG TRAIN Batch 8/6900 loss 13.908010 loss_att 13.730080 loss_ctc 14.323183 lr 0.00133936 rank 0\n",
            "2022-05-23 14:15:41,662 DEBUG TRAIN Batch 8/7000 loss 9.944249 loss_att 9.025053 loss_ctc 12.089042 lr 0.00134136 rank 0\n",
            "2022-05-23 14:16:10,905 DEBUG TRAIN Batch 8/7100 loss 10.679627 loss_att 9.869868 loss_ctc 12.569064 lr 0.00134336 rank 0\n",
            "2022-05-23 14:16:40,670 DEBUG TRAIN Batch 8/7200 loss 8.030528 loss_att 7.477051 loss_ctc 9.321975 lr 0.00134536 rank 0\n",
            "2022-05-23 14:17:10,812 DEBUG TRAIN Batch 8/7300 loss 16.164949 loss_att 15.463641 loss_ctc 17.801334 lr 0.00134736 rank 0\n",
            "2022-05-23 14:17:40,564 DEBUG TRAIN Batch 8/7400 loss 14.980139 loss_att 13.749897 loss_ctc 17.850704 lr 0.00134936 rank 0\n",
            "2022-05-23 14:18:11,524 DEBUG TRAIN Batch 8/7500 loss 10.849575 loss_att 9.692665 loss_ctc 13.549030 lr 0.00135136 rank 0\n",
            "2022-05-23 14:18:17,816 DEBUG CV Batch 8/0 loss 5.615265 loss_att 5.036847 loss_ctc 6.964907 history loss 5.284955 rank 0\n",
            "2022-05-23 14:18:29,955 DEBUG CV Batch 8/100 loss 4.670525 loss_att 4.576456 loss_ctc 4.890020 history loss 9.168535 rank 0\n",
            "2022-05-23 14:18:40,998 DEBUG CV Batch 8/200 loss 8.447732 loss_att 8.140142 loss_ctc 9.165441 history loss 9.187963 rank 0\n",
            "2022-05-23 14:18:52,604 DEBUG CV Batch 8/300 loss 7.128267 loss_att 6.966810 loss_ctc 7.504999 history loss 8.604009 rank 0\n",
            "2022-05-23 14:19:04,917 DEBUG CV Batch 8/400 loss 12.835037 loss_att 13.103835 loss_ctc 12.207842 history loss 7.949092 rank 0\n",
            "2022-05-23 14:19:17,702 DEBUG CV Batch 8/500 loss 5.227604 loss_att 4.882185 loss_ctc 6.033581 history loss 7.853688 rank 0\n",
            "2022-05-23 14:19:30,045 DEBUG CV Batch 8/600 loss 5.793677 loss_att 5.816589 loss_ctc 5.740217 history loss 7.801465 rank 0\n",
            "2022-05-23 14:19:41,655 DEBUG CV Batch 8/700 loss 5.552845 loss_att 5.251665 loss_ctc 6.255596 history loss 7.667329 rank 0\n",
            "2022-05-23 14:19:53,754 DEBUG CV Batch 8/800 loss 8.333900 loss_att 7.980871 loss_ctc 9.157635 history loss 7.611994 rank 0\n",
            "2022-05-23 14:20:05,683 INFO Epoch 8 CV info cv_loss 7.620419348560109\n",
            "2022-05-23 14:20:05,684 INFO Checkpoint: save to checkpoint exp/conformer/8.pt\n",
            "2022-05-23 14:20:06,059 INFO Epoch 9 TRAIN info lr 0.00135144\n",
            "2022-05-23 14:20:06,062 INFO using accumulate grad, new batch size is 4 times larger than before\n",
            "2022-05-23 14:20:33,068 DEBUG TRAIN Batch 9/0 loss 9.917830 loss_att 8.656368 loss_ctc 12.861238 lr 0.00135152 rank 0\n",
            "2022-05-23 14:21:02,265 DEBUG TRAIN Batch 9/100 loss 8.066990 loss_att 7.640683 loss_ctc 9.061707 lr 0.00135352 rank 0\n",
            "2022-05-23 14:21:31,886 DEBUG TRAIN Batch 9/200 loss 10.664133 loss_att 10.063452 loss_ctc 12.065722 lr 0.00135552 rank 0\n",
            "2022-05-23 14:22:01,445 DEBUG TRAIN Batch 9/300 loss 10.406960 loss_att 9.892538 loss_ctc 11.607277 lr 0.00135752 rank 0\n",
            "2022-05-23 14:22:32,203 DEBUG TRAIN Batch 9/400 loss 14.626427 loss_att 14.073214 loss_ctc 15.917258 lr 0.00135952 rank 0\n",
            "2022-05-23 14:23:02,668 DEBUG TRAIN Batch 9/500 loss 7.831997 loss_att 6.646105 loss_ctc 10.599078 lr 0.00136152 rank 0\n",
            "2022-05-23 14:23:32,799 DEBUG TRAIN Batch 9/600 loss 9.390098 loss_att 8.390440 loss_ctc 11.722631 lr 0.00136352 rank 0\n",
            "2022-05-23 14:24:02,560 DEBUG TRAIN Batch 9/700 loss 13.840700 loss_att 12.683457 loss_ctc 16.540932 lr 0.00136552 rank 0\n",
            "2022-05-23 14:24:32,187 DEBUG TRAIN Batch 9/800 loss 11.949091 loss_att 10.744324 loss_ctc 14.760212 lr 0.00136752 rank 0\n",
            "2022-05-23 14:25:02,588 DEBUG TRAIN Batch 9/900 loss 14.985691 loss_att 13.735648 loss_ctc 17.902456 lr 0.00136952 rank 0\n",
            "2022-05-23 14:25:33,503 DEBUG TRAIN Batch 9/1000 loss 11.622837 loss_att 9.750084 loss_ctc 15.992596 lr 0.00137152 rank 0\n",
            "2022-05-23 14:26:03,054 DEBUG TRAIN Batch 9/1100 loss 13.939745 loss_att 12.549107 loss_ctc 17.184566 lr 0.00137352 rank 0\n",
            "2022-05-23 14:26:32,564 DEBUG TRAIN Batch 9/1200 loss 9.653931 loss_att 8.796901 loss_ctc 11.653666 lr 0.00137552 rank 0\n",
            "2022-05-23 14:27:02,208 DEBUG TRAIN Batch 9/1300 loss 10.086961 loss_att 9.635452 loss_ctc 11.140482 lr 0.00137752 rank 0\n",
            "2022-05-23 14:27:32,489 DEBUG TRAIN Batch 9/1400 loss 11.228188 loss_att 10.737976 loss_ctc 12.372014 lr 0.00137952 rank 0\n",
            "2022-05-23 14:28:03,200 DEBUG TRAIN Batch 9/1500 loss 16.405115 loss_att 14.948038 loss_ctc 19.804960 lr 0.00138152 rank 0\n",
            "2022-05-23 14:28:32,898 DEBUG TRAIN Batch 9/1600 loss 14.169519 loss_att 12.305911 loss_ctc 18.517939 lr 0.00138352 rank 0\n",
            "2022-05-23 14:29:02,809 DEBUG TRAIN Batch 9/1700 loss 12.124279 loss_att 10.760164 loss_ctc 15.307211 lr 0.00138552 rank 0\n",
            "2022-05-23 14:29:32,906 DEBUG TRAIN Batch 9/1800 loss 9.663975 loss_att 9.246208 loss_ctc 10.638763 lr 0.00138752 rank 0\n",
            "2022-05-23 14:30:03,087 DEBUG TRAIN Batch 9/1900 loss 15.388987 loss_att 14.677180 loss_ctc 17.049870 lr 0.00138952 rank 0\n",
            "2022-05-23 14:30:33,794 DEBUG TRAIN Batch 9/2000 loss 13.446906 loss_att 12.080584 loss_ctc 16.634991 lr 0.00139152 rank 0\n",
            "2022-05-23 14:31:03,666 DEBUG TRAIN Batch 9/2100 loss 8.429549 loss_att 7.803384 loss_ctc 9.890600 lr 0.00139352 rank 0\n",
            "2022-05-23 14:31:33,080 DEBUG TRAIN Batch 9/2200 loss 13.065372 loss_att 11.773693 loss_ctc 16.079292 lr 0.00139552 rank 0\n",
            "2022-05-23 14:32:03,062 DEBUG TRAIN Batch 9/2300 loss 11.909403 loss_att 10.457776 loss_ctc 15.296531 lr 0.00139752 rank 0\n",
            "2022-05-23 14:32:32,843 DEBUG TRAIN Batch 9/2400 loss 14.721809 loss_att 13.581052 loss_ctc 17.383579 lr 0.00139952 rank 0\n",
            "2022-05-23 14:33:03,539 DEBUG TRAIN Batch 9/2500 loss 8.117995 loss_att 7.333878 loss_ctc 9.947603 lr 0.00140152 rank 0\n",
            "2022-05-23 14:33:33,220 DEBUG TRAIN Batch 9/2600 loss 9.419166 loss_att 9.219639 loss_ctc 9.884727 lr 0.00140352 rank 0\n",
            "2022-05-23 14:34:02,892 DEBUG TRAIN Batch 9/2700 loss 11.200088 loss_att 10.472338 loss_ctc 12.898169 lr 0.00140552 rank 0\n",
            "2022-05-23 14:34:32,717 DEBUG TRAIN Batch 9/2800 loss 14.649864 loss_att 12.964745 loss_ctc 18.581808 lr 0.00140752 rank 0\n",
            "2022-05-23 14:35:03,053 DEBUG TRAIN Batch 9/2900 loss 16.457983 loss_att 15.489454 loss_ctc 18.717882 lr 0.00140952 rank 0\n",
            "2022-05-23 14:35:34,007 DEBUG TRAIN Batch 9/3000 loss 9.922083 loss_att 8.717781 loss_ctc 12.732121 lr 0.00141152 rank 0\n",
            "2022-05-23 14:36:03,642 DEBUG TRAIN Batch 9/3100 loss 11.422283 loss_att 9.937860 loss_ctc 14.885934 lr 0.00141352 rank 0\n",
            "2022-05-23 14:36:33,339 DEBUG TRAIN Batch 9/3200 loss 11.812640 loss_att 11.345116 loss_ctc 12.903530 lr 0.00141552 rank 0\n",
            "2022-05-23 14:37:03,512 DEBUG TRAIN Batch 9/3300 loss 13.345772 loss_att 12.279495 loss_ctc 15.833751 lr 0.00141752 rank 0\n",
            "2022-05-23 14:37:33,770 DEBUG TRAIN Batch 9/3400 loss 11.448827 loss_att 11.303128 loss_ctc 11.788792 lr 0.00141952 rank 0\n",
            "2022-05-23 14:38:04,587 DEBUG TRAIN Batch 9/3500 loss 8.560560 loss_att 7.814658 loss_ctc 10.301000 lr 0.00142152 rank 0\n",
            "2022-05-23 14:38:33,957 DEBUG TRAIN Batch 9/3600 loss 9.527316 loss_att 8.962165 loss_ctc 10.846003 lr 0.00142352 rank 0\n",
            "2022-05-23 14:39:03,402 DEBUG TRAIN Batch 9/3700 loss 12.692535 loss_att 11.953354 loss_ctc 14.417290 lr 0.00142552 rank 0\n",
            "2022-05-23 14:39:33,609 DEBUG TRAIN Batch 9/3800 loss 14.287794 loss_att 13.403152 loss_ctc 16.351961 lr 0.00142752 rank 0\n",
            "2022-05-23 14:40:03,556 DEBUG TRAIN Batch 9/3900 loss 10.808692 loss_att 9.955610 loss_ctc 12.799215 lr 0.00142952 rank 0\n",
            "2022-05-23 14:40:34,268 DEBUG TRAIN Batch 9/4000 loss 10.893287 loss_att 10.049957 loss_ctc 12.861056 lr 0.00143152 rank 0\n",
            "2022-05-23 14:41:03,976 DEBUG TRAIN Batch 9/4100 loss 12.377412 loss_att 11.400672 loss_ctc 14.656471 lr 0.00143352 rank 0\n",
            "2022-05-23 14:41:33,531 DEBUG TRAIN Batch 9/4200 loss 12.325214 loss_att 10.904739 loss_ctc 15.639657 lr 0.00143552 rank 0\n",
            "2022-05-23 14:42:03,459 DEBUG TRAIN Batch 9/4300 loss 11.941645 loss_att 10.725587 loss_ctc 14.779113 lr 0.00143752 rank 0\n",
            "2022-05-23 14:42:34,074 DEBUG TRAIN Batch 9/4400 loss 17.328318 loss_att 16.416862 loss_ctc 19.455048 lr 0.00143952 rank 0\n",
            "2022-05-23 14:43:05,249 DEBUG TRAIN Batch 9/4500 loss 9.193668 loss_att 8.170893 loss_ctc 11.580146 lr 0.00144152 rank 0\n",
            "2022-05-23 14:43:34,805 DEBUG TRAIN Batch 9/4600 loss 11.367309 loss_att 10.903773 loss_ctc 12.448890 lr 0.00144352 rank 0\n",
            "2022-05-23 14:44:04,260 DEBUG TRAIN Batch 9/4700 loss 14.221212 loss_att 12.447577 loss_ctc 18.359695 lr 0.00144552 rank 0\n",
            "2022-05-23 14:44:34,046 DEBUG TRAIN Batch 9/4800 loss 14.692234 loss_att 13.270266 loss_ctc 18.010162 lr 0.00144752 rank 0\n",
            "2022-05-23 14:45:04,137 DEBUG TRAIN Batch 9/4900 loss 14.192051 loss_att 13.716349 loss_ctc 15.302023 lr 0.00144952 rank 0\n",
            "2022-05-23 14:45:34,920 DEBUG TRAIN Batch 9/5000 loss 12.687695 loss_att 11.410953 loss_ctc 15.666760 lr 0.00145152 rank 0\n",
            "2022-05-23 14:46:04,374 DEBUG TRAIN Batch 9/5100 loss 9.083940 loss_att 7.899467 loss_ctc 11.847707 lr 0.00145352 rank 0\n",
            "2022-05-23 14:46:33,841 DEBUG TRAIN Batch 9/5200 loss 13.919085 loss_att 12.753919 loss_ctc 16.637806 lr 0.00145552 rank 0\n",
            "2022-05-23 14:47:03,426 DEBUG TRAIN Batch 9/5300 loss 13.795197 loss_att 12.463065 loss_ctc 16.903500 lr 0.00145752 rank 0\n",
            "2022-05-23 14:47:33,624 DEBUG TRAIN Batch 9/5400 loss 18.498901 loss_att 17.991177 loss_ctc 19.683590 lr 0.00145952 rank 0\n",
            "2022-05-23 14:48:04,686 DEBUG TRAIN Batch 9/5500 loss 8.253374 loss_att 7.337876 loss_ctc 10.389537 lr 0.00146152 rank 0\n",
            "2022-05-23 14:48:33,870 DEBUG TRAIN Batch 9/5600 loss 10.117993 loss_att 9.471429 loss_ctc 11.626644 lr 0.00146352 rank 0\n",
            "2022-05-23 14:49:03,288 DEBUG TRAIN Batch 9/5700 loss 10.901983 loss_att 10.159670 loss_ctc 12.634048 lr 0.00146552 rank 0\n",
            "2022-05-23 14:49:33,206 DEBUG TRAIN Batch 9/5800 loss 15.025273 loss_att 13.788706 loss_ctc 17.910597 lr 0.00146752 rank 0\n",
            "2022-05-23 14:50:03,665 DEBUG TRAIN Batch 9/5900 loss 12.246808 loss_att 12.118749 loss_ctc 12.545614 lr 0.00146952 rank 0\n",
            "2022-05-23 14:50:34,607 DEBUG TRAIN Batch 9/6000 loss 12.025942 loss_att 10.550577 loss_ctc 15.468457 lr 0.00147152 rank 0\n",
            "2022-05-23 14:51:03,803 DEBUG TRAIN Batch 9/6100 loss 10.437349 loss_att 9.562769 loss_ctc 12.478035 lr 0.00147352 rank 0\n",
            "2022-05-23 14:51:32,938 DEBUG TRAIN Batch 9/6200 loss 9.316700 loss_att 8.347944 loss_ctc 11.577132 lr 0.00147552 rank 0\n",
            "2022-05-23 14:52:03,166 DEBUG TRAIN Batch 9/6300 loss 14.555696 loss_att 13.903574 loss_ctc 16.077314 lr 0.00147752 rank 0\n",
            "2022-05-23 14:52:33,020 DEBUG TRAIN Batch 9/6400 loss 15.643572 loss_att 14.695879 loss_ctc 17.854855 lr 0.00147952 rank 0\n",
            "2022-05-23 14:53:04,218 DEBUG TRAIN Batch 9/6500 loss 13.761709 loss_att 11.834103 loss_ctc 18.259457 lr 0.00148152 rank 0\n",
            "2022-05-23 14:53:33,073 DEBUG TRAIN Batch 9/6600 loss 8.982675 loss_att 7.941619 loss_ctc 11.411802 lr 0.00148352 rank 0\n",
            "2022-05-23 14:54:03,017 DEBUG TRAIN Batch 9/6700 loss 11.972841 loss_att 10.684923 loss_ctc 14.977982 lr 0.00148552 rank 0\n",
            "2022-05-23 14:54:32,672 DEBUG TRAIN Batch 9/6800 loss 17.206352 loss_att 16.134178 loss_ctc 19.708096 lr 0.00148752 rank 0\n",
            "2022-05-23 14:55:02,797 DEBUG TRAIN Batch 9/6900 loss 16.816339 loss_att 16.879276 loss_ctc 16.669485 lr 0.00148952 rank 0\n",
            "2022-05-23 14:55:33,053 DEBUG TRAIN Batch 9/7000 loss 11.172915 loss_att 9.585072 loss_ctc 14.877879 lr 0.00149152 rank 0\n",
            "2022-05-23 14:56:02,983 DEBUG TRAIN Batch 9/7100 loss 10.680721 loss_att 10.353965 loss_ctc 11.443152 lr 0.00149352 rank 0\n",
            "2022-05-23 14:56:33,070 DEBUG TRAIN Batch 9/7200 loss 12.829793 loss_att 11.393071 loss_ctc 16.182144 lr 0.00149552 rank 0\n",
            "2022-05-23 14:57:03,043 DEBUG TRAIN Batch 9/7300 loss 10.768805 loss_att 10.104971 loss_ctc 12.317749 lr 0.00149752 rank 0\n",
            "2022-05-23 14:57:33,188 DEBUG TRAIN Batch 9/7400 loss 13.999292 loss_att 12.955893 loss_ctc 16.433895 lr 0.00149952 rank 0\n",
            "2022-05-23 14:58:03,467 DEBUG TRAIN Batch 9/7500 loss 10.370256 loss_att 8.960863 loss_ctc 13.658841 lr 0.00150152 rank 0\n",
            "2022-05-23 14:58:09,691 DEBUG CV Batch 9/0 loss 5.276646 loss_att 4.751425 loss_ctc 6.502161 history loss 4.966255 rank 0\n",
            "2022-05-23 14:58:21,179 DEBUG CV Batch 9/100 loss 4.390557 loss_att 4.289099 loss_ctc 4.627293 history loss 8.398390 rank 0\n",
            "2022-05-23 14:58:31,815 DEBUG CV Batch 9/200 loss 6.776543 loss_att 6.443187 loss_ctc 7.554375 history loss 8.458482 rank 0\n",
            "2022-05-23 14:58:42,856 DEBUG CV Batch 9/300 loss 5.560885 loss_att 5.662040 loss_ctc 5.324857 history loss 7.976946 rank 0\n",
            "2022-05-23 14:58:54,564 DEBUG CV Batch 9/400 loss 12.554747 loss_att 12.160154 loss_ctc 13.475461 history loss 7.391469 rank 0\n",
            "2022-05-23 14:59:06,825 DEBUG CV Batch 9/500 loss 4.740839 loss_att 4.509270 loss_ctc 5.281168 history loss 7.344704 rank 0\n",
            "2022-05-23 14:59:18,407 DEBUG CV Batch 9/600 loss 5.620732 loss_att 5.615467 loss_ctc 5.633017 history loss 7.306611 rank 0\n",
            "2022-05-23 14:59:29,524 DEBUG CV Batch 9/700 loss 6.015162 loss_att 5.688799 loss_ctc 6.776675 history loss 7.184989 rank 0\n",
            "2022-05-23 14:59:40,985 DEBUG CV Batch 9/800 loss 6.864547 loss_att 6.562325 loss_ctc 7.569732 history loss 7.142448 rank 0\n",
            "2022-05-23 14:59:52,637 INFO Epoch 9 CV info cv_loss 7.162384459084906\n",
            "2022-05-23 14:59:52,637 INFO Checkpoint: save to checkpoint exp/conformer/9.pt\n",
            "2022-05-23 14:59:52,987 INFO Epoch 10 TRAIN info lr 0.0015015999999999999\n",
            "2022-05-23 14:59:52,988 INFO using accumulate grad, new batch size is 4 times larger than before\n",
            "2022-05-23 15:00:18,013 DEBUG TRAIN Batch 10/0 loss 9.628409 loss_att 8.265007 loss_ctc 12.809681 lr 0.00150168 rank 0\n",
            "2022-05-23 15:00:47,901 DEBUG TRAIN Batch 10/100 loss 10.386082 loss_att 8.808762 loss_ctc 14.066496 lr 0.00150368 rank 0\n",
            "2022-05-23 15:01:17,717 DEBUG TRAIN Batch 10/200 loss 9.784163 loss_att 8.793940 loss_ctc 12.094687 lr 0.00150568 rank 0\n",
            "2022-05-23 15:01:46,962 DEBUG TRAIN Batch 10/300 loss 13.238069 loss_att 12.259588 loss_ctc 15.521189 lr 0.00150768 rank 0\n",
            "2022-05-23 15:02:17,201 DEBUG TRAIN Batch 10/400 loss 14.778833 loss_att 13.826804 loss_ctc 17.000235 lr 0.00150968 rank 0\n",
            "2022-05-23 15:02:48,151 DEBUG TRAIN Batch 10/500 loss 13.183873 loss_att 11.944305 loss_ctc 16.076199 lr 0.00151168 rank 0\n",
            "2022-05-23 15:03:17,613 DEBUG TRAIN Batch 10/600 loss 10.436881 loss_att 9.404867 loss_ctc 12.844913 lr 0.00151368 rank 0\n",
            "2022-05-23 15:03:47,497 DEBUG TRAIN Batch 10/700 loss 8.398685 loss_att 7.643318 loss_ctc 10.161206 lr 0.00151568 rank 0\n",
            "2022-05-23 15:04:17,277 DEBUG TRAIN Batch 10/800 loss 8.688749 loss_att 8.286346 loss_ctc 9.627691 lr 0.00151768 rank 0\n",
            "2022-05-23 15:04:47,714 DEBUG TRAIN Batch 10/900 loss 13.863031 loss_att 13.174913 loss_ctc 15.468639 lr 0.00151968 rank 0\n",
            "2022-05-23 15:05:18,746 DEBUG TRAIN Batch 10/1000 loss 11.140576 loss_att 9.580414 loss_ctc 14.780954 lr 0.00152168 rank 0\n",
            "2022-05-23 15:05:48,108 DEBUG TRAIN Batch 10/1100 loss 10.798816 loss_att 10.191141 loss_ctc 12.216722 lr 0.00152368 rank 0\n",
            "2022-05-23 15:06:18,207 DEBUG TRAIN Batch 10/1200 loss 11.176107 loss_att 10.503017 loss_ctc 12.746651 lr 0.00152568 rank 0\n",
            "2022-05-23 15:06:47,937 DEBUG TRAIN Batch 10/1300 loss 12.759525 loss_att 11.601174 loss_ctc 15.462343 lr 0.00152768 rank 0\n",
            "2022-05-23 15:07:17,786 DEBUG TRAIN Batch 10/1400 loss 9.806621 loss_att 9.404211 loss_ctc 10.745575 lr 0.00152968 rank 0\n",
            "2022-05-23 15:07:48,327 DEBUG TRAIN Batch 10/1500 loss 13.669674 loss_att 12.232685 loss_ctc 17.022648 lr 0.00153168 rank 0\n",
            "2022-05-23 15:08:17,982 DEBUG TRAIN Batch 10/1600 loss 8.974290 loss_att 8.394356 loss_ctc 10.327469 lr 0.00153368 rank 0\n",
            "2022-05-23 15:08:47,478 DEBUG TRAIN Batch 10/1700 loss 10.238895 loss_att 9.337185 loss_ctc 12.342887 lr 0.00153568 rank 0\n",
            "2022-05-23 15:09:17,142 DEBUG TRAIN Batch 10/1800 loss 11.967674 loss_att 11.415996 loss_ctc 13.254924 lr 0.00153768 rank 0\n",
            "2022-05-23 15:09:47,509 DEBUG TRAIN Batch 10/1900 loss 13.871466 loss_att 13.358610 loss_ctc 15.068129 lr 0.00153968 rank 0\n",
            "2022-05-23 15:10:18,239 DEBUG TRAIN Batch 10/2000 loss 11.467163 loss_att 9.929109 loss_ctc 15.055956 lr 0.00154168 rank 0\n",
            "2022-05-23 15:10:47,956 DEBUG TRAIN Batch 10/2100 loss 10.655296 loss_att 9.871963 loss_ctc 12.483073 lr 0.00154368 rank 0\n",
            "2022-05-23 15:11:17,241 DEBUG TRAIN Batch 10/2200 loss 8.842252 loss_att 8.615582 loss_ctc 9.371149 lr 0.00154568 rank 0\n",
            "2022-05-23 15:11:47,157 DEBUG TRAIN Batch 10/2300 loss 13.688679 loss_att 12.718853 loss_ctc 15.951603 lr 0.00154768 rank 0\n",
            "2022-05-23 15:12:17,353 DEBUG TRAIN Batch 10/2400 loss 14.944858 loss_att 13.958224 loss_ctc 17.247002 lr 0.00154968 rank 0\n",
            "2022-05-23 15:12:47,919 DEBUG TRAIN Batch 10/2500 loss 9.681437 loss_att 8.549818 loss_ctc 12.321879 lr 0.00155168 rank 0\n",
            "2022-05-23 15:13:17,568 DEBUG TRAIN Batch 10/2600 loss 9.362844 loss_att 8.462793 loss_ctc 11.462962 lr 0.00155368 rank 0\n",
            "2022-05-23 15:13:46,978 DEBUG TRAIN Batch 10/2700 loss 7.970234 loss_att 7.714371 loss_ctc 8.567250 lr 0.00155568 rank 0\n",
            "2022-05-23 15:14:16,664 DEBUG TRAIN Batch 10/2800 loss 12.266598 loss_att 11.462882 loss_ctc 14.141933 lr 0.00155768 rank 0\n",
            "2022-05-23 15:14:46,788 DEBUG TRAIN Batch 10/2900 loss 14.371805 loss_att 13.734613 loss_ctc 15.858584 lr 0.00155968 rank 0\n",
            "2022-05-23 15:15:17,303 DEBUG TRAIN Batch 10/3000 loss 8.340408 loss_att 7.592412 loss_ctc 10.085732 lr 0.00156168 rank 0\n",
            "2022-05-23 15:15:46,624 DEBUG TRAIN Batch 10/3100 loss 12.090467 loss_att 11.205051 loss_ctc 14.156439 lr 0.00156368 rank 0\n",
            "2022-05-23 15:16:15,983 DEBUG TRAIN Batch 10/3200 loss 12.951451 loss_att 11.733963 loss_ctc 15.792258 lr 0.00156568 rank 0\n",
            "2022-05-23 15:16:45,637 DEBUG TRAIN Batch 10/3300 loss 13.396879 loss_att 12.320971 loss_ctc 15.907330 lr 0.00156768 rank 0\n",
            "2022-05-23 15:17:15,438 DEBUG TRAIN Batch 10/3400 loss 19.431963 loss_att 19.171402 loss_ctc 20.039940 lr 0.00156968 rank 0\n",
            "2022-05-23 15:17:46,293 DEBUG TRAIN Batch 10/3500 loss 10.640530 loss_att 9.360161 loss_ctc 13.628058 lr 0.00157168 rank 0\n",
            "2022-05-23 15:18:15,758 DEBUG TRAIN Batch 10/3600 loss 16.796356 loss_att 15.159836 loss_ctc 20.614906 lr 0.00157368 rank 0\n",
            "2022-05-23 15:18:45,800 DEBUG TRAIN Batch 10/3700 loss 9.502635 loss_att 8.338223 loss_ctc 12.219596 lr 0.00157568 rank 0\n",
            "2022-05-23 15:19:15,834 DEBUG TRAIN Batch 10/3800 loss 13.360604 loss_att 12.330569 loss_ctc 15.764020 lr 0.00157768 rank 0\n",
            "2022-05-23 15:19:45,872 DEBUG TRAIN Batch 10/3900 loss 11.367416 loss_att 11.085352 loss_ctc 12.025568 lr 0.00157968 rank 0\n",
            "2022-05-23 15:20:16,667 DEBUG TRAIN Batch 10/4000 loss 7.093104 loss_att 6.836067 loss_ctc 7.692858 lr 0.00158168 rank 0\n",
            "2022-05-23 15:20:46,233 DEBUG TRAIN Batch 10/4100 loss 13.200823 loss_att 12.086884 loss_ctc 15.800011 lr 0.00158368 rank 0\n",
            "2022-05-23 15:21:16,002 DEBUG TRAIN Batch 10/4200 loss 11.725058 loss_att 11.323995 loss_ctc 12.660872 lr 0.00158568 rank 0\n",
            "2022-05-23 15:21:45,841 DEBUG TRAIN Batch 10/4300 loss 11.947893 loss_att 10.922146 loss_ctc 14.341303 lr 0.00158768 rank 0\n",
            "2022-05-23 15:22:15,638 DEBUG TRAIN Batch 10/4400 loss 18.678484 loss_att 17.680378 loss_ctc 21.007397 lr 0.00158968 rank 0\n",
            "2022-05-23 15:22:46,774 DEBUG TRAIN Batch 10/4500 loss 11.201998 loss_att 9.899022 loss_ctc 14.242275 lr 0.00159168 rank 0\n",
            "2022-05-23 15:23:16,145 DEBUG TRAIN Batch 10/4600 loss 11.045350 loss_att 9.971548 loss_ctc 13.550888 lr 0.00159368 rank 0\n",
            "2022-05-23 15:23:45,760 DEBUG TRAIN Batch 10/4700 loss 10.752478 loss_att 10.057629 loss_ctc 12.373794 lr 0.00159568 rank 0\n",
            "2022-05-23 15:24:15,633 DEBUG TRAIN Batch 10/4800 loss 11.722919 loss_att 11.117338 loss_ctc 13.135939 lr 0.00159768 rank 0\n",
            "2022-05-23 15:24:45,867 DEBUG TRAIN Batch 10/4900 loss 17.704708 loss_att 17.457874 loss_ctc 18.280659 lr 0.00159968 rank 0\n",
            "2022-05-23 15:25:16,770 DEBUG TRAIN Batch 10/5000 loss 8.711845 loss_att 7.921847 loss_ctc 10.555174 lr 0.00160168 rank 0\n",
            "2022-05-23 15:25:46,717 DEBUG TRAIN Batch 10/5100 loss 12.246802 loss_att 11.007652 loss_ctc 15.138151 lr 0.00160368 rank 0\n",
            "2022-05-23 15:26:16,040 DEBUG TRAIN Batch 10/5200 loss 10.746040 loss_att 9.506385 loss_ctc 13.638567 lr 0.00160568 rank 0\n",
            "2022-05-23 15:26:45,640 DEBUG TRAIN Batch 10/5300 loss 12.997375 loss_att 12.412432 loss_ctc 14.362247 lr 0.00160768 rank 0\n",
            "2022-05-23 15:27:15,624 DEBUG TRAIN Batch 10/5400 loss 14.690769 loss_att 13.568192 loss_ctc 17.310116 lr 0.00160968 rank 0\n",
            "2022-05-23 15:27:46,317 DEBUG TRAIN Batch 10/5500 loss 11.068014 loss_att 9.056032 loss_ctc 15.762636 lr 0.00161168 rank 0\n",
            "2022-05-23 15:28:16,234 DEBUG TRAIN Batch 10/5600 loss 10.199860 loss_att 9.531878 loss_ctc 11.758486 lr 0.00161368 rank 0\n",
            "2022-05-23 15:28:46,105 DEBUG TRAIN Batch 10/5700 loss 11.368218 loss_att 10.727338 loss_ctc 12.863606 lr 0.00161568 rank 0\n",
            "2022-05-23 15:29:16,203 DEBUG TRAIN Batch 10/5800 loss 17.314369 loss_att 15.305920 loss_ctc 22.000748 lr 0.00161768 rank 0\n",
            "2022-05-23 15:29:45,890 DEBUG TRAIN Batch 10/5900 loss 14.730578 loss_att 13.618320 loss_ctc 17.325846 lr 0.00161968 rank 0\n",
            "2022-05-23 15:30:17,260 DEBUG TRAIN Batch 10/6000 loss 11.590875 loss_att 9.874700 loss_ctc 15.595284 lr 0.00162168 rank 0\n",
            "2022-05-23 15:30:46,607 DEBUG TRAIN Batch 10/6100 loss 10.949456 loss_att 10.258905 loss_ctc 12.560740 lr 0.00162368 rank 0\n",
            "2022-05-23 15:31:16,390 DEBUG TRAIN Batch 10/6200 loss 9.657533 loss_att 9.368776 loss_ctc 10.331298 lr 0.00162568 rank 0\n",
            "2022-05-23 15:31:46,226 DEBUG TRAIN Batch 10/6300 loss 12.722760 loss_att 11.752262 loss_ctc 14.987255 lr 0.00162768 rank 0\n",
            "2022-05-23 15:32:16,690 DEBUG TRAIN Batch 10/6400 loss 10.195148 loss_att 9.904467 loss_ctc 10.873407 lr 0.00162968 rank 0\n",
            "2022-05-23 15:32:47,021 DEBUG TRAIN Batch 10/6500 loss 9.546291 loss_att 8.488491 loss_ctc 12.014492 lr 0.00163168 rank 0\n",
            "2022-05-23 15:33:16,013 DEBUG TRAIN Batch 10/6600 loss 9.182665 loss_att 9.113411 loss_ctc 9.344259 lr 0.00163368 rank 0\n",
            "2022-05-23 15:33:45,954 DEBUG TRAIN Batch 10/6700 loss 13.186448 loss_att 12.847324 loss_ctc 13.977736 lr 0.00163568 rank 0\n",
            "2022-05-23 15:34:16,015 DEBUG TRAIN Batch 10/6800 loss 12.933146 loss_att 12.290280 loss_ctc 14.433165 lr 0.00163768 rank 0\n",
            "2022-05-23 15:34:45,861 DEBUG TRAIN Batch 10/6900 loss 13.724659 loss_att 13.254744 loss_ctc 14.821130 lr 0.00163968 rank 0\n",
            "2022-05-23 15:35:17,077 DEBUG TRAIN Batch 10/7000 loss 11.860062 loss_att 9.756184 loss_ctc 16.769108 lr 0.00164168 rank 0\n",
            "2022-05-23 15:35:46,617 DEBUG TRAIN Batch 10/7100 loss 8.190364 loss_att 7.726479 loss_ctc 9.272760 lr 0.00164368 rank 0\n",
            "2022-05-23 15:36:16,020 DEBUG TRAIN Batch 10/7200 loss 11.087532 loss_att 10.673608 loss_ctc 12.053354 lr 0.00164568 rank 0\n",
            "2022-05-23 15:36:45,549 DEBUG TRAIN Batch 10/7300 loss 12.385698 loss_att 11.666151 loss_ctc 14.064644 lr 0.00164768 rank 0\n",
            "2022-05-23 15:37:15,982 DEBUG TRAIN Batch 10/7400 loss 15.152527 loss_att 14.645433 loss_ctc 16.335743 lr 0.00164968 rank 0\n",
            "2022-05-23 15:37:46,607 DEBUG TRAIN Batch 10/7500 loss 10.827541 loss_att 9.601029 loss_ctc 13.689402 lr 0.00165168 rank 0\n",
            "2022-05-23 15:37:52,588 DEBUG CV Batch 10/0 loss 5.482127 loss_att 5.262391 loss_ctc 5.994845 history loss 5.159649 rank 0\n",
            "2022-05-23 15:38:04,042 DEBUG CV Batch 10/100 loss 3.604829 loss_att 3.685875 loss_ctc 3.415722 history loss 8.228961 rank 0\n",
            "2022-05-23 15:38:14,541 DEBUG CV Batch 10/200 loss 6.634478 loss_att 6.647361 loss_ctc 6.604418 history loss 8.218757 rank 0\n",
            "2022-05-23 15:38:25,567 DEBUG CV Batch 10/300 loss 5.181361 loss_att 5.350592 loss_ctc 4.786489 history loss 7.761611 rank 0\n",
            "2022-05-23 15:38:37,423 DEBUG CV Batch 10/400 loss 11.715835 loss_att 11.897760 loss_ctc 11.291342 history loss 7.185039 rank 0\n",
            "2022-05-23 15:38:49,655 DEBUG CV Batch 10/500 loss 4.540222 loss_att 4.555342 loss_ctc 4.504941 history loss 7.112144 rank 0\n",
            "2022-05-23 15:39:01,484 DEBUG CV Batch 10/600 loss 5.280447 loss_att 5.326890 loss_ctc 5.172082 history loss 7.029705 rank 0\n",
            "2022-05-23 15:39:12,632 DEBUG CV Batch 10/700 loss 5.368109 loss_att 5.297120 loss_ctc 5.533748 history loss 6.898768 rank 0\n",
            "2022-05-23 15:39:24,499 DEBUG CV Batch 10/800 loss 7.375001 loss_att 7.145795 loss_ctc 7.909818 history loss 6.865769 rank 0\n",
            "2022-05-23 15:39:36,410 INFO Epoch 10 CV info cv_loss 6.8845096512783375\n",
            "2022-05-23 15:39:36,410 INFO Checkpoint: save to checkpoint exp/conformer/10.pt\n",
            "2022-05-23 15:39:36,746 INFO Epoch 11 TRAIN info lr 0.00165176\n",
            "2022-05-23 15:39:36,748 INFO using accumulate grad, new batch size is 4 times larger than before\n",
            "2022-05-23 15:40:01,728 DEBUG TRAIN Batch 11/0 loss 12.078053 loss_att 10.498846 loss_ctc 15.762865 lr 0.00165184 rank 0\n",
            "2022-05-23 15:40:31,270 DEBUG TRAIN Batch 11/100 loss 8.335286 loss_att 8.065573 loss_ctc 8.964617 lr 0.00165384 rank 0\n",
            "2022-05-23 15:41:00,898 DEBUG TRAIN Batch 11/200 loss 11.755026 loss_att 10.946777 loss_ctc 13.640941 lr 0.00165584 rank 0\n",
            "2022-05-23 15:41:29,846 DEBUG TRAIN Batch 11/300 loss 14.678442 loss_att 13.735884 loss_ctc 16.877745 lr 0.00165784 rank 0\n",
            "2022-05-23 15:41:59,876 DEBUG TRAIN Batch 11/400 loss 11.528923 loss_att 10.753366 loss_ctc 13.338559 lr 0.00165984 rank 0\n",
            "2022-05-23 15:42:30,878 DEBUG TRAIN Batch 11/500 loss 10.330412 loss_att 9.683180 loss_ctc 11.840618 lr 0.00166184 rank 0\n",
            "2022-05-23 15:43:00,352 DEBUG TRAIN Batch 11/600 loss 10.745495 loss_att 9.642847 loss_ctc 13.318340 lr 0.00166384 rank 0\n",
            "2022-05-23 15:43:30,273 DEBUG TRAIN Batch 11/700 loss 10.864624 loss_att 10.363548 loss_ctc 12.033801 lr 0.00166584 rank 0\n",
            "2022-05-23 15:44:00,150 DEBUG TRAIN Batch 11/800 loss 15.822771 loss_att 14.856518 loss_ctc 18.077364 lr 0.00166784 rank 0\n",
            "2022-05-23 15:44:30,052 DEBUG TRAIN Batch 11/900 loss 11.814759 loss_att 11.732553 loss_ctc 12.006571 lr 0.00166984 rank 0\n",
            "2022-05-23 15:45:00,528 DEBUG TRAIN Batch 11/1000 loss 9.345802 loss_att 7.641599 loss_ctc 13.322278 lr 0.00167184 rank 0\n",
            "2022-05-23 15:45:29,939 DEBUG TRAIN Batch 11/1100 loss 9.961740 loss_att 8.923058 loss_ctc 12.385334 lr 0.00167384 rank 0\n",
            "2022-05-23 15:45:59,608 DEBUG TRAIN Batch 11/1200 loss 11.633734 loss_att 10.429955 loss_ctc 14.442554 lr 0.00167584 rank 0\n",
            "2022-05-23 15:46:29,627 DEBUG TRAIN Batch 11/1300 loss 11.376145 loss_att 11.184008 loss_ctc 11.824465 lr 0.00167784 rank 0\n",
            "2022-05-23 15:46:59,381 DEBUG TRAIN Batch 11/1400 loss 7.159569 loss_att 6.970573 loss_ctc 7.600559 lr 0.00167984 rank 0\n",
            "2022-05-23 15:47:30,303 DEBUG TRAIN Batch 11/1500 loss 11.871133 loss_att 10.536608 loss_ctc 14.985025 lr 0.00168184 rank 0\n",
            "2022-05-23 15:47:59,650 DEBUG TRAIN Batch 11/1600 loss 10.394412 loss_att 9.175979 loss_ctc 13.237423 lr 0.00168384 rank 0\n",
            "2022-05-23 15:48:29,220 DEBUG TRAIN Batch 11/1700 loss 10.892401 loss_att 10.326433 loss_ctc 12.212993 lr 0.00168584 rank 0\n",
            "2022-05-23 15:48:59,272 DEBUG TRAIN Batch 11/1800 loss 12.498380 loss_att 11.487671 loss_ctc 14.856701 lr 0.00168784 rank 0\n",
            "2022-05-23 15:49:29,359 DEBUG TRAIN Batch 11/1900 loss 8.877926 loss_att 8.392799 loss_ctc 10.009886 lr 0.00168984 rank 0\n",
            "2022-05-23 15:50:00,215 DEBUG TRAIN Batch 11/2000 loss 12.717554 loss_att 10.980552 loss_ctc 16.770557 lr 0.00169184 rank 0\n",
            "2022-05-23 15:50:30,260 DEBUG TRAIN Batch 11/2100 loss 10.121675 loss_att 9.057507 loss_ctc 12.604736 lr 0.00169384 rank 0\n",
            "2022-05-23 15:50:59,744 DEBUG TRAIN Batch 11/2200 loss 10.771967 loss_att 9.829366 loss_ctc 12.971371 lr 0.00169584 rank 0\n",
            "2022-05-23 15:51:29,538 DEBUG TRAIN Batch 11/2300 loss 10.775866 loss_att 10.274787 loss_ctc 11.945049 lr 0.00169784 rank 0\n",
            "2022-05-23 15:51:59,669 DEBUG TRAIN Batch 11/2400 loss 12.687438 loss_att 12.886614 loss_ctc 12.222693 lr 0.00169984 rank 0\n",
            "2022-05-23 15:52:30,406 DEBUG TRAIN Batch 11/2500 loss 9.347254 loss_att 8.482341 loss_ctc 11.365385 lr 0.00170184 rank 0\n",
            "2022-05-23 15:52:59,299 DEBUG TRAIN Batch 11/2600 loss 11.047520 loss_att 10.302922 loss_ctc 12.784914 lr 0.00170384 rank 0\n",
            "2022-05-23 15:53:28,873 DEBUG TRAIN Batch 11/2700 loss 7.389976 loss_att 6.777474 loss_ctc 8.819143 lr 0.00170584 rank 0\n",
            "2022-05-23 15:53:58,565 DEBUG TRAIN Batch 11/2800 loss 11.610762 loss_att 10.110262 loss_ctc 15.111929 lr 0.00170784 rank 0\n",
            "2022-05-23 15:54:28,815 DEBUG TRAIN Batch 11/2900 loss 12.991239 loss_att 12.315605 loss_ctc 14.567719 lr 0.00170984 rank 0\n",
            "2022-05-23 15:54:59,097 DEBUG TRAIN Batch 11/3000 loss 10.608773 loss_att 9.513493 loss_ctc 13.164429 lr 0.00171184 rank 0\n",
            "2022-05-23 15:55:28,550 DEBUG TRAIN Batch 11/3100 loss 11.153461 loss_att 10.012680 loss_ctc 13.815282 lr 0.00171384 rank 0\n",
            "2022-05-23 15:55:58,088 DEBUG TRAIN Batch 11/3200 loss 10.307007 loss_att 9.320858 loss_ctc 12.608019 lr 0.00171584 rank 0\n",
            "2022-05-23 15:56:27,777 DEBUG TRAIN Batch 11/3300 loss 10.251375 loss_att 9.587454 loss_ctc 11.800524 lr 0.00171784 rank 0\n",
            "2022-05-23 15:56:57,458 DEBUG TRAIN Batch 11/3400 loss 12.230070 loss_att 11.784006 loss_ctc 13.270885 lr 0.00171984 rank 0\n",
            "2022-05-23 15:57:28,052 DEBUG TRAIN Batch 11/3500 loss 10.323648 loss_att 9.284498 loss_ctc 12.748331 lr 0.00172184 rank 0\n",
            "2022-05-23 15:57:57,256 DEBUG TRAIN Batch 11/3600 loss 8.953957 loss_att 8.333040 loss_ctc 10.402761 lr 0.00172384 rank 0\n",
            "2022-05-23 15:58:27,278 DEBUG TRAIN Batch 11/3700 loss 9.452005 loss_att 8.620096 loss_ctc 11.393127 lr 0.00172584 rank 0\n",
            "2022-05-23 15:58:57,159 DEBUG TRAIN Batch 11/3800 loss 7.405259 loss_att 7.127172 loss_ctc 8.054132 lr 0.00172784 rank 0\n",
            "2022-05-23 15:59:27,385 DEBUG TRAIN Batch 11/3900 loss 15.470081 loss_att 14.794528 loss_ctc 17.046371 lr 0.00172984 rank 0\n",
            "2022-05-23 15:59:58,651 DEBUG TRAIN Batch 11/4000 loss 10.356611 loss_att 8.976054 loss_ctc 13.577912 lr 0.00173184 rank 0\n",
            "2022-05-23 16:00:27,948 DEBUG TRAIN Batch 11/4100 loss 7.280510 loss_att 6.508451 loss_ctc 9.081981 lr 0.00173384 rank 0\n",
            "2022-05-23 16:00:57,464 DEBUG TRAIN Batch 11/4200 loss 8.557644 loss_att 8.045268 loss_ctc 9.753187 lr 0.00173584 rank 0\n",
            "2022-05-23 16:01:27,257 DEBUG TRAIN Batch 11/4300 loss 11.481405 loss_att 10.652601 loss_ctc 13.415281 lr 0.00173784 rank 0\n",
            "2022-05-23 16:01:57,318 DEBUG TRAIN Batch 11/4400 loss 18.488937 loss_att 17.571798 loss_ctc 20.628927 lr 0.00173984 rank 0\n",
            "2022-05-23 16:02:28,270 DEBUG TRAIN Batch 11/4500 loss 11.348381 loss_att 10.200930 loss_ctc 14.025767 lr 0.00174184 rank 0\n",
            "2022-05-23 16:02:57,603 DEBUG TRAIN Batch 11/4600 loss 5.802752 loss_att 5.246118 loss_ctc 7.101563 lr 0.00174384 rank 0\n",
            "2022-05-23 16:03:27,497 DEBUG TRAIN Batch 11/4700 loss 8.957740 loss_att 8.542289 loss_ctc 9.927126 lr 0.00174584 rank 0\n",
            "2022-05-23 16:03:56,898 DEBUG TRAIN Batch 11/4800 loss 9.518583 loss_att 8.723230 loss_ctc 11.374406 lr 0.00174784 rank 0\n",
            "2022-05-23 16:04:26,756 DEBUG TRAIN Batch 11/4900 loss 13.527987 loss_att 13.034338 loss_ctc 14.679832 lr 0.00174984 rank 0\n",
            "2022-05-23 16:04:57,327 DEBUG TRAIN Batch 11/5000 loss 8.649454 loss_att 7.538833 loss_ctc 11.240905 lr 0.00175184 rank 0\n",
            "2022-05-23 16:05:26,697 DEBUG TRAIN Batch 11/5100 loss 8.181345 loss_att 7.488856 loss_ctc 9.797153 lr 0.00175384 rank 0\n",
            "2022-05-23 16:05:56,365 DEBUG TRAIN Batch 11/5200 loss 10.811701 loss_att 9.979103 loss_ctc 12.754430 lr 0.00175584 rank 0\n",
            "2022-05-23 16:06:26,031 DEBUG TRAIN Batch 11/5300 loss 10.090946 loss_att 9.331440 loss_ctc 11.863127 lr 0.00175784 rank 0\n",
            "2022-05-23 16:06:55,722 DEBUG TRAIN Batch 11/5400 loss 13.254925 loss_att 12.410656 loss_ctc 15.224884 lr 0.00175984 rank 0\n",
            "2022-05-23 16:07:26,471 DEBUG TRAIN Batch 11/5500 loss 9.690100 loss_att 8.856624 loss_ctc 11.634876 lr 0.00176184 rank 0\n",
            "2022-05-23 16:07:56,139 DEBUG TRAIN Batch 11/5600 loss 9.442111 loss_att 8.613436 loss_ctc 11.375686 lr 0.00176384 rank 0\n",
            "2022-05-23 16:08:26,255 DEBUG TRAIN Batch 11/5700 loss 10.598729 loss_att 10.319220 loss_ctc 11.250917 lr 0.00176584 rank 0\n",
            "2022-05-23 16:08:55,906 DEBUG TRAIN Batch 11/5800 loss 12.055365 loss_att 11.380661 loss_ctc 13.629674 lr 0.00176784 rank 0\n",
            "2022-05-23 16:09:26,246 DEBUG TRAIN Batch 11/5900 loss 12.220198 loss_att 11.890745 loss_ctc 12.988920 lr 0.00176984 rank 0\n",
            "2022-05-23 16:09:56,984 DEBUG TRAIN Batch 11/6000 loss 6.470329 loss_att 5.886786 loss_ctc 7.831928 lr 0.00177184 rank 0\n",
            "2022-05-23 16:10:26,263 DEBUG TRAIN Batch 11/6100 loss 11.025265 loss_att 9.262947 loss_ctc 15.137339 lr 0.00177384 rank 0\n",
            "2022-05-23 16:10:56,045 DEBUG TRAIN Batch 11/6200 loss 11.198863 loss_att 10.329231 loss_ctc 13.228004 lr 0.00177584 rank 0\n",
            "2022-05-23 16:11:25,734 DEBUG TRAIN Batch 11/6300 loss 10.930388 loss_att 10.454521 loss_ctc 12.040744 lr 0.00177784 rank 0\n",
            "2022-05-23 16:11:55,757 DEBUG TRAIN Batch 11/6400 loss 9.764987 loss_att 9.142118 loss_ctc 11.218349 lr 0.00177984 rank 0\n",
            "2022-05-23 16:12:26,078 DEBUG TRAIN Batch 11/6500 loss 9.404009 loss_att 8.636078 loss_ctc 11.195847 lr 0.00178184 rank 0\n",
            "2022-05-23 16:12:55,793 DEBUG TRAIN Batch 11/6600 loss 9.658195 loss_att 9.353555 loss_ctc 10.369021 lr 0.00178384 rank 0\n",
            "2022-05-23 16:13:24,894 DEBUG TRAIN Batch 11/6700 loss 13.478682 loss_att 12.175292 loss_ctc 16.519926 lr 0.00178584 rank 0\n",
            "2022-05-23 16:13:55,196 DEBUG TRAIN Batch 11/6800 loss 13.861103 loss_att 12.498657 loss_ctc 17.040142 lr 0.00178784 rank 0\n",
            "2022-05-23 16:14:25,286 DEBUG TRAIN Batch 11/6900 loss 10.062704 loss_att 9.420527 loss_ctc 11.561117 lr 0.00178984 rank 0\n",
            "2022-05-23 16:14:56,550 DEBUG TRAIN Batch 11/7000 loss 8.585587 loss_att 7.607016 loss_ctc 10.868919 lr 0.00179184 rank 0\n",
            "2022-05-23 16:15:25,783 DEBUG TRAIN Batch 11/7100 loss 9.609583 loss_att 8.365140 loss_ctc 12.513281 lr 0.00179384 rank 0\n",
            "2022-05-23 16:15:55,261 DEBUG TRAIN Batch 11/7200 loss 9.779367 loss_att 8.669471 loss_ctc 12.369128 lr 0.00179584 rank 0\n",
            "2022-05-23 16:16:25,648 DEBUG TRAIN Batch 11/7300 loss 9.553259 loss_att 9.058412 loss_ctc 10.707901 lr 0.00179784 rank 0\n",
            "2022-05-23 16:16:55,821 DEBUG TRAIN Batch 11/7400 loss 17.697910 loss_att 17.450760 loss_ctc 18.274593 lr 0.00179984 rank 0\n",
            "2022-05-23 16:17:26,040 DEBUG TRAIN Batch 11/7500 loss 10.008945 loss_att 9.060173 loss_ctc 12.222748 lr 0.00180184 rank 0\n",
            "2022-05-23 16:17:32,031 DEBUG CV Batch 11/0 loss 6.336471 loss_att 5.901848 loss_ctc 7.350593 history loss 5.963737 rank 0\n",
            "2022-05-23 16:17:43,338 DEBUG CV Batch 11/100 loss 4.573057 loss_att 4.594259 loss_ctc 4.523585 history loss 7.885059 rank 0\n",
            "2022-05-23 16:17:53,650 DEBUG CV Batch 11/200 loss 6.669759 loss_att 6.157317 loss_ctc 7.865455 history loss 7.826000 rank 0\n",
            "2022-05-23 16:18:04,573 DEBUG CV Batch 11/300 loss 5.696389 loss_att 5.559440 loss_ctc 6.015937 history loss 7.365735 rank 0\n",
            "2022-05-23 16:18:16,132 DEBUG CV Batch 11/400 loss 10.816681 loss_att 10.567629 loss_ctc 11.397801 history loss 6.823342 rank 0\n",
            "2022-05-23 16:18:28,296 DEBUG CV Batch 11/500 loss 3.614153 loss_att 3.590186 loss_ctc 3.670076 history loss 6.724551 rank 0\n",
            "2022-05-23 16:18:39,825 DEBUG CV Batch 11/600 loss 5.050325 loss_att 4.945659 loss_ctc 5.294546 history loss 6.676154 rank 0\n",
            "2022-05-23 16:18:50,698 DEBUG CV Batch 11/700 loss 5.141976 loss_att 4.875307 loss_ctc 5.764203 history loss 6.545843 rank 0\n",
            "2022-05-23 16:19:02,150 DEBUG CV Batch 11/800 loss 6.497461 loss_att 6.198658 loss_ctc 7.194667 history loss 6.504486 rank 0\n",
            "2022-05-23 16:19:13,703 INFO Epoch 11 CV info cv_loss 6.49849047607243\n",
            "2022-05-23 16:19:13,703 INFO Checkpoint: save to checkpoint exp/conformer/11.pt\n",
            "2022-05-23 16:19:14,034 INFO Epoch 12 TRAIN info lr 0.0018019199999999998\n",
            "2022-05-23 16:19:14,036 INFO using accumulate grad, new batch size is 4 times larger than before\n",
            "2022-05-23 16:19:39,469 DEBUG TRAIN Batch 12/0 loss 12.319983 loss_att 10.333093 loss_ctc 16.956062 lr 0.00180200 rank 0\n",
            "2022-05-23 16:20:09,364 DEBUG TRAIN Batch 12/100 loss 7.458000 loss_att 6.507580 loss_ctc 9.675646 lr 0.00180400 rank 0\n",
            "2022-05-23 16:20:39,207 DEBUG TRAIN Batch 12/200 loss 7.874360 loss_att 7.317484 loss_ctc 9.173737 lr 0.00180600 rank 0\n",
            "2022-05-23 16:21:08,529 DEBUG TRAIN Batch 12/300 loss 10.999676 loss_att 10.260273 loss_ctc 12.724951 lr 0.00180800 rank 0\n",
            "2022-05-23 16:21:38,875 DEBUG TRAIN Batch 12/400 loss 10.743036 loss_att 10.617138 loss_ctc 11.036800 lr 0.00181000 rank 0\n",
            "2022-05-23 16:22:10,072 DEBUG TRAIN Batch 12/500 loss 9.795801 loss_att 8.812729 loss_ctc 12.089636 lr 0.00181200 rank 0\n",
            "2022-05-23 16:22:38,993 DEBUG TRAIN Batch 12/600 loss 8.496512 loss_att 7.327187 loss_ctc 11.224937 lr 0.00181400 rank 0\n",
            "2022-05-23 16:23:08,571 DEBUG TRAIN Batch 12/700 loss 10.764749 loss_att 10.362026 loss_ctc 11.704434 lr 0.00181600 rank 0\n",
            "2022-05-23 16:23:38,546 DEBUG TRAIN Batch 12/800 loss 9.330299 loss_att 8.865213 loss_ctc 10.415499 lr 0.00181800 rank 0\n",
            "2022-05-23 16:24:08,271 DEBUG TRAIN Batch 12/900 loss 16.427444 loss_att 15.864900 loss_ctc 17.740051 lr 0.00182000 rank 0\n",
            "2022-05-23 16:24:39,017 DEBUG TRAIN Batch 12/1000 loss 11.227398 loss_att 10.258909 loss_ctc 13.487204 lr 0.00182200 rank 0\n",
            "2022-05-23 16:25:08,456 DEBUG TRAIN Batch 12/1100 loss 9.529139 loss_att 8.686247 loss_ctc 11.495884 lr 0.00182400 rank 0\n",
            "2022-05-23 16:25:37,888 DEBUG TRAIN Batch 12/1200 loss 8.913472 loss_att 7.750141 loss_ctc 11.627912 lr 0.00182600 rank 0\n",
            "2022-05-23 16:26:07,919 DEBUG TRAIN Batch 12/1300 loss 11.268957 loss_att 10.717947 loss_ctc 12.554646 lr 0.00182800 rank 0\n",
            "2022-05-23 16:26:37,777 DEBUG TRAIN Batch 12/1400 loss 10.699646 loss_att 10.148151 loss_ctc 11.986465 lr 0.00183000 rank 0\n",
            "2022-05-23 16:27:08,696 DEBUG TRAIN Batch 12/1500 loss 10.321423 loss_att 9.184683 loss_ctc 12.973815 lr 0.00183200 rank 0\n",
            "2022-05-23 16:27:38,457 DEBUG TRAIN Batch 12/1600 loss 9.890422 loss_att 9.286549 loss_ctc 11.299459 lr 0.00183400 rank 0\n",
            "2022-05-23 16:28:08,269 DEBUG TRAIN Batch 12/1700 loss 12.464294 loss_att 10.911318 loss_ctc 16.087908 lr 0.00183600 rank 0\n",
            "2022-05-23 16:28:38,071 DEBUG TRAIN Batch 12/1800 loss 14.151087 loss_att 13.931048 loss_ctc 14.664511 lr 0.00183800 rank 0\n",
            "2022-05-23 16:29:08,166 DEBUG TRAIN Batch 12/1900 loss 11.874180 loss_att 11.207886 loss_ctc 13.428864 lr 0.00184000 rank 0\n",
            "2022-05-23 16:29:38,511 DEBUG TRAIN Batch 12/2000 loss 8.457073 loss_att 7.094927 loss_ctc 11.635412 lr 0.00184200 rank 0\n",
            "2022-05-23 16:30:08,072 DEBUG TRAIN Batch 12/2100 loss 7.106556 loss_att 6.733894 loss_ctc 7.976100 lr 0.00184400 rank 0\n",
            "2022-05-23 16:30:37,919 DEBUG TRAIN Batch 12/2200 loss 7.468104 loss_att 7.008217 loss_ctc 8.541175 lr 0.00184600 rank 0\n",
            "2022-05-23 16:31:07,939 DEBUG TRAIN Batch 12/2300 loss 10.176614 loss_att 9.252550 loss_ctc 12.332762 lr 0.00184800 rank 0\n",
            "2022-05-23 16:31:38,113 DEBUG TRAIN Batch 12/2400 loss 13.085729 loss_att 12.343000 loss_ctc 14.818760 lr 0.00185000 rank 0\n",
            "2022-05-23 16:32:09,048 DEBUG TRAIN Batch 12/2500 loss 12.135581 loss_att 10.837557 loss_ctc 15.164303 lr 0.00185200 rank 0\n",
            "2022-05-23 16:32:38,595 DEBUG TRAIN Batch 12/2600 loss 6.827228 loss_att 6.247110 loss_ctc 8.180836 lr 0.00185400 rank 0\n",
            "2022-05-23 16:33:08,396 DEBUG TRAIN Batch 12/2700 loss 11.072140 loss_att 9.697317 loss_ctc 14.280062 lr 0.00185600 rank 0\n",
            "2022-05-23 16:33:38,386 DEBUG TRAIN Batch 12/2800 loss 10.504505 loss_att 9.500237 loss_ctc 12.847799 lr 0.00185800 rank 0\n",
            "2022-05-23 16:34:08,909 DEBUG TRAIN Batch 12/2900 loss 15.144239 loss_att 14.073467 loss_ctc 17.642706 lr 0.00186000 rank 0\n",
            "2022-05-23 16:34:39,337 DEBUG TRAIN Batch 12/3000 loss 6.101176 loss_att 5.612919 loss_ctc 7.240444 lr 0.00186200 rank 0\n",
            "2022-05-23 16:35:08,649 DEBUG TRAIN Batch 12/3100 loss 6.788011 loss_att 6.090133 loss_ctc 8.416391 lr 0.00186400 rank 0\n",
            "2022-05-23 16:35:37,980 DEBUG TRAIN Batch 12/3200 loss 8.165864 loss_att 7.613814 loss_ctc 9.453979 lr 0.00186600 rank 0\n",
            "2022-05-23 16:36:07,809 DEBUG TRAIN Batch 12/3300 loss 12.500395 loss_att 11.207795 loss_ctc 15.516462 lr 0.00186800 rank 0\n",
            "2022-05-23 16:36:38,209 DEBUG TRAIN Batch 12/3400 loss 9.671900 loss_att 9.241927 loss_ctc 10.675169 lr 0.00187000 rank 0\n",
            "2022-05-23 16:37:08,663 DEBUG TRAIN Batch 12/3500 loss 9.125162 loss_att 8.194389 loss_ctc 11.296966 lr 0.00187200 rank 0\n",
            "2022-05-23 16:37:38,423 DEBUG TRAIN Batch 12/3600 loss 6.822626 loss_att 6.124656 loss_ctc 8.451223 lr 0.00187400 rank 0\n",
            "2022-05-23 16:38:08,191 DEBUG TRAIN Batch 12/3700 loss 9.330631 loss_att 8.647484 loss_ctc 10.924641 lr 0.00187600 rank 0\n",
            "2022-05-23 16:38:38,188 DEBUG TRAIN Batch 12/3800 loss 10.428191 loss_att 9.303587 loss_ctc 13.052267 lr 0.00187800 rank 0\n",
            "2022-05-23 16:39:08,691 DEBUG TRAIN Batch 12/3900 loss 10.891578 loss_att 10.265267 loss_ctc 12.352967 lr 0.00188000 rank 0\n",
            "2022-05-23 16:39:39,772 DEBUG TRAIN Batch 12/4000 loss 10.622448 loss_att 9.653275 loss_ctc 12.883850 lr 0.00188200 rank 0\n",
            "2022-05-23 16:40:09,414 DEBUG TRAIN Batch 12/4100 loss 9.922401 loss_att 8.953056 loss_ctc 12.184208 lr 0.00188400 rank 0\n",
            "2022-05-23 16:40:38,645 DEBUG TRAIN Batch 12/4200 loss 12.022167 loss_att 10.955084 loss_ctc 14.512029 lr 0.00188600 rank 0\n",
            "2022-05-23 16:41:08,669 DEBUG TRAIN Batch 12/4300 loss 10.609801 loss_att 10.133682 loss_ctc 11.720745 lr 0.00188800 rank 0\n",
            "2022-05-23 16:41:38,539 DEBUG TRAIN Batch 12/4400 loss 9.334643 loss_att 8.832959 loss_ctc 10.505241 lr 0.00189000 rank 0\n",
            "2022-05-23 16:42:09,621 DEBUG TRAIN Batch 12/4500 loss 10.145964 loss_att 8.772622 loss_ctc 13.350430 lr 0.00189200 rank 0\n",
            "2022-05-23 16:42:39,102 DEBUG TRAIN Batch 12/4600 loss 11.702213 loss_att 10.974543 loss_ctc 13.400112 lr 0.00189400 rank 0\n",
            "2022-05-23 16:43:08,778 DEBUG TRAIN Batch 12/4700 loss 9.135454 loss_att 8.891033 loss_ctc 9.705772 lr 0.00189600 rank 0\n",
            "2022-05-23 16:43:38,935 DEBUG TRAIN Batch 12/4800 loss 10.803652 loss_att 9.716009 loss_ctc 13.341485 lr 0.00189800 rank 0\n",
            "2022-05-23 16:44:08,842 DEBUG TRAIN Batch 12/4900 loss 11.831954 loss_att 11.302337 loss_ctc 13.067729 lr 0.00190000 rank 0\n",
            "2022-05-23 16:44:39,882 DEBUG TRAIN Batch 12/5000 loss 7.388945 loss_att 6.577822 loss_ctc 9.281565 lr 0.00190200 rank 0\n",
            "2022-05-23 16:45:09,526 DEBUG TRAIN Batch 12/5100 loss 11.112295 loss_att 10.246452 loss_ctc 13.132595 lr 0.00190400 rank 0\n",
            "2022-05-23 16:45:39,452 DEBUG TRAIN Batch 12/5200 loss 8.901379 loss_att 8.104019 loss_ctc 10.761883 lr 0.00190600 rank 0\n",
            "2022-05-23 16:46:09,350 DEBUG TRAIN Batch 12/5300 loss 11.349251 loss_att 10.379320 loss_ctc 13.612420 lr 0.00190800 rank 0\n",
            "2022-05-23 16:46:39,387 DEBUG TRAIN Batch 12/5400 loss 13.467563 loss_att 13.166904 loss_ctc 14.169098 lr 0.00191000 rank 0\n",
            "2022-05-23 16:47:10,077 DEBUG TRAIN Batch 12/5500 loss 13.507133 loss_att 11.725247 loss_ctc 17.664867 lr 0.00191200 rank 0\n",
            "2022-05-23 16:47:39,581 DEBUG TRAIN Batch 12/5600 loss 8.582992 loss_att 7.447887 loss_ctc 11.231569 lr 0.00191400 rank 0\n",
            "2022-05-23 16:48:09,273 DEBUG TRAIN Batch 12/5700 loss 10.592721 loss_att 9.661482 loss_ctc 12.765612 lr 0.00191600 rank 0\n",
            "2022-05-23 16:48:38,860 DEBUG TRAIN Batch 12/5800 loss 11.516418 loss_att 10.024475 loss_ctc 14.997618 lr 0.00191800 rank 0\n",
            "2022-05-23 16:49:08,855 DEBUG TRAIN Batch 12/5900 loss 8.793410 loss_att 8.038363 loss_ctc 10.555189 lr 0.00192000 rank 0\n",
            "2022-05-23 16:49:39,903 DEBUG TRAIN Batch 12/6000 loss 11.368652 loss_att 9.902536 loss_ctc 14.789589 lr 0.00192200 rank 0\n",
            "2022-05-23 16:50:09,091 DEBUG TRAIN Batch 12/6100 loss 8.045509 loss_att 7.524475 loss_ctc 9.261256 lr 0.00192400 rank 0\n",
            "2022-05-23 16:50:38,727 DEBUG TRAIN Batch 12/6200 loss 10.208507 loss_att 8.939446 loss_ctc 13.169647 lr 0.00192600 rank 0\n",
            "2022-05-23 16:51:08,668 DEBUG TRAIN Batch 12/6300 loss 10.818205 loss_att 9.643984 loss_ctc 13.558055 lr 0.00192800 rank 0\n",
            "2022-05-23 16:51:38,573 DEBUG TRAIN Batch 12/6400 loss 11.941154 loss_att 11.125640 loss_ctc 13.844024 lr 0.00193000 rank 0\n",
            "2022-05-23 16:52:09,368 DEBUG TRAIN Batch 12/6500 loss 11.211569 loss_att 9.925474 loss_ctc 14.212455 lr 0.00193200 rank 0\n",
            "2022-05-23 16:52:38,694 DEBUG TRAIN Batch 12/6600 loss 8.371634 loss_att 7.919938 loss_ctc 9.425590 lr 0.00193400 rank 0\n",
            "2022-05-23 16:53:08,572 DEBUG TRAIN Batch 12/6700 loss 6.126974 loss_att 5.923009 loss_ctc 6.602893 lr 0.00193600 rank 0\n",
            "2022-05-23 16:53:38,509 DEBUG TRAIN Batch 12/6800 loss 10.521562 loss_att 9.602594 loss_ctc 12.665818 lr 0.00193800 rank 0\n",
            "2022-05-23 16:54:08,499 DEBUG TRAIN Batch 12/6900 loss 13.230947 loss_att 12.076154 loss_ctc 15.925465 lr 0.00194000 rank 0\n",
            "2022-05-23 16:54:39,490 DEBUG TRAIN Batch 12/7000 loss 9.280847 loss_att 8.466944 loss_ctc 11.179955 lr 0.00194200 rank 0\n",
            "2022-05-23 16:55:08,969 DEBUG TRAIN Batch 12/7100 loss 6.964556 loss_att 6.749700 loss_ctc 7.465886 lr 0.00194400 rank 0\n",
            "2022-05-23 16:55:38,584 DEBUG TRAIN Batch 12/7200 loss 9.529684 loss_att 8.850569 loss_ctc 11.114287 lr 0.00194600 rank 0\n",
            "2022-05-23 16:56:08,374 DEBUG TRAIN Batch 12/7300 loss 12.204395 loss_att 11.653399 loss_ctc 13.490052 lr 0.00194800 rank 0\n",
            "2022-05-23 16:56:38,816 DEBUG TRAIN Batch 12/7400 loss 13.292770 loss_att 13.019319 loss_ctc 13.930824 lr 0.00195000 rank 0\n",
            "2022-05-23 16:57:09,247 DEBUG TRAIN Batch 12/7500 loss 11.207266 loss_att 10.138542 loss_ctc 13.700955 lr 0.00195200 rank 0\n",
            "2022-05-23 16:57:15,580 DEBUG CV Batch 12/0 loss 5.574419 loss_att 5.318144 loss_ctc 6.172396 history loss 5.246512 rank 0\n",
            "2022-05-23 16:57:26,815 DEBUG CV Batch 12/100 loss 4.461655 loss_att 4.344886 loss_ctc 4.734115 history loss 7.696660 rank 0\n",
            "2022-05-23 16:57:37,104 DEBUG CV Batch 12/200 loss 7.830722 loss_att 7.360437 loss_ctc 8.928053 history loss 7.719933 rank 0\n",
            "2022-05-23 16:57:48,098 DEBUG CV Batch 12/300 loss 5.331631 loss_att 5.516338 loss_ctc 4.900645 history loss 7.215749 rank 0\n",
            "2022-05-23 16:57:59,895 DEBUG CV Batch 12/400 loss 11.979238 loss_att 12.144744 loss_ctc 11.593056 history loss 6.716707 rank 0\n",
            "2022-05-23 16:58:11,747 DEBUG CV Batch 12/500 loss 3.883168 loss_att 3.578506 loss_ctc 4.594047 history loss 6.685230 rank 0\n",
            "2022-05-23 16:58:23,346 DEBUG CV Batch 12/600 loss 5.271337 loss_att 5.037107 loss_ctc 5.817872 history loss 6.609117 rank 0\n",
            "2022-05-23 16:58:34,318 DEBUG CV Batch 12/700 loss 5.311098 loss_att 5.071500 loss_ctc 5.870159 history loss 6.512153 rank 0\n",
            "2022-05-23 16:58:45,705 DEBUG CV Batch 12/800 loss 6.396164 loss_att 6.185822 loss_ctc 6.886962 history loss 6.490479 rank 0\n",
            "2022-05-23 16:58:57,276 INFO Epoch 12 CV info cv_loss 6.512175950932804\n",
            "2022-05-23 16:58:57,277 INFO Checkpoint: save to checkpoint exp/conformer/12.pt\n",
            "2022-05-23 16:58:57,609 INFO Epoch 13 TRAIN info lr 0.0019520800000000001\n",
            "2022-05-23 16:58:57,611 INFO using accumulate grad, new batch size is 4 times larger than before\n",
            "2022-05-23 16:59:22,815 DEBUG TRAIN Batch 13/0 loss 9.215305 loss_att 8.523205 loss_ctc 10.830208 lr 0.00195216 rank 0\n",
            "2022-05-23 16:59:53,239 DEBUG TRAIN Batch 13/100 loss 8.232492 loss_att 7.312387 loss_ctc 10.379405 lr 0.00195416 rank 0\n",
            "2022-05-23 17:00:22,791 DEBUG TRAIN Batch 13/200 loss 11.854435 loss_att 10.657525 loss_ctc 14.647225 lr 0.00195616 rank 0\n",
            "2022-05-23 17:00:52,374 DEBUG TRAIN Batch 13/300 loss 10.300390 loss_att 9.467929 loss_ctc 12.242800 lr 0.00195816 rank 0\n",
            "2022-05-23 17:01:22,663 DEBUG TRAIN Batch 13/400 loss 12.063847 loss_att 11.022713 loss_ctc 14.493158 lr 0.00196016 rank 0\n",
            "2022-05-23 17:01:53,720 DEBUG TRAIN Batch 13/500 loss 7.962407 loss_att 7.276323 loss_ctc 9.563269 lr 0.00196216 rank 0\n",
            "2022-05-23 17:02:23,084 DEBUG TRAIN Batch 13/600 loss 6.000117 loss_att 5.437074 loss_ctc 7.313886 lr 0.00196416 rank 0\n",
            "2022-05-23 17:02:52,380 DEBUG TRAIN Batch 13/700 loss 8.130479 loss_att 7.652720 loss_ctc 9.245249 lr 0.00196616 rank 0\n",
            "2022-05-23 17:03:22,178 DEBUG TRAIN Batch 13/800 loss 12.866286 loss_att 11.847052 loss_ctc 15.244499 lr 0.00196816 rank 0\n",
            "2022-05-23 17:03:52,582 DEBUG TRAIN Batch 13/900 loss 10.920300 loss_att 10.355348 loss_ctc 12.238523 lr 0.00197016 rank 0\n",
            "2022-05-23 17:04:23,581 DEBUG TRAIN Batch 13/1000 loss 6.144463 loss_att 5.604119 loss_ctc 7.405265 lr 0.00197216 rank 0\n",
            "2022-05-23 17:04:53,025 DEBUG TRAIN Batch 13/1100 loss 8.708451 loss_att 7.547000 loss_ctc 11.418503 lr 0.00197416 rank 0\n",
            "2022-05-23 17:05:22,708 DEBUG TRAIN Batch 13/1200 loss 5.609711 loss_att 5.073566 loss_ctc 6.860714 lr 0.00197616 rank 0\n",
            "2022-05-23 17:05:52,328 DEBUG TRAIN Batch 13/1300 loss 7.708596 loss_att 7.106455 loss_ctc 9.113591 lr 0.00197816 rank 0\n",
            "2022-05-23 17:06:22,813 DEBUG TRAIN Batch 13/1400 loss 14.998448 loss_att 14.390905 loss_ctc 16.416050 lr 0.00198016 rank 0\n",
            "2022-05-23 17:06:53,627 DEBUG TRAIN Batch 13/1500 loss 8.358004 loss_att 7.334626 loss_ctc 10.745884 lr 0.00198216 rank 0\n",
            "2022-05-23 17:07:23,028 DEBUG TRAIN Batch 13/1600 loss 10.353691 loss_att 9.550165 loss_ctc 12.228586 lr 0.00198416 rank 0\n",
            "2022-05-23 17:07:52,574 DEBUG TRAIN Batch 13/1700 loss 8.354486 loss_att 7.317834 loss_ctc 10.773339 lr 0.00198616 rank 0\n",
            "2022-05-23 17:08:22,623 DEBUG TRAIN Batch 13/1800 loss 10.574539 loss_att 9.889203 loss_ctc 12.173658 lr 0.00198816 rank 0\n",
            "2022-05-23 17:08:52,986 DEBUG TRAIN Batch 13/1900 loss 15.360888 loss_att 13.743732 loss_ctc 19.134249 lr 0.00199016 rank 0\n",
            "2022-05-23 17:09:23,724 DEBUG TRAIN Batch 13/2000 loss 6.718250 loss_att 5.968130 loss_ctc 8.468529 lr 0.00199216 rank 0\n",
            "2022-05-23 17:09:52,844 DEBUG TRAIN Batch 13/2100 loss 8.815005 loss_att 7.464037 loss_ctc 11.967265 lr 0.00199416 rank 0\n",
            "2022-05-23 17:10:22,923 DEBUG TRAIN Batch 13/2200 loss 8.059086 loss_att 7.368653 loss_ctc 9.670095 lr 0.00199616 rank 0\n",
            "2022-05-23 17:10:52,393 DEBUG TRAIN Batch 13/2300 loss 10.117524 loss_att 10.201939 loss_ctc 9.920557 lr 0.00199816 rank 0\n",
            "2022-05-23 17:11:22,114 DEBUG TRAIN Batch 13/2400 loss 13.868893 loss_att 13.380735 loss_ctc 15.007929 lr 0.00199992 rank 0\n",
            "2022-05-23 17:11:52,996 DEBUG TRAIN Batch 13/2500 loss 7.548845 loss_att 6.491336 loss_ctc 10.016367 lr 0.00199892 rank 0\n",
            "2022-05-23 17:12:22,391 DEBUG TRAIN Batch 13/2600 loss 7.573894 loss_att 6.413079 loss_ctc 10.282460 lr 0.00199792 rank 0\n",
            "2022-05-23 17:12:51,509 DEBUG TRAIN Batch 13/2700 loss 9.204800 loss_att 8.207396 loss_ctc 11.532074 lr 0.00199693 rank 0\n",
            "2022-05-23 17:13:21,255 DEBUG TRAIN Batch 13/2800 loss 12.518973 loss_att 11.195963 loss_ctc 15.605998 lr 0.00199593 rank 0\n",
            "2022-05-23 17:13:51,168 DEBUG TRAIN Batch 13/2900 loss 11.353464 loss_att 11.205818 loss_ctc 11.697972 lr 0.00199494 rank 0\n",
            "2022-05-23 17:14:22,235 DEBUG TRAIN Batch 13/3000 loss 8.357356 loss_att 7.237887 loss_ctc 10.969452 lr 0.00199395 rank 0\n",
            "2022-05-23 17:14:51,517 DEBUG TRAIN Batch 13/3100 loss 10.286512 loss_att 9.388479 loss_ctc 12.381924 lr 0.00199296 rank 0\n",
            "2022-05-23 17:15:21,015 DEBUG TRAIN Batch 13/3200 loss 9.770308 loss_att 8.867388 loss_ctc 11.877118 lr 0.00199197 rank 0\n",
            "2022-05-23 17:15:50,756 DEBUG TRAIN Batch 13/3300 loss 8.438819 loss_att 7.591820 loss_ctc 10.415149 lr 0.00199098 rank 0\n",
            "2022-05-23 17:16:20,451 DEBUG TRAIN Batch 13/3400 loss 11.868225 loss_att 11.777588 loss_ctc 12.079714 lr 0.00199000 rank 0\n",
            "2022-05-23 17:16:51,222 DEBUG TRAIN Batch 13/3500 loss 11.293007 loss_att 9.560100 loss_ctc 15.336455 lr 0.00198901 rank 0\n",
            "2022-05-23 17:17:21,007 DEBUG TRAIN Batch 13/3600 loss 6.663178 loss_att 5.998540 loss_ctc 8.214001 lr 0.00198803 rank 0\n",
            "2022-05-23 17:17:50,994 DEBUG TRAIN Batch 13/3700 loss 8.026855 loss_att 7.065665 loss_ctc 10.269630 lr 0.00198705 rank 0\n",
            "2022-05-23 17:18:20,804 DEBUG TRAIN Batch 13/3800 loss 9.189067 loss_att 8.451738 loss_ctc 10.909499 lr 0.00198607 rank 0\n",
            "2022-05-23 17:18:50,726 DEBUG TRAIN Batch 13/3900 loss 12.725178 loss_att 12.008934 loss_ctc 14.396415 lr 0.00198509 rank 0\n",
            "2022-05-23 17:19:21,384 DEBUG TRAIN Batch 13/4000 loss 13.434187 loss_att 12.275351 loss_ctc 16.138138 lr 0.00198411 rank 0\n",
            "2022-05-23 17:19:51,010 DEBUG TRAIN Batch 13/4100 loss 10.729395 loss_att 9.942077 loss_ctc 12.566469 lr 0.00198314 rank 0\n",
            "2022-05-23 17:20:20,233 DEBUG TRAIN Batch 13/4200 loss 10.757142 loss_att 10.020543 loss_ctc 12.475874 lr 0.00198216 rank 0\n",
            "2022-05-23 17:20:50,052 DEBUG TRAIN Batch 13/4300 loss 11.037822 loss_att 10.332970 loss_ctc 12.682478 lr 0.00198119 rank 0\n",
            "2022-05-23 17:21:20,252 DEBUG TRAIN Batch 13/4400 loss 12.106608 loss_att 11.949477 loss_ctc 12.473249 lr 0.00198022 rank 0\n",
            "2022-05-23 17:21:51,227 DEBUG TRAIN Batch 13/4500 loss 7.653611 loss_att 6.960793 loss_ctc 9.270185 lr 0.00197925 rank 0\n",
            "2022-05-23 17:22:20,547 DEBUG TRAIN Batch 13/4600 loss 9.588522 loss_att 8.661041 loss_ctc 11.752644 lr 0.00197828 rank 0\n",
            "2022-05-23 17:22:49,688 DEBUG TRAIN Batch 13/4700 loss 6.792796 loss_att 6.197020 loss_ctc 8.182941 lr 0.00197731 rank 0\n",
            "2022-05-23 17:23:19,444 DEBUG TRAIN Batch 13/4800 loss 10.827564 loss_att 9.752687 loss_ctc 13.335609 lr 0.00197635 rank 0\n",
            "2022-05-23 17:23:49,079 DEBUG TRAIN Batch 13/4900 loss 10.526335 loss_att 10.585217 loss_ctc 10.388940 lr 0.00197538 rank 0\n",
            "2022-05-23 17:24:20,229 DEBUG TRAIN Batch 13/5000 loss 7.583076 loss_att 6.964758 loss_ctc 9.025820 lr 0.00197442 rank 0\n",
            "2022-05-23 17:24:49,890 DEBUG TRAIN Batch 13/5100 loss 9.028103 loss_att 8.209302 loss_ctc 10.938639 lr 0.00197346 rank 0\n",
            "2022-05-23 17:25:19,401 DEBUG TRAIN Batch 13/5200 loss 9.351354 loss_att 8.073662 loss_ctc 12.332634 lr 0.00197250 rank 0\n",
            "2022-05-23 17:25:48,981 DEBUG TRAIN Batch 13/5300 loss 9.027646 loss_att 8.309886 loss_ctc 10.702419 lr 0.00197154 rank 0\n",
            "2022-05-23 17:26:18,766 DEBUG TRAIN Batch 13/5400 loss 11.368196 loss_att 10.653762 loss_ctc 13.035209 lr 0.00197058 rank 0\n",
            "2022-05-23 17:26:49,595 DEBUG TRAIN Batch 13/5500 loss 8.521147 loss_att 7.251555 loss_ctc 11.483527 lr 0.00196963 rank 0\n",
            "2022-05-23 17:27:18,943 DEBUG TRAIN Batch 13/5600 loss 8.596873 loss_att 8.040087 loss_ctc 9.896041 lr 0.00196867 rank 0\n",
            "2022-05-23 17:27:48,837 DEBUG TRAIN Batch 13/5700 loss 11.489424 loss_att 10.372786 loss_ctc 14.094913 lr 0.00196772 rank 0\n",
            "2022-05-23 17:28:18,563 DEBUG TRAIN Batch 13/5800 loss 10.436258 loss_att 9.461010 loss_ctc 12.711839 lr 0.00196677 rank 0\n",
            "2022-05-23 17:28:48,121 DEBUG TRAIN Batch 13/5900 loss 11.699342 loss_att 11.069487 loss_ctc 13.169004 lr 0.00196582 rank 0\n",
            "2022-05-23 17:29:19,425 DEBUG TRAIN Batch 13/6000 loss 8.818240 loss_att 8.028028 loss_ctc 10.662067 lr 0.00196487 rank 0\n",
            "2022-05-23 17:29:48,597 DEBUG TRAIN Batch 13/6100 loss 5.043298 loss_att 4.829582 loss_ctc 5.541969 lr 0.00196392 rank 0\n",
            "2022-05-23 17:30:18,265 DEBUG TRAIN Batch 13/6200 loss 11.144686 loss_att 9.954571 loss_ctc 13.921620 lr 0.00196297 rank 0\n",
            "2022-05-23 17:30:47,526 DEBUG TRAIN Batch 13/6300 loss 9.171316 loss_att 8.389654 loss_ctc 10.995193 lr 0.00196203 rank 0\n",
            "2022-05-23 17:31:17,958 DEBUG TRAIN Batch 13/6400 loss 13.009178 loss_att 12.277420 loss_ctc 14.716616 lr 0.00196109 rank 0\n",
            "2022-05-23 17:31:48,544 DEBUG TRAIN Batch 13/6500 loss 6.460223 loss_att 5.685353 loss_ctc 8.268255 lr 0.00196014 rank 0\n",
            "2022-05-23 17:32:18,257 DEBUG TRAIN Batch 13/6600 loss 7.341677 loss_att 6.571885 loss_ctc 9.137858 lr 0.00195920 rank 0\n",
            "2022-05-23 17:32:47,924 DEBUG TRAIN Batch 13/6700 loss 12.395571 loss_att 11.577166 loss_ctc 14.305186 lr 0.00195826 rank 0\n",
            "2022-05-23 17:33:18,076 DEBUG TRAIN Batch 13/6800 loss 9.675142 loss_att 9.153591 loss_ctc 10.892094 lr 0.00195733 rank 0\n",
            "2022-05-23 17:33:48,200 DEBUG TRAIN Batch 13/6900 loss 11.567587 loss_att 11.114281 loss_ctc 12.625299 lr 0.00195639 rank 0\n",
            "2022-05-23 17:34:19,148 DEBUG TRAIN Batch 13/7000 loss 8.661301 loss_att 6.955575 loss_ctc 12.641329 lr 0.00195545 rank 0\n",
            "2022-05-23 17:34:48,539 DEBUG TRAIN Batch 13/7100 loss 10.459934 loss_att 9.503008 loss_ctc 12.692762 lr 0.00195452 rank 0\n",
            "2022-05-23 17:35:18,272 DEBUG TRAIN Batch 13/7200 loss 9.872093 loss_att 8.744653 loss_ctc 12.502787 lr 0.00195359 rank 0\n",
            "2022-05-23 17:35:48,229 DEBUG TRAIN Batch 13/7300 loss 9.581343 loss_att 8.822771 loss_ctc 11.351345 lr 0.00195266 rank 0\n",
            "2022-05-23 17:36:17,991 DEBUG TRAIN Batch 13/7400 loss 9.704862 loss_att 9.853289 loss_ctc 9.358532 lr 0.00195173 rank 0\n",
            "2022-05-23 17:36:48,158 DEBUG TRAIN Batch 13/7500 loss 10.002446 loss_att 8.789513 loss_ctc 12.832623 lr 0.00195080 rank 0\n",
            "2022-05-23 17:36:53,938 DEBUG CV Batch 13/0 loss 4.707210 loss_att 4.553897 loss_ctc 5.064938 history loss 4.430315 rank 0\n",
            "2022-05-23 17:37:05,165 DEBUG CV Batch 13/100 loss 3.197789 loss_att 3.062756 loss_ctc 3.512866 history loss 7.332350 rank 0\n",
            "2022-05-23 17:37:15,386 DEBUG CV Batch 13/200 loss 5.913972 loss_att 5.513378 loss_ctc 6.848689 history loss 7.276727 rank 0\n",
            "2022-05-23 17:37:26,137 DEBUG CV Batch 13/300 loss 4.770311 loss_att 4.905757 loss_ctc 4.454272 history loss 6.845716 rank 0\n",
            "2022-05-23 17:37:37,719 DEBUG CV Batch 13/400 loss 9.531057 loss_att 9.270647 loss_ctc 10.138681 history loss 6.341547 rank 0\n",
            "2022-05-23 17:37:49,826 DEBUG CV Batch 13/500 loss 4.255318 loss_att 4.023313 loss_ctc 4.796663 history loss 6.303946 rank 0\n",
            "2022-05-23 17:38:01,556 DEBUG CV Batch 13/600 loss 5.192307 loss_att 5.030859 loss_ctc 5.569017 history loss 6.251369 rank 0\n",
            "2022-05-23 17:38:12,511 DEBUG CV Batch 13/700 loss 4.642512 loss_att 4.408822 loss_ctc 5.187788 history loss 6.132778 rank 0\n",
            "2022-05-23 17:38:23,814 DEBUG CV Batch 13/800 loss 5.524333 loss_att 5.181518 loss_ctc 6.324235 history loss 6.086327 rank 0\n",
            "2022-05-23 17:38:35,281 INFO Epoch 13 CV info cv_loss 6.094451947031495\n",
            "2022-05-23 17:38:35,281 INFO Checkpoint: save to checkpoint exp/conformer/13.pt\n",
            "2022-05-23 17:38:35,647 INFO Epoch 14 TRAIN info lr 0.001950760017847769\n",
            "2022-05-23 17:38:35,649 INFO using accumulate grad, new batch size is 4 times larger than before\n",
            "2022-05-23 17:39:00,248 DEBUG TRAIN Batch 14/0 loss 7.334675 loss_att 6.336987 loss_ctc 9.662611 lr 0.00195072 rank 0\n",
            "2022-05-23 17:39:30,201 DEBUG TRAIN Batch 14/100 loss 10.870136 loss_att 9.791666 loss_ctc 13.386569 lr 0.00194980 rank 0\n",
            "2022-05-23 17:39:59,857 DEBUG TRAIN Batch 14/200 loss 7.665696 loss_att 6.938092 loss_ctc 9.363439 lr 0.00194887 rank 0\n",
            "2022-05-23 17:40:28,628 DEBUG TRAIN Batch 14/300 loss 8.358992 loss_att 7.529038 loss_ctc 10.295551 lr 0.00194795 rank 0\n",
            "2022-05-23 17:40:58,019 DEBUG TRAIN Batch 14/400 loss 11.607385 loss_att 10.996004 loss_ctc 13.033939 lr 0.00194702 rank 0\n",
            "2022-05-23 17:41:28,759 DEBUG TRAIN Batch 14/500 loss 7.379837 loss_att 6.795241 loss_ctc 8.743893 lr 0.00194610 rank 0\n",
            "2022-05-23 17:41:57,664 DEBUG TRAIN Batch 14/600 loss 9.488310 loss_att 8.551289 loss_ctc 11.674693 lr 0.00194518 rank 0\n",
            "2022-05-23 17:42:27,078 DEBUG TRAIN Batch 14/700 loss 11.599916 loss_att 10.660750 loss_ctc 13.791301 lr 0.00194426 rank 0\n",
            "2022-05-23 17:42:56,817 DEBUG TRAIN Batch 14/800 loss 9.670302 loss_att 8.841206 loss_ctc 11.604862 lr 0.00194334 rank 0\n",
            "2022-05-23 17:43:27,213 DEBUG TRAIN Batch 14/900 loss 12.613175 loss_att 11.939181 loss_ctc 14.185828 lr 0.00194243 rank 0\n",
            "2022-05-23 17:43:57,686 DEBUG TRAIN Batch 14/1000 loss 13.292594 loss_att 11.783060 loss_ctc 16.814838 lr 0.00194151 rank 0\n",
            "2022-05-23 17:44:27,126 DEBUG TRAIN Batch 14/1100 loss 10.846111 loss_att 10.625407 loss_ctc 11.361086 lr 0.00194060 rank 0\n",
            "2022-05-23 17:44:56,889 DEBUG TRAIN Batch 14/1200 loss 7.635865 loss_att 6.727401 loss_ctc 9.755612 lr 0.00193968 rank 0\n",
            "2022-05-23 17:45:26,827 DEBUG TRAIN Batch 14/1300 loss 15.105557 loss_att 13.619233 loss_ctc 18.573647 lr 0.00193877 rank 0\n",
            "2022-05-23 17:45:56,944 DEBUG TRAIN Batch 14/1400 loss 10.813573 loss_att 10.082460 loss_ctc 12.519503 lr 0.00193786 rank 0\n",
            "2022-05-23 17:46:27,869 DEBUG TRAIN Batch 14/1500 loss 5.752599 loss_att 4.803823 loss_ctc 7.966409 lr 0.00193695 rank 0\n",
            "2022-05-23 17:46:56,836 DEBUG TRAIN Batch 14/1600 loss 5.469537 loss_att 4.728528 loss_ctc 7.198558 lr 0.00193604 rank 0\n",
            "2022-05-23 17:47:26,343 DEBUG TRAIN Batch 14/1700 loss 13.071918 loss_att 11.891365 loss_ctc 15.826544 lr 0.00193514 rank 0\n",
            "2022-05-23 17:47:56,408 DEBUG TRAIN Batch 14/1800 loss 11.180415 loss_att 10.441162 loss_ctc 12.905337 lr 0.00193423 rank 0\n",
            "2022-05-23 17:48:26,504 DEBUG TRAIN Batch 14/1900 loss 12.033470 loss_att 10.688027 loss_ctc 15.172838 lr 0.00193333 rank 0\n",
            "2022-05-23 17:48:57,816 DEBUG TRAIN Batch 14/2000 loss 10.218910 loss_att 8.481037 loss_ctc 14.273946 lr 0.00193243 rank 0\n",
            "2022-05-23 17:49:27,022 DEBUG TRAIN Batch 14/2100 loss 9.654651 loss_att 8.519199 loss_ctc 12.304038 lr 0.00193152 rank 0\n",
            "2022-05-23 17:49:56,794 DEBUG TRAIN Batch 14/2200 loss 8.500639 loss_att 7.972120 loss_ctc 9.733849 lr 0.00193062 rank 0\n",
            "2022-05-23 17:50:26,490 DEBUG TRAIN Batch 14/2300 loss 9.390265 loss_att 8.766493 loss_ctc 10.845736 lr 0.00192973 rank 0\n",
            "2022-05-23 17:50:56,607 DEBUG TRAIN Batch 14/2400 loss 12.862295 loss_att 12.683331 loss_ctc 13.279875 lr 0.00192883 rank 0\n",
            "2022-05-23 17:51:27,554 DEBUG TRAIN Batch 14/2500 loss 8.048182 loss_att 6.856018 loss_ctc 10.829901 lr 0.00192793 rank 0\n",
            "2022-05-23 17:51:57,210 DEBUG TRAIN Batch 14/2600 loss 7.612887 loss_att 6.595086 loss_ctc 9.987759 lr 0.00192704 rank 0\n",
            "2022-05-23 17:52:27,048 DEBUG TRAIN Batch 14/2700 loss 8.347782 loss_att 7.388270 loss_ctc 10.586641 lr 0.00192614 rank 0\n",
            "2022-05-23 17:52:56,637 DEBUG TRAIN Batch 14/2800 loss 7.653261 loss_att 7.093227 loss_ctc 8.960007 lr 0.00192525 rank 0\n",
            "2022-05-23 17:53:26,926 DEBUG TRAIN Batch 14/2900 loss 11.849895 loss_att 10.820185 loss_ctc 14.252554 lr 0.00192436 rank 0\n",
            "2022-05-23 17:53:57,774 DEBUG TRAIN Batch 14/3000 loss 8.155214 loss_att 7.183744 loss_ctc 10.421977 lr 0.00192347 rank 0\n",
            "2022-05-23 17:54:27,159 DEBUG TRAIN Batch 14/3100 loss 5.410287 loss_att 5.144781 loss_ctc 6.029800 lr 0.00192258 rank 0\n",
            "2022-05-23 17:54:56,753 DEBUG TRAIN Batch 14/3200 loss 8.146974 loss_att 7.759093 loss_ctc 9.052028 lr 0.00192169 rank 0\n",
            "2022-05-23 17:55:26,548 DEBUG TRAIN Batch 14/3300 loss 8.811740 loss_att 8.661526 loss_ctc 9.162240 lr 0.00192081 rank 0\n",
            "2022-05-23 17:55:56,473 DEBUG TRAIN Batch 14/3400 loss 12.936058 loss_att 12.094534 loss_ctc 14.899617 lr 0.00191992 rank 0\n",
            "2022-05-23 17:56:27,537 DEBUG TRAIN Batch 14/3500 loss 10.735342 loss_att 9.308074 loss_ctc 14.065636 lr 0.00191904 rank 0\n",
            "2022-05-23 17:56:57,023 DEBUG TRAIN Batch 14/3600 loss 13.058996 loss_att 12.328728 loss_ctc 14.762956 lr 0.00191815 rank 0\n",
            "2022-05-23 17:57:26,373 DEBUG TRAIN Batch 14/3700 loss 7.121769 loss_att 6.654014 loss_ctc 8.213199 lr 0.00191727 rank 0\n",
            "2022-05-23 17:57:56,016 DEBUG TRAIN Batch 14/3800 loss 10.213032 loss_att 9.518372 loss_ctc 11.833907 lr 0.00191639 rank 0\n",
            "2022-05-23 17:58:25,746 DEBUG TRAIN Batch 14/3900 loss 13.887719 loss_att 13.166368 loss_ctc 15.570871 lr 0.00191551 rank 0\n",
            "2022-05-23 17:58:56,704 DEBUG TRAIN Batch 14/4000 loss 9.893415 loss_att 9.178083 loss_ctc 11.562524 lr 0.00191463 rank 0\n",
            "2022-05-23 17:59:26,431 DEBUG TRAIN Batch 14/4100 loss 5.948092 loss_att 5.282345 loss_ctc 7.501502 lr 0.00191376 rank 0\n",
            "2022-05-23 17:59:56,053 DEBUG TRAIN Batch 14/4200 loss 8.329081 loss_att 7.726609 loss_ctc 9.734848 lr 0.00191288 rank 0\n",
            "2022-05-23 18:00:25,911 DEBUG TRAIN Batch 14/4300 loss 9.453794 loss_att 8.334295 loss_ctc 12.065956 lr 0.00191201 rank 0\n",
            "2022-05-23 18:00:55,837 DEBUG TRAIN Batch 14/4400 loss 8.888172 loss_att 8.572901 loss_ctc 9.623807 lr 0.00191113 rank 0\n",
            "2022-05-23 18:01:26,663 DEBUG TRAIN Batch 14/4500 loss 8.875675 loss_att 7.333844 loss_ctc 12.473279 lr 0.00191026 rank 0\n",
            "2022-05-23 18:01:56,413 DEBUG TRAIN Batch 14/4600 loss 7.765658 loss_att 6.976312 loss_ctc 9.607467 lr 0.00190939 rank 0\n",
            "2022-05-23 18:02:26,140 DEBUG TRAIN Batch 14/4700 loss 9.802603 loss_att 9.230846 loss_ctc 11.136700 lr 0.00190852 rank 0\n",
            "2022-05-23 18:02:55,888 DEBUG TRAIN Batch 14/4800 loss 8.439724 loss_att 7.772344 loss_ctc 9.996944 lr 0.00190765 rank 0\n",
            "2022-05-23 18:03:25,770 DEBUG TRAIN Batch 14/4900 loss 10.616025 loss_att 10.174653 loss_ctc 11.645891 lr 0.00190679 rank 0\n",
            "2022-05-23 18:03:56,364 DEBUG TRAIN Batch 14/5000 loss 8.719134 loss_att 7.798553 loss_ctc 10.867157 lr 0.00190592 rank 0\n",
            "2022-05-23 18:04:26,028 DEBUG TRAIN Batch 14/5100 loss 7.798166 loss_att 7.351718 loss_ctc 8.839876 lr 0.00190506 rank 0\n",
            "2022-05-23 18:04:55,781 DEBUG TRAIN Batch 14/5200 loss 5.678991 loss_att 5.592759 loss_ctc 5.880200 lr 0.00190419 rank 0\n",
            "2022-05-23 18:05:25,424 DEBUG TRAIN Batch 14/5300 loss 9.695222 loss_att 8.813450 loss_ctc 11.752688 lr 0.00190333 rank 0\n",
            "2022-05-23 18:05:55,530 DEBUG TRAIN Batch 14/5400 loss 12.933657 loss_att 12.104589 loss_ctc 14.868147 lr 0.00190247 rank 0\n",
            "2022-05-23 18:06:26,407 DEBUG TRAIN Batch 14/5500 loss 7.458369 loss_att 6.501510 loss_ctc 9.691040 lr 0.00190161 rank 0\n",
            "2022-05-23 18:06:56,044 DEBUG TRAIN Batch 14/5600 loss 7.803668 loss_att 7.338199 loss_ctc 8.889764 lr 0.00190075 rank 0\n",
            "2022-05-23 18:07:25,468 DEBUG TRAIN Batch 14/5700 loss 9.754632 loss_att 8.729963 loss_ctc 12.145525 lr 0.00189989 rank 0\n",
            "2022-05-23 18:07:55,274 DEBUG TRAIN Batch 14/5800 loss 7.012059 loss_att 6.501501 loss_ctc 8.203362 lr 0.00189903 rank 0\n",
            "2022-05-23 18:08:25,246 DEBUG TRAIN Batch 14/5900 loss 12.445514 loss_att 12.103330 loss_ctc 13.243941 lr 0.00189818 rank 0\n",
            "2022-05-23 18:08:56,409 DEBUG TRAIN Batch 14/6000 loss 8.709296 loss_att 7.476461 loss_ctc 11.585912 lr 0.00189732 rank 0\n",
            "2022-05-23 18:09:26,031 DEBUG TRAIN Batch 14/6100 loss 10.139711 loss_att 8.628957 loss_ctc 13.664804 lr 0.00189647 rank 0\n",
            "2022-05-23 18:09:55,426 DEBUG TRAIN Batch 14/6200 loss 12.785683 loss_att 11.571978 loss_ctc 15.617662 lr 0.00189562 rank 0\n",
            "2022-05-23 18:10:25,178 DEBUG TRAIN Batch 14/6300 loss 13.023055 loss_att 11.368683 loss_ctc 16.883257 lr 0.00189477 rank 0\n",
            "2022-05-23 18:10:55,478 DEBUG TRAIN Batch 14/6400 loss 11.389084 loss_att 10.109566 loss_ctc 14.374628 lr 0.00189392 rank 0\n",
            "2022-05-23 18:11:26,039 DEBUG TRAIN Batch 14/6500 loss 6.025427 loss_att 5.374788 loss_ctc 7.543585 lr 0.00189307 rank 0\n",
            "2022-05-23 18:11:55,505 DEBUG TRAIN Batch 14/6600 loss 5.887894 loss_att 5.384964 loss_ctc 7.061395 lr 0.00189222 rank 0\n",
            "2022-05-23 18:12:25,110 DEBUG TRAIN Batch 14/6700 loss 6.918670 loss_att 6.428271 loss_ctc 8.062933 lr 0.00189138 rank 0\n",
            "2022-05-23 18:12:55,161 DEBUG TRAIN Batch 14/6800 loss 10.423533 loss_att 9.379749 loss_ctc 12.859029 lr 0.00189053 rank 0\n",
            "2022-05-23 18:13:25,325 DEBUG TRAIN Batch 14/6900 loss 11.634620 loss_att 11.320062 loss_ctc 12.368587 lr 0.00188969 rank 0\n",
            "2022-05-23 18:13:55,735 DEBUG TRAIN Batch 14/7000 loss 11.629999 loss_att 10.542564 loss_ctc 14.167344 lr 0.00188884 rank 0\n",
            "2022-05-23 18:14:25,404 DEBUG TRAIN Batch 14/7100 loss 9.556048 loss_att 8.797466 loss_ctc 11.326074 lr 0.00188800 rank 0\n",
            "2022-05-23 18:14:55,568 DEBUG TRAIN Batch 14/7200 loss 8.284190 loss_att 7.243583 loss_ctc 10.712275 lr 0.00188716 rank 0\n",
            "2022-05-23 18:15:25,298 DEBUG TRAIN Batch 14/7300 loss 6.539035 loss_att 6.261288 loss_ctc 7.187111 lr 0.00188632 rank 0\n",
            "2022-05-23 18:15:54,999 DEBUG TRAIN Batch 14/7400 loss 9.241562 loss_att 8.844442 loss_ctc 10.168175 lr 0.00188548 rank 0\n",
            "2022-05-23 18:16:25,828 DEBUG TRAIN Batch 14/7500 loss 5.908792 loss_att 5.536649 loss_ctc 6.777127 lr 0.00188465 rank 0\n",
            "2022-05-23 18:16:32,091 DEBUG CV Batch 14/0 loss 4.631183 loss_att 4.568210 loss_ctc 4.778119 history loss 4.358760 rank 0\n",
            "2022-05-23 18:16:43,538 DEBUG CV Batch 14/100 loss 3.223831 loss_att 3.172502 loss_ctc 3.343600 history loss 6.775831 rank 0\n",
            "2022-05-23 18:16:54,088 DEBUG CV Batch 14/200 loss 5.772202 loss_att 5.215434 loss_ctc 7.071328 history loss 6.808095 rank 0\n",
            "2022-05-23 18:17:04,962 DEBUG CV Batch 14/300 loss 4.152966 loss_att 4.290512 loss_ctc 3.832025 history loss 6.401299 rank 0\n",
            "2022-05-23 18:17:16,735 DEBUG CV Batch 14/400 loss 10.092068 loss_att 9.871948 loss_ctc 10.605679 history loss 5.920396 rank 0\n",
            "2022-05-23 18:17:28,853 DEBUG CV Batch 14/500 loss 3.318375 loss_att 3.253253 loss_ctc 3.470325 history loss 5.869628 rank 0\n",
            "2022-05-23 18:17:40,325 DEBUG CV Batch 14/600 loss 5.068889 loss_att 5.109337 loss_ctc 4.974510 history loss 5.837749 rank 0\n",
            "2022-05-23 18:17:51,364 DEBUG CV Batch 14/700 loss 4.764065 loss_att 4.350204 loss_ctc 5.729743 history loss 5.719663 rank 0\n",
            "2022-05-23 18:18:02,944 DEBUG CV Batch 14/800 loss 5.487568 loss_att 5.052881 loss_ctc 6.501837 history loss 5.690529 rank 0\n",
            "2022-05-23 18:18:14,514 INFO Epoch 14 CV info cv_loss 5.701034731414618\n",
            "2022-05-23 18:18:14,514 INFO Checkpoint: save to checkpoint exp/conformer/14.pt\n",
            "2022-05-23 18:18:14,862 INFO Epoch 15 TRAIN info lr 0.00188461322400235\n",
            "2022-05-23 18:18:14,864 INFO using accumulate grad, new batch size is 4 times larger than before\n",
            "2022-05-23 18:18:40,110 DEBUG TRAIN Batch 15/0 loss 8.564726 loss_att 7.082206 loss_ctc 12.023941 lr 0.00188458 rank 0\n",
            "2022-05-23 18:19:09,895 DEBUG TRAIN Batch 15/100 loss 8.003614 loss_att 6.931447 loss_ctc 10.505341 lr 0.00188374 rank 0\n",
            "2022-05-23 18:19:39,297 DEBUG TRAIN Batch 15/200 loss 8.943174 loss_att 8.103306 loss_ctc 10.902866 lr 0.00188291 rank 0\n",
            "2022-05-23 18:20:08,716 DEBUG TRAIN Batch 15/300 loss 10.083721 loss_att 9.775978 loss_ctc 10.801788 lr 0.00188207 rank 0\n",
            "2022-05-23 18:20:38,726 DEBUG TRAIN Batch 15/400 loss 10.812708 loss_att 9.791027 loss_ctc 13.196631 lr 0.00188124 rank 0\n",
            "2022-05-23 18:21:09,943 DEBUG TRAIN Batch 15/500 loss 5.988353 loss_att 5.113934 loss_ctc 8.028666 lr 0.00188041 rank 0\n",
            "2022-05-23 18:21:38,840 DEBUG TRAIN Batch 15/600 loss 9.059580 loss_att 8.022703 loss_ctc 11.478959 lr 0.00187958 rank 0\n",
            "2022-05-23 18:22:08,534 DEBUG TRAIN Batch 15/700 loss 8.416761 loss_att 7.303373 loss_ctc 11.014666 lr 0.00187875 rank 0\n",
            "2022-05-23 18:22:38,276 DEBUG TRAIN Batch 15/800 loss 8.724394 loss_att 7.967222 loss_ctc 10.491131 lr 0.00187792 rank 0\n",
            "2022-05-23 18:23:08,419 DEBUG TRAIN Batch 15/900 loss 10.322379 loss_att 9.863306 loss_ctc 11.393549 lr 0.00187709 rank 0\n",
            "2022-05-23 18:23:38,911 DEBUG TRAIN Batch 15/1000 loss 8.145733 loss_att 6.961402 loss_ctc 10.909174 lr 0.00187627 rank 0\n",
            "2022-05-23 18:24:08,518 DEBUG TRAIN Batch 15/1100 loss 9.991390 loss_att 8.667322 loss_ctc 13.080881 lr 0.00187544 rank 0\n",
            "2022-05-23 18:24:37,908 DEBUG TRAIN Batch 15/1200 loss 6.588544 loss_att 6.307561 loss_ctc 7.244169 lr 0.00187462 rank 0\n",
            "2022-05-23 18:25:07,635 DEBUG TRAIN Batch 15/1300 loss 7.814587 loss_att 7.300889 loss_ctc 9.013213 lr 0.00187380 rank 0\n",
            "2022-05-23 18:25:37,618 DEBUG TRAIN Batch 15/1400 loss 9.823392 loss_att 9.514618 loss_ctc 10.543865 lr 0.00187297 rank 0\n",
            "2022-05-23 18:26:08,171 DEBUG TRAIN Batch 15/1500 loss 8.944527 loss_att 7.799520 loss_ctc 11.616208 lr 0.00187215 rank 0\n",
            "2022-05-23 18:26:37,872 DEBUG TRAIN Batch 15/1600 loss 6.445263 loss_att 5.394765 loss_ctc 8.896423 lr 0.00187133 rank 0\n",
            "2022-05-23 18:27:07,205 DEBUG TRAIN Batch 15/1700 loss 8.355522 loss_att 7.800570 loss_ctc 9.650410 lr 0.00187052 rank 0\n",
            "2022-05-23 18:27:36,954 DEBUG TRAIN Batch 15/1800 loss 10.775888 loss_att 9.853326 loss_ctc 12.928535 lr 0.00186970 rank 0\n",
            "2022-05-23 18:28:07,512 DEBUG TRAIN Batch 15/1900 loss 13.461834 loss_att 12.592048 loss_ctc 15.491337 lr 0.00186888 rank 0\n",
            "2022-05-23 18:28:38,265 DEBUG TRAIN Batch 15/2000 loss 11.813513 loss_att 10.854416 loss_ctc 14.051405 lr 0.00186807 rank 0\n",
            "2022-05-23 18:29:07,555 DEBUG TRAIN Batch 15/2100 loss 9.889823 loss_att 8.682457 loss_ctc 12.707009 lr 0.00186725 rank 0\n",
            "2022-05-23 18:29:37,024 DEBUG TRAIN Batch 15/2200 loss 11.842056 loss_att 10.907135 loss_ctc 14.023540 lr 0.00186644 rank 0\n",
            "2022-05-23 18:30:06,800 DEBUG TRAIN Batch 15/2300 loss 10.854804 loss_att 10.075075 loss_ctc 12.674170 lr 0.00186563 rank 0\n",
            "2022-05-23 18:30:36,738 DEBUG TRAIN Batch 15/2400 loss 8.873610 loss_att 8.101505 loss_ctc 10.675188 lr 0.00186482 rank 0\n",
            "2022-05-23 18:31:07,862 DEBUG TRAIN Batch 15/2500 loss 8.152115 loss_att 7.368084 loss_ctc 9.981522 lr 0.00186400 rank 0\n",
            "2022-05-23 18:31:36,874 DEBUG TRAIN Batch 15/2600 loss 7.056114 loss_att 6.171093 loss_ctc 9.121162 lr 0.00186320 rank 0\n",
            "2022-05-23 18:32:06,512 DEBUG TRAIN Batch 15/2700 loss 8.927902 loss_att 8.246102 loss_ctc 10.518766 lr 0.00186239 rank 0\n",
            "2022-05-23 18:32:36,675 DEBUG TRAIN Batch 15/2800 loss 8.461924 loss_att 8.087864 loss_ctc 9.334730 lr 0.00186158 rank 0\n",
            "2022-05-23 18:33:06,157 DEBUG TRAIN Batch 15/2900 loss 12.519747 loss_att 11.383805 loss_ctc 15.170279 lr 0.00186078 rank 0\n",
            "2022-05-23 18:33:37,057 DEBUG TRAIN Batch 15/3000 loss 4.893496 loss_att 4.602283 loss_ctc 5.572992 lr 0.00185997 rank 0\n",
            "2022-05-23 18:34:06,942 DEBUG TRAIN Batch 15/3100 loss 8.869843 loss_att 8.057484 loss_ctc 10.765350 lr 0.00185917 rank 0\n",
            "2022-05-23 18:34:36,674 DEBUG TRAIN Batch 15/3200 loss 10.533369 loss_att 9.254641 loss_ctc 13.517067 lr 0.00185836 rank 0\n",
            "2022-05-23 18:35:06,694 DEBUG TRAIN Batch 15/3300 loss 7.156719 loss_att 6.490450 loss_ctc 8.711346 lr 0.00185756 rank 0\n",
            "2022-05-23 18:35:36,509 DEBUG TRAIN Batch 15/3400 loss 9.740788 loss_att 9.079485 loss_ctc 11.283827 lr 0.00185676 rank 0\n",
            "2022-05-23 18:36:07,185 DEBUG TRAIN Batch 15/3500 loss 8.772763 loss_att 7.963366 loss_ctc 10.661357 lr 0.00185596 rank 0\n",
            "2022-05-23 18:36:36,963 DEBUG TRAIN Batch 15/3600 loss 8.126226 loss_att 7.522995 loss_ctc 9.533765 lr 0.00185516 rank 0\n",
            "2022-05-23 18:37:06,560 DEBUG TRAIN Batch 15/3700 loss 8.524969 loss_att 8.273751 loss_ctc 9.111142 lr 0.00185437 rank 0\n",
            "2022-05-23 18:37:36,010 DEBUG TRAIN Batch 15/3800 loss 8.435970 loss_att 7.432011 loss_ctc 10.778542 lr 0.00185357 rank 0\n",
            "2022-05-23 18:38:05,532 DEBUG TRAIN Batch 15/3900 loss 9.832967 loss_att 9.206406 loss_ctc 11.294943 lr 0.00185277 rank 0\n",
            "2022-05-23 18:38:36,284 DEBUG TRAIN Batch 15/4000 loss 6.932944 loss_att 6.016105 loss_ctc 9.072235 lr 0.00185198 rank 0\n",
            "2022-05-23 18:39:05,611 DEBUG TRAIN Batch 15/4100 loss 9.832397 loss_att 9.304615 loss_ctc 11.063887 lr 0.00185119 rank 0\n",
            "2022-05-23 18:39:34,623 DEBUG TRAIN Batch 15/4200 loss 8.831877 loss_att 8.369144 loss_ctc 9.911586 lr 0.00185039 rank 0\n",
            "2022-05-23 18:40:04,326 DEBUG TRAIN Batch 15/4300 loss 8.013598 loss_att 7.811408 loss_ctc 8.485378 lr 0.00184960 rank 0\n",
            "2022-05-23 18:40:34,572 DEBUG TRAIN Batch 15/4400 loss 11.566165 loss_att 11.150982 loss_ctc 12.534924 lr 0.00184881 rank 0\n",
            "2022-05-23 18:41:05,491 DEBUG TRAIN Batch 15/4500 loss 7.039919 loss_att 6.634600 loss_ctc 7.985663 lr 0.00184802 rank 0\n",
            "2022-05-23 18:41:35,045 DEBUG TRAIN Batch 15/4600 loss 7.227390 loss_att 6.471227 loss_ctc 8.991769 lr 0.00184723 rank 0\n",
            "2022-05-23 18:42:04,648 DEBUG TRAIN Batch 15/4700 loss 7.983028 loss_att 7.367240 loss_ctc 9.419867 lr 0.00184645 rank 0\n",
            "2022-05-23 18:42:34,784 DEBUG TRAIN Batch 15/4800 loss 11.243259 loss_att 10.037514 loss_ctc 14.056667 lr 0.00184566 rank 0\n",
            "2022-05-23 18:43:04,833 DEBUG TRAIN Batch 15/4900 loss 10.044208 loss_att 9.659398 loss_ctc 10.942097 lr 0.00184487 rank 0\n",
            "2022-05-23 18:43:35,617 DEBUG TRAIN Batch 15/5000 loss 6.807136 loss_att 5.804737 loss_ctc 9.146067 lr 0.00184409 rank 0\n",
            "2022-05-23 18:44:05,067 DEBUG TRAIN Batch 15/5100 loss 9.381207 loss_att 8.666341 loss_ctc 11.049230 lr 0.00184331 rank 0\n",
            "2022-05-23 18:44:34,573 DEBUG TRAIN Batch 15/5200 loss 9.450150 loss_att 8.546791 loss_ctc 11.557988 lr 0.00184252 rank 0\n",
            "2022-05-23 18:45:04,629 DEBUG TRAIN Batch 15/5300 loss 7.086304 loss_att 6.574006 loss_ctc 8.281668 lr 0.00184174 rank 0\n",
            "2022-05-23 18:45:34,730 DEBUG TRAIN Batch 15/5400 loss 8.991574 loss_att 7.972142 loss_ctc 11.370249 lr 0.00184096 rank 0\n",
            "2022-05-23 18:46:05,766 DEBUG TRAIN Batch 15/5500 loss 9.240578 loss_att 8.053927 loss_ctc 12.009428 lr 0.00184018 rank 0\n",
            "2022-05-23 18:46:34,699 DEBUG TRAIN Batch 15/5600 loss 8.792274 loss_att 7.638313 loss_ctc 11.484847 lr 0.00183940 rank 0\n",
            "2022-05-23 18:47:04,238 DEBUG TRAIN Batch 15/5700 loss 8.465897 loss_att 7.816590 loss_ctc 9.980946 lr 0.00183863 rank 0\n",
            "2022-05-23 18:47:34,326 DEBUG TRAIN Batch 15/5800 loss 9.461591 loss_att 8.826204 loss_ctc 10.944159 lr 0.00183785 rank 0\n",
            "2022-05-23 18:48:04,511 DEBUG TRAIN Batch 15/5900 loss 10.333530 loss_att 9.943825 loss_ctc 11.242843 lr 0.00183707 rank 0\n",
            "2022-05-23 18:48:34,719 DEBUG TRAIN Batch 15/6000 loss 9.483963 loss_att 8.874127 loss_ctc 10.906912 lr 0.00183630 rank 0\n",
            "2022-05-23 18:49:04,141 DEBUG TRAIN Batch 15/6100 loss 6.553669 loss_att 6.002941 loss_ctc 7.838701 lr 0.00183553 rank 0\n",
            "2022-05-23 18:49:33,612 DEBUG TRAIN Batch 15/6200 loss 10.739054 loss_att 9.588395 loss_ctc 13.423924 lr 0.00183475 rank 0\n",
            "2022-05-23 18:50:03,577 DEBUG TRAIN Batch 15/6300 loss 13.160055 loss_att 12.573291 loss_ctc 14.529168 lr 0.00183398 rank 0\n",
            "2022-05-23 18:50:33,595 DEBUG TRAIN Batch 15/6400 loss 9.807303 loss_att 9.371256 loss_ctc 10.824748 lr 0.00183321 rank 0\n",
            "2022-05-23 18:51:04,395 DEBUG TRAIN Batch 15/6500 loss 8.382207 loss_att 7.047878 loss_ctc 11.495642 lr 0.00183244 rank 0\n",
            "2022-05-23 18:51:33,889 DEBUG TRAIN Batch 15/6600 loss 7.841299 loss_att 6.875072 loss_ctc 10.095829 lr 0.00183167 rank 0\n",
            "2022-05-23 18:52:03,486 DEBUG TRAIN Batch 15/6700 loss 8.126562 loss_att 6.884577 loss_ctc 11.024527 lr 0.00183091 rank 0\n",
            "2022-05-23 18:52:33,433 DEBUG TRAIN Batch 15/6800 loss 11.925641 loss_att 10.765477 loss_ctc 14.632690 lr 0.00183014 rank 0\n",
            "2022-05-23 18:53:03,833 DEBUG TRAIN Batch 15/6900 loss 10.010185 loss_att 9.290117 loss_ctc 11.690345 lr 0.00182937 rank 0\n",
            "2022-05-23 18:53:34,892 DEBUG TRAIN Batch 15/7000 loss 10.579420 loss_att 8.832371 loss_ctc 14.655867 lr 0.00182861 rank 0\n",
            "2022-05-23 18:54:04,517 DEBUG TRAIN Batch 15/7100 loss 6.168171 loss_att 5.486663 loss_ctc 7.758355 lr 0.00182785 rank 0\n",
            "2022-05-23 18:54:34,322 DEBUG TRAIN Batch 15/7200 loss 5.314012 loss_att 4.865479 loss_ctc 6.360588 lr 0.00182708 rank 0\n",
            "2022-05-23 18:55:04,193 DEBUG TRAIN Batch 15/7300 loss 6.972010 loss_att 6.565358 loss_ctc 7.920866 lr 0.00182632 rank 0\n",
            "2022-05-23 18:55:34,471 DEBUG TRAIN Batch 15/7400 loss 13.367288 loss_att 12.153366 loss_ctc 16.199772 lr 0.00182556 rank 0\n",
            "2022-05-23 18:56:05,265 DEBUG TRAIN Batch 15/7500 loss 9.912127 loss_att 8.824781 loss_ctc 12.449265 lr 0.00182480 rank 0\n",
            "2022-05-23 18:56:11,502 DEBUG CV Batch 15/0 loss 4.279544 loss_att 4.062508 loss_ctc 4.785963 history loss 4.027806 rank 0\n",
            "2022-05-23 18:56:22,850 DEBUG CV Batch 15/100 loss 3.302015 loss_att 3.358994 loss_ctc 3.169064 history loss 6.609951 rank 0\n",
            "2022-05-23 18:56:33,294 DEBUG CV Batch 15/200 loss 5.572663 loss_att 5.449188 loss_ctc 5.860770 history loss 6.559492 rank 0\n",
            "2022-05-23 18:56:44,079 DEBUG CV Batch 15/300 loss 3.802642 loss_att 3.909651 loss_ctc 3.552953 history loss 6.134632 rank 0\n",
            "2022-05-23 18:56:55,692 DEBUG CV Batch 15/400 loss 9.214398 loss_att 8.924506 loss_ctc 9.890814 history loss 5.662418 rank 0\n",
            "2022-05-23 18:57:07,587 DEBUG CV Batch 15/500 loss 2.367294 loss_att 2.444797 loss_ctc 2.186452 history loss 5.604174 rank 0\n",
            "2022-05-23 18:57:18,957 DEBUG CV Batch 15/600 loss 4.078969 loss_att 3.892110 loss_ctc 4.514971 history loss 5.557003 rank 0\n",
            "2022-05-23 18:57:29,995 DEBUG CV Batch 15/700 loss 4.258678 loss_att 4.203413 loss_ctc 4.387632 history loss 5.443132 rank 0\n",
            "2022-05-23 18:57:41,548 DEBUG CV Batch 15/800 loss 5.207770 loss_att 4.942149 loss_ctc 5.827553 history loss 5.422543 rank 0\n",
            "2022-05-23 18:57:52,995 INFO Epoch 15 CV info cv_loss 5.426517270236569\n",
            "2022-05-23 18:57:52,995 INFO Checkpoint: save to checkpoint exp/conformer/15.pt\n",
            "2022-05-23 18:57:53,326 INFO Epoch 16 TRAIN info lr 0.0018247689076508423\n",
            "2022-05-23 18:57:53,327 INFO using accumulate grad, new batch size is 4 times larger than before\n",
            "2022-05-23 18:58:18,471 DEBUG TRAIN Batch 16/0 loss 5.178477 loss_att 4.427156 loss_ctc 6.931559 lr 0.00182474 rank 0\n",
            "2022-05-23 18:58:48,796 DEBUG TRAIN Batch 16/100 loss 9.245619 loss_att 7.976187 loss_ctc 12.207626 lr 0.00182398 rank 0\n",
            "2022-05-23 18:59:18,316 DEBUG TRAIN Batch 16/200 loss 10.250862 loss_att 9.438145 loss_ctc 12.147203 lr 0.00182322 rank 0\n",
            "2022-05-23 18:59:47,817 DEBUG TRAIN Batch 16/300 loss 8.331762 loss_att 7.688018 loss_ctc 9.833834 lr 0.00182246 rank 0\n",
            "2022-05-23 19:00:17,877 DEBUG TRAIN Batch 16/400 loss 13.529319 loss_att 12.287260 loss_ctc 16.427456 lr 0.00182171 rank 0\n",
            "2022-05-23 19:00:48,401 DEBUG TRAIN Batch 16/500 loss 6.303759 loss_att 5.676679 loss_ctc 7.766947 lr 0.00182095 rank 0\n",
            "2022-05-23 19:01:17,627 DEBUG TRAIN Batch 16/600 loss 7.677009 loss_att 6.963524 loss_ctc 9.341808 lr 0.00182020 rank 0\n",
            "2022-05-23 19:01:47,534 DEBUG TRAIN Batch 16/700 loss 6.586602 loss_att 5.667521 loss_ctc 8.731123 lr 0.00181945 rank 0\n",
            "2022-05-23 19:02:17,529 DEBUG TRAIN Batch 16/800 loss 12.184344 loss_att 11.733459 loss_ctc 13.236411 lr 0.00181869 rank 0\n",
            "2022-05-23 19:02:47,413 DEBUG TRAIN Batch 16/900 loss 13.916537 loss_att 12.747583 loss_ctc 16.644096 lr 0.00181794 rank 0\n",
            "2022-05-23 19:03:18,472 DEBUG TRAIN Batch 16/1000 loss 5.627176 loss_att 5.113472 loss_ctc 6.825817 lr 0.00181719 rank 0\n",
            "2022-05-23 19:03:47,913 DEBUG TRAIN Batch 16/1100 loss 7.184646 loss_att 6.737317 loss_ctc 8.228414 lr 0.00181644 rank 0\n",
            "2022-05-23 19:04:18,144 DEBUG TRAIN Batch 16/1200 loss 10.568247 loss_att 9.008303 loss_ctc 14.208115 lr 0.00181569 rank 0\n",
            "2022-05-23 19:04:47,464 DEBUG TRAIN Batch 16/1300 loss 9.116697 loss_att 8.584947 loss_ctc 10.357449 lr 0.00181494 rank 0\n",
            "2022-05-23 19:05:17,301 DEBUG TRAIN Batch 16/1400 loss 10.016195 loss_att 9.767211 loss_ctc 10.597160 lr 0.00181420 rank 0\n",
            "2022-05-23 19:05:48,001 DEBUG TRAIN Batch 16/1500 loss 9.165617 loss_att 8.018038 loss_ctc 11.843300 lr 0.00181345 rank 0\n",
            "2022-05-23 19:06:17,547 DEBUG TRAIN Batch 16/1600 loss 9.649390 loss_att 8.725842 loss_ctc 11.804337 lr 0.00181271 rank 0\n",
            "2022-05-23 19:06:47,093 DEBUG TRAIN Batch 16/1700 loss 7.210804 loss_att 6.498592 loss_ctc 8.872631 lr 0.00181196 rank 0\n",
            "2022-05-23 19:07:16,863 DEBUG TRAIN Batch 16/1800 loss 8.771816 loss_att 7.946709 loss_ctc 10.697067 lr 0.00181122 rank 0\n",
            "2022-05-23 19:07:47,174 DEBUG TRAIN Batch 16/1900 loss 11.226875 loss_att 11.393900 loss_ctc 10.837152 lr 0.00181048 rank 0\n",
            "2022-05-23 19:08:18,085 DEBUG TRAIN Batch 16/2000 loss 10.396255 loss_att 8.484324 loss_ctc 14.857427 lr 0.00180974 rank 0\n",
            "2022-05-23 19:08:47,595 DEBUG TRAIN Batch 16/2100 loss 9.493986 loss_att 8.612453 loss_ctc 11.550894 lr 0.00180900 rank 0\n",
            "2022-05-23 19:09:17,173 DEBUG TRAIN Batch 16/2200 loss 8.173109 loss_att 7.469710 loss_ctc 9.814374 lr 0.00180826 rank 0\n",
            "2022-05-23 19:09:47,065 DEBUG TRAIN Batch 16/2300 loss 9.985357 loss_att 9.656376 loss_ctc 10.752979 lr 0.00180752 rank 0\n",
            "2022-05-23 19:10:16,956 DEBUG TRAIN Batch 16/2400 loss 9.399899 loss_att 8.242455 loss_ctc 12.100604 lr 0.00180678 rank 0\n",
            "2022-05-23 19:10:48,195 DEBUG TRAIN Batch 16/2500 loss 7.768642 loss_att 6.717331 loss_ctc 10.221701 lr 0.00180604 rank 0\n",
            "2022-05-23 19:11:17,288 DEBUG TRAIN Batch 16/2600 loss 10.607618 loss_att 9.764542 loss_ctc 12.574799 lr 0.00180531 rank 0\n",
            "2022-05-23 19:11:47,045 DEBUG TRAIN Batch 16/2700 loss 7.745831 loss_att 6.969298 loss_ctc 9.557743 lr 0.00180457 rank 0\n",
            "2022-05-23 19:12:17,072 DEBUG TRAIN Batch 16/2800 loss 9.097827 loss_att 7.977798 loss_ctc 11.711227 lr 0.00180384 rank 0\n",
            "2022-05-23 19:12:46,930 DEBUG TRAIN Batch 16/2900 loss 10.608107 loss_att 10.134744 loss_ctc 11.712620 lr 0.00180310 rank 0\n",
            "2022-05-23 19:13:17,524 DEBUG TRAIN Batch 16/3000 loss 6.651962 loss_att 5.980226 loss_ctc 8.219347 lr 0.00180237 rank 0\n",
            "2022-05-23 19:13:46,766 DEBUG TRAIN Batch 16/3100 loss 6.332000 loss_att 5.882965 loss_ctc 7.379747 lr 0.00180164 rank 0\n",
            "2022-05-23 19:14:16,896 DEBUG TRAIN Batch 16/3200 loss 10.368916 loss_att 9.764515 loss_ctc 11.779184 lr 0.00180091 rank 0\n",
            "2022-05-23 19:14:46,943 DEBUG TRAIN Batch 16/3300 loss 7.390701 loss_att 7.146458 loss_ctc 7.960605 lr 0.00180018 rank 0\n",
            "2022-05-23 19:15:16,810 DEBUG TRAIN Batch 16/3400 loss 12.017570 loss_att 11.506580 loss_ctc 13.209876 lr 0.00179945 rank 0\n",
            "2022-05-23 19:15:47,461 DEBUG TRAIN Batch 16/3500 loss 11.873756 loss_att 10.005884 loss_ctc 16.232124 lr 0.00179872 rank 0\n",
            "2022-05-23 19:16:17,097 DEBUG TRAIN Batch 16/3600 loss 7.534970 loss_att 6.673922 loss_ctc 9.544084 lr 0.00179800 rank 0\n",
            "2022-05-23 19:16:46,711 DEBUG TRAIN Batch 16/3700 loss 8.622486 loss_att 7.490771 loss_ctc 11.263152 lr 0.00179727 rank 0\n",
            "2022-05-23 19:17:16,633 DEBUG TRAIN Batch 16/3800 loss 10.951177 loss_att 10.135386 loss_ctc 12.854689 lr 0.00179655 rank 0\n",
            "2022-05-23 19:17:47,126 DEBUG TRAIN Batch 16/3900 loss 8.792357 loss_att 8.347668 loss_ctc 9.829968 lr 0.00179582 rank 0\n",
            "2022-05-23 19:18:17,205 DEBUG TRAIN Batch 16/4000 loss 11.375113 loss_att 9.459999 loss_ctc 15.843708 lr 0.00179510 rank 0\n",
            "2022-05-23 19:18:46,585 DEBUG TRAIN Batch 16/4100 loss 8.411574 loss_att 7.367822 loss_ctc 10.846998 lr 0.00179438 rank 0\n",
            "2022-05-23 19:19:16,254 DEBUG TRAIN Batch 16/4200 loss 7.793376 loss_att 7.223883 loss_ctc 9.122192 lr 0.00179365 rank 0\n",
            "2022-05-23 19:19:45,625 DEBUG TRAIN Batch 16/4300 loss 9.290165 loss_att 8.542439 loss_ctc 11.034859 lr 0.00179293 rank 0\n",
            "2022-05-23 19:20:15,060 DEBUG TRAIN Batch 16/4400 loss 10.003468 loss_att 9.567793 loss_ctc 11.020041 lr 0.00179221 rank 0\n",
            "2022-05-23 19:20:45,283 DEBUG TRAIN Batch 16/4500 loss 8.284595 loss_att 7.370234 loss_ctc 10.418100 lr 0.00179149 rank 0\n",
            "2022-05-23 19:21:15,310 DEBUG TRAIN Batch 16/4600 loss 5.634070 loss_att 4.691174 loss_ctc 7.834163 lr 0.00179078 rank 0\n",
            "2022-05-23 19:21:44,918 DEBUG TRAIN Batch 16/4700 loss 5.057592 loss_att 4.797518 loss_ctc 5.664432 lr 0.00179006 rank 0\n",
            "2022-05-23 19:22:14,821 DEBUG TRAIN Batch 16/4800 loss 7.178802 loss_att 6.743787 loss_ctc 8.193837 lr 0.00178934 rank 0\n",
            "2022-05-23 19:22:44,715 DEBUG TRAIN Batch 16/4900 loss 16.910538 loss_att 15.378908 loss_ctc 20.484341 lr 0.00178863 rank 0\n",
            "2022-05-23 19:23:15,548 DEBUG TRAIN Batch 16/5000 loss 8.534447 loss_att 7.302535 loss_ctc 11.408908 lr 0.00178791 rank 0\n",
            "2022-05-23 19:23:44,376 DEBUG TRAIN Batch 16/5100 loss 7.743971 loss_att 6.755946 loss_ctc 10.049364 lr 0.00178720 rank 0\n",
            "2022-05-23 19:24:14,352 DEBUG TRAIN Batch 16/5200 loss 11.531578 loss_att 10.696305 loss_ctc 13.480548 lr 0.00178648 rank 0\n",
            "2022-05-23 19:24:44,324 DEBUG TRAIN Batch 16/5300 loss 9.266137 loss_att 8.428782 loss_ctc 11.219965 lr 0.00178577 rank 0\n",
            "2022-05-23 19:25:13,906 DEBUG TRAIN Batch 16/5400 loss 8.650587 loss_att 8.127131 loss_ctc 9.871986 lr 0.00178506 rank 0\n",
            "2022-05-23 19:25:44,629 DEBUG TRAIN Batch 16/5500 loss 10.393405 loss_att 8.938459 loss_ctc 13.788279 lr 0.00178435 rank 0\n",
            "2022-05-23 19:26:13,966 DEBUG TRAIN Batch 16/5600 loss 6.447823 loss_att 5.619909 loss_ctc 8.379622 lr 0.00178364 rank 0\n",
            "2022-05-23 19:26:43,511 DEBUG TRAIN Batch 16/5700 loss 8.386105 loss_att 7.337649 loss_ctc 10.832501 lr 0.00178293 rank 0\n",
            "2022-05-23 19:27:13,454 DEBUG TRAIN Batch 16/5800 loss 11.104360 loss_att 10.138096 loss_ctc 13.358975 lr 0.00178222 rank 0\n",
            "2022-05-23 19:27:43,454 DEBUG TRAIN Batch 16/5900 loss 6.169284 loss_att 6.131101 loss_ctc 6.258377 lr 0.00178152 rank 0\n",
            "2022-05-23 19:28:14,757 DEBUG TRAIN Batch 16/6000 loss 8.063309 loss_att 7.220903 loss_ctc 10.028923 lr 0.00178081 rank 0\n",
            "2022-05-23 19:28:44,394 DEBUG TRAIN Batch 16/6100 loss 7.900356 loss_att 6.968505 loss_ctc 10.074674 lr 0.00178010 rank 0\n",
            "2022-05-23 19:29:13,926 DEBUG TRAIN Batch 16/6200 loss 6.116931 loss_att 5.333861 loss_ctc 7.944094 lr 0.00177940 rank 0\n",
            "2022-05-23 19:29:43,547 DEBUG TRAIN Batch 16/6300 loss 10.576810 loss_att 9.333967 loss_ctc 13.476776 lr 0.00177870 rank 0\n",
            "2022-05-23 19:30:14,132 DEBUG TRAIN Batch 16/6400 loss 11.305269 loss_att 10.841902 loss_ctc 12.386459 lr 0.00177799 rank 0\n",
            "2022-05-23 19:30:45,121 DEBUG TRAIN Batch 16/6500 loss 7.901642 loss_att 6.979253 loss_ctc 10.053883 lr 0.00177729 rank 0\n",
            "2022-05-23 19:31:14,244 DEBUG TRAIN Batch 16/6600 loss 8.673022 loss_att 7.889044 loss_ctc 10.502304 lr 0.00177659 rank 0\n",
            "2022-05-23 19:31:44,177 DEBUG TRAIN Batch 16/6700 loss 6.830194 loss_att 6.429706 loss_ctc 7.764668 lr 0.00177589 rank 0\n",
            "2022-05-23 19:32:13,955 DEBUG TRAIN Batch 16/6800 loss 9.719487 loss_att 8.878174 loss_ctc 11.682552 lr 0.00177519 rank 0\n",
            "2022-05-23 19:32:43,864 DEBUG TRAIN Batch 16/6900 loss 8.394059 loss_att 8.044436 loss_ctc 9.209846 lr 0.00177449 rank 0\n",
            "2022-05-23 19:33:14,781 DEBUG TRAIN Batch 16/7000 loss 7.488709 loss_att 6.508162 loss_ctc 9.776651 lr 0.00177379 rank 0\n",
            "2022-05-23 19:33:44,618 DEBUG TRAIN Batch 16/7100 loss 7.029172 loss_att 6.456653 loss_ctc 8.365050 lr 0.00177309 rank 0\n",
            "2022-05-23 19:34:14,167 DEBUG TRAIN Batch 16/7200 loss 8.720881 loss_att 7.328016 loss_ctc 11.970898 lr 0.00177240 rank 0\n",
            "2022-05-23 19:34:44,146 DEBUG TRAIN Batch 16/7300 loss 12.753052 loss_att 11.501904 loss_ctc 15.672395 lr 0.00177170 rank 0\n",
            "2022-05-23 19:35:14,090 DEBUG TRAIN Batch 16/7400 loss 14.849242 loss_att 13.632830 loss_ctc 17.687540 lr 0.00177101 rank 0\n",
            "2022-05-23 19:35:44,620 DEBUG TRAIN Batch 16/7500 loss 9.947951 loss_att 9.004604 loss_ctc 12.149095 lr 0.00177031 rank 0\n",
            "2022-05-23 19:35:50,568 DEBUG CV Batch 16/0 loss 4.581563 loss_att 4.253510 loss_ctc 5.347020 history loss 4.312059 rank 0\n",
            "2022-05-23 19:36:02,036 DEBUG CV Batch 16/100 loss 3.163580 loss_att 3.201629 loss_ctc 3.074797 history loss 6.312090 rank 0\n",
            "2022-05-23 19:36:12,451 DEBUG CV Batch 16/200 loss 5.583906 loss_att 5.022467 loss_ctc 6.893929 history loss 6.315533 rank 0\n",
            "2022-05-23 19:36:23,410 DEBUG CV Batch 16/300 loss 4.867205 loss_att 4.808825 loss_ctc 5.003426 history loss 5.938568 rank 0\n",
            "2022-05-23 19:36:35,123 DEBUG CV Batch 16/400 loss 9.233565 loss_att 8.806091 loss_ctc 10.231004 history loss 5.475357 rank 0\n",
            "2022-05-23 19:36:47,309 DEBUG CV Batch 16/500 loss 3.187730 loss_att 3.245092 loss_ctc 3.053885 history loss 5.406995 rank 0\n",
            "2022-05-23 19:36:59,190 DEBUG CV Batch 16/600 loss 4.271795 loss_att 4.285787 loss_ctc 4.239148 history loss 5.368466 rank 0\n",
            "2022-05-23 19:37:10,328 DEBUG CV Batch 16/700 loss 4.028485 loss_att 3.985136 loss_ctc 4.129633 history loss 5.244642 rank 0\n",
            "2022-05-23 19:37:21,837 DEBUG CV Batch 16/800 loss 5.220136 loss_att 4.770542 loss_ctc 6.269189 history loss 5.219670 rank 0\n",
            "2022-05-23 19:37:33,400 INFO Epoch 16 CV info cv_loss 5.207282605120947\n",
            "2022-05-23 19:37:33,400 INFO Checkpoint: save to checkpoint exp/conformer/16.pt\n",
            "2022-05-23 19:37:33,729 INFO Epoch 17 TRAIN info lr 0.0017702858702558545\n",
            "2022-05-23 19:37:33,731 INFO using accumulate grad, new batch size is 4 times larger than before\n",
            "2022-05-23 19:37:59,333 DEBUG TRAIN Batch 17/0 loss 6.818290 loss_att 5.769130 loss_ctc 9.266331 lr 0.00177026 rank 0\n",
            "2022-05-23 19:38:29,622 DEBUG TRAIN Batch 17/100 loss 8.345793 loss_att 7.282360 loss_ctc 10.827139 lr 0.00176957 rank 0\n",
            "2022-05-23 19:38:59,505 DEBUG TRAIN Batch 17/200 loss 6.243103 loss_att 5.495062 loss_ctc 7.988532 lr 0.00176887 rank 0\n",
            "2022-05-23 19:39:28,980 DEBUG TRAIN Batch 17/300 loss 7.974699 loss_att 7.035508 loss_ctc 10.166146 lr 0.00176818 rank 0\n",
            "2022-05-23 19:39:58,872 DEBUG TRAIN Batch 17/400 loss 7.906349 loss_att 7.502726 loss_ctc 8.848134 lr 0.00176749 rank 0\n",
            "2022-05-23 19:40:29,733 DEBUG TRAIN Batch 17/500 loss 9.618338 loss_att 7.841761 loss_ctc 13.763683 lr 0.00176680 rank 0\n",
            "2022-05-23 19:40:59,207 DEBUG TRAIN Batch 17/600 loss 10.458002 loss_att 9.193233 loss_ctc 13.409130 lr 0.00176611 rank 0\n",
            "2022-05-23 19:41:29,161 DEBUG TRAIN Batch 17/700 loss 12.065989 loss_att 10.826137 loss_ctc 14.958979 lr 0.00176542 rank 0\n",
            "2022-05-23 19:41:59,121 DEBUG TRAIN Batch 17/800 loss 7.497437 loss_att 7.098329 loss_ctc 8.428692 lr 0.00176474 rank 0\n",
            "2022-05-23 19:42:29,507 DEBUG TRAIN Batch 17/900 loss 8.621495 loss_att 7.741878 loss_ctc 10.673935 lr 0.00176405 rank 0\n",
            "2022-05-23 19:43:00,484 DEBUG TRAIN Batch 17/1000 loss 8.734582 loss_att 7.446155 loss_ctc 11.740911 lr 0.00176336 rank 0\n",
            "2022-05-23 19:43:29,991 DEBUG TRAIN Batch 17/1100 loss 7.022274 loss_att 6.521834 loss_ctc 8.189966 lr 0.00176268 rank 0\n",
            "2022-05-23 19:43:59,558 DEBUG TRAIN Batch 17/1200 loss 8.756026 loss_att 7.709297 loss_ctc 11.198395 lr 0.00176199 rank 0\n",
            "2022-05-23 19:44:29,030 DEBUG TRAIN Batch 17/1300 loss 5.481905 loss_att 5.076864 loss_ctc 6.427001 lr 0.00176131 rank 0\n",
            "2022-05-23 19:44:59,147 DEBUG TRAIN Batch 17/1400 loss 11.278156 loss_att 10.565207 loss_ctc 12.941706 lr 0.00176063 rank 0\n",
            "2022-05-23 19:45:29,413 DEBUG TRAIN Batch 17/1500 loss 11.036469 loss_att 9.699790 loss_ctc 14.155383 lr 0.00175995 rank 0\n",
            "2022-05-23 19:45:58,850 DEBUG TRAIN Batch 17/1600 loss 10.374079 loss_att 9.037164 loss_ctc 13.493546 lr 0.00175927 rank 0\n",
            "2022-05-23 19:46:28,497 DEBUG TRAIN Batch 17/1700 loss 7.562971 loss_att 6.794042 loss_ctc 9.357140 lr 0.00175859 rank 0\n",
            "2022-05-23 19:46:58,210 DEBUG TRAIN Batch 17/1800 loss 9.370832 loss_att 8.875332 loss_ctc 10.527000 lr 0.00175791 rank 0\n",
            "2022-05-23 19:47:28,279 DEBUG TRAIN Batch 17/1900 loss 9.904303 loss_att 9.391833 loss_ctc 11.100063 lr 0.00175723 rank 0\n",
            "2022-05-23 19:47:59,102 DEBUG TRAIN Batch 17/2000 loss 7.947431 loss_att 6.824107 loss_ctc 10.568520 lr 0.00175655 rank 0\n",
            "2022-05-23 19:48:28,468 DEBUG TRAIN Batch 17/2100 loss 7.593450 loss_att 6.973644 loss_ctc 9.039663 lr 0.00175587 rank 0\n",
            "2022-05-23 19:48:58,210 DEBUG TRAIN Batch 17/2200 loss 7.310749 loss_att 6.581155 loss_ctc 9.013135 lr 0.00175520 rank 0\n",
            "2022-05-23 19:49:28,392 DEBUG TRAIN Batch 17/2300 loss 10.222824 loss_att 8.986595 loss_ctc 13.107358 lr 0.00175452 rank 0\n",
            "2022-05-23 19:49:58,428 DEBUG TRAIN Batch 17/2400 loss 9.716174 loss_att 9.038166 loss_ctc 11.298193 lr 0.00175385 rank 0\n",
            "2022-05-23 19:50:29,437 DEBUG TRAIN Batch 17/2500 loss 6.435663 loss_att 5.290025 loss_ctc 9.108818 lr 0.00175317 rank 0\n",
            "2022-05-23 19:50:58,996 DEBUG TRAIN Batch 17/2600 loss 6.915421 loss_att 6.079635 loss_ctc 8.865588 lr 0.00175250 rank 0\n",
            "2022-05-23 19:51:28,990 DEBUG TRAIN Batch 17/2700 loss 6.707488 loss_att 6.021646 loss_ctc 8.307787 lr 0.00175183 rank 0\n",
            "2022-05-23 19:51:59,410 DEBUG TRAIN Batch 17/2800 loss 7.612950 loss_att 7.211993 loss_ctc 8.548516 lr 0.00175116 rank 0\n",
            "2022-05-23 19:52:29,420 DEBUG TRAIN Batch 17/2900 loss 9.623690 loss_att 8.903165 loss_ctc 11.304913 lr 0.00175048 rank 0\n",
            "2022-05-23 19:53:00,889 DEBUG TRAIN Batch 17/3000 loss 6.995753 loss_att 5.768489 loss_ctc 9.859371 lr 0.00174981 rank 0\n",
            "2022-05-23 19:53:30,364 DEBUG TRAIN Batch 17/3100 loss 7.176431 loss_att 6.244686 loss_ctc 9.350505 lr 0.00174914 rank 0\n",
            "2022-05-23 19:54:00,116 DEBUG TRAIN Batch 17/3200 loss 8.900779 loss_att 7.744033 loss_ctc 11.599852 lr 0.00174848 rank 0\n",
            "2022-05-23 19:54:29,507 DEBUG TRAIN Batch 17/3300 loss 7.160159 loss_att 6.782922 loss_ctc 8.040379 lr 0.00174781 rank 0\n",
            "2022-05-23 19:54:59,592 DEBUG TRAIN Batch 17/3400 loss 9.266138 loss_att 8.618552 loss_ctc 10.777172 lr 0.00174714 rank 0\n",
            "2022-05-23 19:55:30,181 DEBUG TRAIN Batch 17/3500 loss 6.882769 loss_att 5.945002 loss_ctc 9.070891 lr 0.00174648 rank 0\n",
            "2022-05-23 19:55:59,601 DEBUG TRAIN Batch 17/3600 loss 6.031146 loss_att 5.533067 loss_ctc 7.193331 lr 0.00174581 rank 0\n",
            "2022-05-23 19:56:28,861 DEBUG TRAIN Batch 17/3700 loss 6.809372 loss_att 6.421293 loss_ctc 7.714890 lr 0.00174514 rank 0\n",
            "2022-05-23 19:56:58,486 DEBUG TRAIN Batch 17/3800 loss 8.237654 loss_att 7.977161 loss_ctc 8.845469 lr 0.00174448 rank 0\n",
            "2022-05-23 19:57:28,258 DEBUG TRAIN Batch 17/3900 loss 9.051521 loss_att 8.433449 loss_ctc 10.493691 lr 0.00174382 rank 0\n",
            "2022-05-23 19:57:58,848 DEBUG TRAIN Batch 17/4000 loss 8.227009 loss_att 7.091943 loss_ctc 10.875498 lr 0.00174316 rank 0\n",
            "2022-05-23 19:58:28,317 DEBUG TRAIN Batch 17/4100 loss 5.268136 loss_att 4.736837 loss_ctc 6.507833 lr 0.00174249 rank 0\n",
            "2022-05-23 19:58:57,963 DEBUG TRAIN Batch 17/4200 loss 8.321398 loss_att 7.691041 loss_ctc 9.792229 lr 0.00174183 rank 0\n",
            "2022-05-23 19:59:27,845 DEBUG TRAIN Batch 17/4300 loss 8.118374 loss_att 7.373333 loss_ctc 9.856799 lr 0.00174117 rank 0\n",
            "2022-05-23 19:59:57,479 DEBUG TRAIN Batch 17/4400 loss 6.890058 loss_att 6.404434 loss_ctc 8.023179 lr 0.00174051 rank 0\n",
            "2022-05-23 20:00:28,130 DEBUG TRAIN Batch 17/4500 loss 9.182973 loss_att 7.922248 loss_ctc 12.124665 lr 0.00173985 rank 0\n",
            "2022-05-23 20:00:57,545 DEBUG TRAIN Batch 17/4600 loss 7.965983 loss_att 7.139318 loss_ctc 9.894868 lr 0.00173920 rank 0\n",
            "2022-05-23 20:01:26,961 DEBUG TRAIN Batch 17/4700 loss 6.475871 loss_att 5.527801 loss_ctc 8.688034 lr 0.00173854 rank 0\n",
            "2022-05-23 20:01:57,108 DEBUG TRAIN Batch 17/4800 loss 7.086361 loss_att 6.574224 loss_ctc 8.281348 lr 0.00173788 rank 0\n",
            "2022-05-23 20:02:27,383 DEBUG TRAIN Batch 17/4900 loss 9.675510 loss_att 9.188596 loss_ctc 10.811644 lr 0.00173723 rank 0\n",
            "2022-05-23 20:02:58,414 DEBUG TRAIN Batch 17/5000 loss 8.032081 loss_att 7.065781 loss_ctc 10.286781 lr 0.00173657 rank 0\n",
            "2022-05-23 20:03:27,778 DEBUG TRAIN Batch 17/5100 loss 7.220621 loss_att 6.604403 loss_ctc 8.658463 lr 0.00173592 rank 0\n",
            "2022-05-23 20:03:57,621 DEBUG TRAIN Batch 17/5200 loss 4.669136 loss_att 4.197657 loss_ctc 5.769254 lr 0.00173526 rank 0\n",
            "2022-05-23 20:04:27,143 DEBUG TRAIN Batch 17/5300 loss 8.101376 loss_att 7.404019 loss_ctc 9.728540 lr 0.00173461 rank 0\n",
            "2022-05-23 20:04:56,976 DEBUG TRAIN Batch 17/5400 loss 9.479242 loss_att 8.910357 loss_ctc 10.806641 lr 0.00173396 rank 0\n",
            "2022-05-23 20:05:27,582 DEBUG TRAIN Batch 17/5500 loss 8.427573 loss_att 7.584560 loss_ctc 10.394605 lr 0.00173331 rank 0\n",
            "2022-05-23 20:05:57,261 DEBUG TRAIN Batch 17/5600 loss 6.466496 loss_att 5.818819 loss_ctc 7.977740 lr 0.00173266 rank 0\n",
            "2022-05-23 20:06:26,751 DEBUG TRAIN Batch 17/5700 loss 6.815513 loss_att 6.356633 loss_ctc 7.886230 lr 0.00173201 rank 0\n",
            "2022-05-23 20:06:56,829 DEBUG TRAIN Batch 17/5800 loss 8.292459 loss_att 8.265054 loss_ctc 8.356407 lr 0.00173136 rank 0\n",
            "2022-05-23 20:07:26,617 DEBUG TRAIN Batch 17/5900 loss 9.293806 loss_att 8.525623 loss_ctc 11.086232 lr 0.00173071 rank 0\n",
            "2022-05-23 20:07:57,571 DEBUG TRAIN Batch 17/6000 loss 6.750221 loss_att 5.932713 loss_ctc 8.657740 lr 0.00173006 rank 0\n",
            "2022-05-23 20:08:27,052 DEBUG TRAIN Batch 17/6100 loss 5.624609 loss_att 4.932565 loss_ctc 7.239378 lr 0.00172942 rank 0\n",
            "2022-05-23 20:08:56,400 DEBUG TRAIN Batch 17/6200 loss 5.528381 loss_att 5.440627 loss_ctc 5.733139 lr 0.00172877 rank 0\n",
            "2022-05-23 20:09:25,892 DEBUG TRAIN Batch 17/6300 loss 10.777445 loss_att 9.914843 loss_ctc 12.790182 lr 0.00172812 rank 0\n",
            "2022-05-23 20:09:55,699 DEBUG TRAIN Batch 17/6400 loss 6.408898 loss_att 6.165052 loss_ctc 6.977871 lr 0.00172748 rank 0\n",
            "2022-05-23 20:10:26,584 DEBUG TRAIN Batch 17/6500 loss 8.345428 loss_att 7.312170 loss_ctc 10.756364 lr 0.00172684 rank 0\n",
            "2022-05-23 20:10:56,432 DEBUG TRAIN Batch 17/6600 loss 8.439061 loss_att 7.835951 loss_ctc 9.846319 lr 0.00172619 rank 0\n",
            "2022-05-23 20:11:26,049 DEBUG TRAIN Batch 17/6700 loss 9.325407 loss_att 8.100573 loss_ctc 12.183353 lr 0.00172555 rank 0\n",
            "2022-05-23 20:11:55,811 DEBUG TRAIN Batch 17/6800 loss 8.778631 loss_att 7.857921 loss_ctc 10.926956 lr 0.00172491 rank 0\n",
            "2022-05-23 20:12:25,520 DEBUG TRAIN Batch 17/6900 loss 13.827435 loss_att 13.024196 loss_ctc 15.701660 lr 0.00172427 rank 0\n",
            "2022-05-23 20:12:56,746 DEBUG TRAIN Batch 17/7000 loss 7.150784 loss_att 6.339045 loss_ctc 9.044840 lr 0.00172363 rank 0\n",
            "2022-05-23 20:13:26,530 DEBUG TRAIN Batch 17/7100 loss 7.437627 loss_att 6.760212 loss_ctc 9.018261 lr 0.00172299 rank 0\n",
            "2022-05-23 20:13:56,750 DEBUG TRAIN Batch 17/7200 loss 6.663231 loss_att 6.177213 loss_ctc 7.797274 lr 0.00172235 rank 0\n",
            "2022-05-23 20:14:26,544 DEBUG TRAIN Batch 17/7300 loss 8.833631 loss_att 7.762347 loss_ctc 11.333292 lr 0.00172171 rank 0\n",
            "2022-05-23 20:14:56,403 DEBUG TRAIN Batch 17/7400 loss 6.871514 loss_att 6.332642 loss_ctc 8.128883 lr 0.00172107 rank 0\n",
            "2022-05-23 20:15:26,793 DEBUG TRAIN Batch 17/7500 loss 12.759212 loss_att 10.831759 loss_ctc 17.256603 lr 0.00172043 rank 0\n",
            "2022-05-23 20:15:32,771 DEBUG CV Batch 17/0 loss 5.049587 loss_att 4.626792 loss_ctc 6.036107 history loss 4.752552 rank 0\n",
            "2022-05-23 20:15:44,247 DEBUG CV Batch 17/100 loss 3.402713 loss_att 3.195803 loss_ctc 3.885503 history loss 6.148691 rank 0\n",
            "2022-05-23 20:15:54,638 DEBUG CV Batch 17/200 loss 5.239568 loss_att 4.664029 loss_ctc 6.582492 history loss 6.137276 rank 0\n",
            "2022-05-23 20:16:05,704 DEBUG CV Batch 17/300 loss 4.672217 loss_att 4.723488 loss_ctc 4.552584 history loss 5.753475 rank 0\n",
            "2022-05-23 20:16:17,320 DEBUG CV Batch 17/400 loss 9.503347 loss_att 9.036727 loss_ctc 10.592129 history loss 5.302229 rank 0\n",
            "2022-05-23 20:16:29,338 DEBUG CV Batch 17/500 loss 3.492766 loss_att 3.255493 loss_ctc 4.046402 history loss 5.245609 rank 0\n",
            "2022-05-23 20:16:41,096 DEBUG CV Batch 17/600 loss 4.353858 loss_att 4.230597 loss_ctc 4.641467 history loss 5.211404 rank 0\n",
            "2022-05-23 20:16:52,238 DEBUG CV Batch 17/700 loss 4.033165 loss_att 3.951241 loss_ctc 4.224319 history loss 5.096894 rank 0\n",
            "2022-05-23 20:17:03,468 DEBUG CV Batch 17/800 loss 5.051543 loss_att 4.696104 loss_ctc 5.880903 history loss 5.071732 rank 0\n",
            "2022-05-23 20:17:14,868 INFO Epoch 17 CV info cv_loss 5.079412818845322\n",
            "2022-05-23 20:17:14,868 INFO Checkpoint: save to checkpoint exp/conformer/17.pt\n",
            "2022-05-23 20:17:15,197 INFO Epoch 18 TRAIN info lr 0.0017204086249310395\n",
            "2022-05-23 20:17:15,198 INFO using accumulate grad, new batch size is 4 times larger than before\n",
            "2022-05-23 20:17:41,046 DEBUG TRAIN Batch 18/0 loss 3.490286 loss_att 3.116154 loss_ctc 4.363259 lr 0.00172038 rank 0\n",
            "2022-05-23 20:18:11,254 DEBUG TRAIN Batch 18/100 loss 4.567587 loss_att 3.941369 loss_ctc 6.028763 lr 0.00171975 rank 0\n",
            "2022-05-23 20:18:41,380 DEBUG TRAIN Batch 18/200 loss 5.832751 loss_att 5.242320 loss_ctc 7.210422 lr 0.00171911 rank 0\n",
            "2022-05-23 20:19:10,457 DEBUG TRAIN Batch 18/300 loss 10.044111 loss_att 9.160520 loss_ctc 12.105825 lr 0.00171848 rank 0\n",
            "2022-05-23 20:19:40,533 DEBUG TRAIN Batch 18/400 loss 8.882255 loss_att 8.465600 loss_ctc 9.854448 lr 0.00171784 rank 0\n",
            "2022-05-23 20:20:11,657 DEBUG TRAIN Batch 18/500 loss 7.780593 loss_att 6.712418 loss_ctc 10.273002 lr 0.00171721 rank 0\n",
            "2022-05-23 20:20:41,201 DEBUG TRAIN Batch 18/600 loss 6.108560 loss_att 5.428043 loss_ctc 7.696430 lr 0.00171658 rank 0\n",
            "2022-05-23 20:21:10,966 DEBUG TRAIN Batch 18/700 loss 5.714211 loss_att 5.273472 loss_ctc 6.742601 lr 0.00171595 rank 0\n",
            "2022-05-23 20:21:40,768 DEBUG TRAIN Batch 18/800 loss 5.587038 loss_att 5.395188 loss_ctc 6.034685 lr 0.00171531 rank 0\n",
            "2022-05-23 20:22:10,604 DEBUG TRAIN Batch 18/900 loss 10.083037 loss_att 9.324028 loss_ctc 11.854057 lr 0.00171468 rank 0\n",
            "2022-05-23 20:22:42,223 DEBUG TRAIN Batch 18/1000 loss 5.866841 loss_att 5.494055 loss_ctc 6.736676 lr 0.00171405 rank 0\n",
            "2022-05-23 20:23:11,959 DEBUG TRAIN Batch 18/1100 loss 5.305827 loss_att 4.709306 loss_ctc 6.697708 lr 0.00171342 rank 0\n",
            "2022-05-23 20:23:41,182 DEBUG TRAIN Batch 18/1200 loss 7.500251 loss_att 6.868895 loss_ctc 8.973416 lr 0.00171280 rank 0\n",
            "2022-05-23 20:24:10,955 DEBUG TRAIN Batch 18/1300 loss 7.395201 loss_att 6.773499 loss_ctc 8.845837 lr 0.00171217 rank 0\n",
            "2022-05-23 20:24:41,143 DEBUG TRAIN Batch 18/1400 loss 8.825586 loss_att 8.405922 loss_ctc 9.804804 lr 0.00171154 rank 0\n",
            "2022-05-23 20:25:11,819 DEBUG TRAIN Batch 18/1500 loss 5.717885 loss_att 4.648265 loss_ctc 8.213665 lr 0.00171091 rank 0\n",
            "2022-05-23 20:25:41,273 DEBUG TRAIN Batch 18/1600 loss 4.830841 loss_att 4.247414 loss_ctc 6.192169 lr 0.00171029 rank 0\n",
            "2022-05-23 20:26:10,965 DEBUG TRAIN Batch 18/1700 loss 5.448222 loss_att 5.322684 loss_ctc 5.741142 lr 0.00170966 rank 0\n",
            "2022-05-23 20:26:40,735 DEBUG TRAIN Batch 18/1800 loss 6.789928 loss_att 6.544818 loss_ctc 7.361852 lr 0.00170904 rank 0\n",
            "2022-05-23 20:27:10,847 DEBUG TRAIN Batch 18/1900 loss 10.948923 loss_att 9.977945 loss_ctc 13.214539 lr 0.00170842 rank 0\n",
            "2022-05-23 20:27:41,488 DEBUG TRAIN Batch 18/2000 loss 8.097277 loss_att 7.291928 loss_ctc 9.976422 lr 0.00170779 rank 0\n",
            "2022-05-23 20:28:11,077 DEBUG TRAIN Batch 18/2100 loss 10.093400 loss_att 8.958393 loss_ctc 12.741751 lr 0.00170717 rank 0\n",
            "2022-05-23 20:28:40,388 DEBUG TRAIN Batch 18/2200 loss 5.736900 loss_att 5.317094 loss_ctc 6.716450 lr 0.00170655 rank 0\n",
            "2022-05-23 20:29:10,073 DEBUG TRAIN Batch 18/2300 loss 8.814323 loss_att 8.032268 loss_ctc 10.639122 lr 0.00170593 rank 0\n",
            "2022-05-23 20:29:39,698 DEBUG TRAIN Batch 18/2400 loss 7.294808 loss_att 6.790112 loss_ctc 8.472435 lr 0.00170531 rank 0\n",
            "2022-05-23 20:30:10,491 DEBUG TRAIN Batch 18/2500 loss 7.322785 loss_att 6.344066 loss_ctc 9.606464 lr 0.00170469 rank 0\n",
            "2022-05-23 20:30:40,052 DEBUG TRAIN Batch 18/2600 loss 7.052345 loss_att 5.778333 loss_ctc 10.025039 lr 0.00170407 rank 0\n",
            "2022-05-23 20:31:10,072 DEBUG TRAIN Batch 18/2700 loss 10.300558 loss_att 9.441447 loss_ctc 12.305150 lr 0.00170345 rank 0\n",
            "2022-05-23 20:31:39,636 DEBUG TRAIN Batch 18/2800 loss 10.996102 loss_att 9.941000 loss_ctc 13.458008 lr 0.00170283 rank 0\n",
            "2022-05-23 20:32:09,657 DEBUG TRAIN Batch 18/2900 loss 7.708161 loss_att 6.532957 loss_ctc 10.450306 lr 0.00170222 rank 0\n",
            "2022-05-23 20:32:40,646 DEBUG TRAIN Batch 18/3000 loss 8.655918 loss_att 8.190756 loss_ctc 9.741295 lr 0.00170160 rank 0\n",
            "2022-05-23 20:33:10,108 DEBUG TRAIN Batch 18/3100 loss 7.334300 loss_att 6.607183 loss_ctc 9.030907 lr 0.00170099 rank 0\n",
            "2022-05-23 20:33:39,746 DEBUG TRAIN Batch 18/3200 loss 8.496262 loss_att 7.281017 loss_ctc 11.331831 lr 0.00170037 rank 0\n",
            "2022-05-23 20:34:09,200 DEBUG TRAIN Batch 18/3300 loss 9.290374 loss_att 8.691530 loss_ctc 10.687675 lr 0.00169976 rank 0\n",
            "2022-05-23 20:34:38,941 DEBUG TRAIN Batch 18/3400 loss 8.826852 loss_att 8.251314 loss_ctc 10.169773 lr 0.00169914 rank 0\n",
            "2022-05-23 20:35:09,879 DEBUG TRAIN Batch 18/3500 loss 10.967825 loss_att 9.208319 loss_ctc 15.073339 lr 0.00169853 rank 0\n",
            "2022-05-23 20:35:38,898 DEBUG TRAIN Batch 18/3600 loss 9.771838 loss_att 8.595808 loss_ctc 12.515909 lr 0.00169792 rank 0\n",
            "2022-05-23 20:36:08,738 DEBUG TRAIN Batch 18/3700 loss 4.755108 loss_att 4.458472 loss_ctc 5.447261 lr 0.00169731 rank 0\n",
            "2022-05-23 20:36:38,290 DEBUG TRAIN Batch 18/3800 loss 8.219733 loss_att 7.496032 loss_ctc 9.908370 lr 0.00169670 rank 0\n",
            "2022-05-23 20:37:08,129 DEBUG TRAIN Batch 18/3900 loss 11.261503 loss_att 10.631678 loss_ctc 12.731096 lr 0.00169609 rank 0\n",
            "2022-05-23 20:37:39,001 DEBUG TRAIN Batch 18/4000 loss 6.897799 loss_att 5.940778 loss_ctc 9.130849 lr 0.00169548 rank 0\n",
            "2022-05-23 20:38:08,700 DEBUG TRAIN Batch 18/4100 loss 6.451768 loss_att 5.980830 loss_ctc 7.550622 lr 0.00169487 rank 0\n",
            "2022-05-23 20:38:38,315 DEBUG TRAIN Batch 18/4200 loss 5.557741 loss_att 5.416286 loss_ctc 5.887804 lr 0.00169426 rank 0\n",
            "2022-05-23 20:39:08,281 DEBUG TRAIN Batch 18/4300 loss 7.861936 loss_att 7.280063 loss_ctc 9.219641 lr 0.00169365 rank 0\n",
            "2022-05-23 20:39:38,221 DEBUG TRAIN Batch 18/4400 loss 8.350924 loss_att 7.829928 loss_ctc 9.566578 lr 0.00169304 rank 0\n",
            "2022-05-23 20:40:09,485 DEBUG TRAIN Batch 18/4500 loss 7.528339 loss_att 6.960450 loss_ctc 8.853413 lr 0.00169244 rank 0\n",
            "2022-05-23 20:40:39,114 DEBUG TRAIN Batch 18/4600 loss 7.692734 loss_att 6.433239 loss_ctc 10.631554 lr 0.00169183 rank 0\n",
            "2022-05-23 20:41:09,025 DEBUG TRAIN Batch 18/4700 loss 10.616766 loss_att 9.952729 loss_ctc 12.166185 lr 0.00169123 rank 0\n",
            "2022-05-23 20:41:38,825 DEBUG TRAIN Batch 18/4800 loss 7.284719 loss_att 6.876460 loss_ctc 8.237321 lr 0.00169062 rank 0\n",
            "2022-05-23 20:42:09,167 DEBUG TRAIN Batch 18/4900 loss 6.578938 loss_att 6.169285 loss_ctc 7.534792 lr 0.00169002 rank 0\n",
            "2022-05-23 20:42:39,973 DEBUG TRAIN Batch 18/5000 loss 11.025612 loss_att 9.701374 loss_ctc 14.115499 lr 0.00168942 rank 0\n",
            "2022-05-23 20:43:09,884 DEBUG TRAIN Batch 18/5100 loss 6.494785 loss_att 5.815494 loss_ctc 8.079799 lr 0.00168881 rank 0\n",
            "2022-05-23 20:43:39,593 DEBUG TRAIN Batch 18/5200 loss 6.126422 loss_att 6.139938 loss_ctc 6.094884 lr 0.00168821 rank 0\n",
            "2022-05-23 20:44:09,136 DEBUG TRAIN Batch 18/5300 loss 7.964118 loss_att 7.734855 loss_ctc 8.499065 lr 0.00168761 rank 0\n",
            "2022-05-23 20:44:38,870 DEBUG TRAIN Batch 18/5400 loss 8.874839 loss_att 8.317092 loss_ctc 10.176249 lr 0.00168701 rank 0\n",
            "2022-05-23 20:45:10,088 DEBUG TRAIN Batch 18/5500 loss 6.804728 loss_att 5.549250 loss_ctc 9.734178 lr 0.00168641 rank 0\n",
            "2022-05-23 20:45:39,344 DEBUG TRAIN Batch 18/5600 loss 4.602112 loss_att 4.223877 loss_ctc 5.484660 lr 0.00168581 rank 0\n",
            "2022-05-23 20:46:08,783 DEBUG TRAIN Batch 18/5700 loss 6.260505 loss_att 5.415840 loss_ctc 8.231388 lr 0.00168521 rank 0\n",
            "2022-05-23 20:46:38,647 DEBUG TRAIN Batch 18/5800 loss 9.207263 loss_att 8.551507 loss_ctc 10.737360 lr 0.00168461 rank 0\n",
            "2022-05-23 20:47:08,918 DEBUG TRAIN Batch 18/5900 loss 6.397981 loss_att 5.943100 loss_ctc 7.459369 lr 0.00168402 rank 0\n",
            "2022-05-23 20:47:39,693 DEBUG TRAIN Batch 18/6000 loss 8.872477 loss_att 7.384233 loss_ctc 12.345047 lr 0.00168342 rank 0\n",
            "2022-05-23 20:48:09,566 DEBUG TRAIN Batch 18/6100 loss 7.851798 loss_att 7.387469 loss_ctc 8.935232 lr 0.00168282 rank 0\n",
            "2022-05-23 20:48:38,869 DEBUG TRAIN Batch 18/6200 loss 7.469246 loss_att 6.998143 loss_ctc 8.568484 lr 0.00168223 rank 0\n",
            "2022-05-23 20:49:08,388 DEBUG TRAIN Batch 18/6300 loss 6.321104 loss_att 5.689260 loss_ctc 7.795406 lr 0.00168163 rank 0\n",
            "2022-05-23 20:49:38,331 DEBUG TRAIN Batch 18/6400 loss 10.613085 loss_att 9.510878 loss_ctc 13.184902 lr 0.00168104 rank 0\n",
            "2022-05-23 20:50:09,019 DEBUG TRAIN Batch 18/6500 loss 7.675624 loss_att 6.503230 loss_ctc 10.411209 lr 0.00168045 rank 0\n",
            "2022-05-23 20:50:38,668 DEBUG TRAIN Batch 18/6600 loss 9.013704 loss_att 7.675771 loss_ctc 12.135549 lr 0.00167985 rank 0\n",
            "2022-05-23 20:51:08,056 DEBUG TRAIN Batch 18/6700 loss 7.468231 loss_att 6.924871 loss_ctc 8.736071 lr 0.00167926 rank 0\n",
            "2022-05-23 20:51:37,933 DEBUG TRAIN Batch 18/6800 loss 10.149302 loss_att 9.041512 loss_ctc 12.734142 lr 0.00167867 rank 0\n",
            "2022-05-23 20:52:08,110 DEBUG TRAIN Batch 18/6900 loss 6.251024 loss_att 5.852740 loss_ctc 7.180352 lr 0.00167808 rank 0\n",
            "2022-05-23 20:52:39,145 DEBUG TRAIN Batch 18/7000 loss 8.420939 loss_att 7.657632 loss_ctc 10.201990 lr 0.00167749 rank 0\n",
            "2022-05-23 20:53:08,485 DEBUG TRAIN Batch 18/7100 loss 7.327627 loss_att 6.095164 loss_ctc 10.203373 lr 0.00167690 rank 0\n",
            "2022-05-23 20:53:38,060 DEBUG TRAIN Batch 18/7200 loss 7.513227 loss_att 6.530128 loss_ctc 9.807122 lr 0.00167631 rank 0\n",
            "2022-05-23 20:54:07,512 DEBUG TRAIN Batch 18/7300 loss 6.342319 loss_att 6.181717 loss_ctc 6.717056 lr 0.00167572 rank 0\n",
            "2022-05-23 20:54:37,536 DEBUG TRAIN Batch 18/7400 loss 7.627010 loss_att 7.242139 loss_ctc 8.525043 lr 0.00167513 rank 0\n",
            "2022-05-23 20:55:08,131 DEBUG TRAIN Batch 18/7500 loss 6.394403 loss_att 5.807329 loss_ctc 7.764241 lr 0.00167455 rank 0\n",
            "2022-05-23 20:55:14,571 DEBUG CV Batch 18/0 loss 5.091407 loss_att 4.584764 loss_ctc 6.273573 history loss 4.791913 rank 0\n",
            "2022-05-23 20:55:26,002 DEBUG CV Batch 18/100 loss 3.182622 loss_att 2.911862 loss_ctc 3.814394 history loss 6.094969 rank 0\n",
            "2022-05-23 20:55:36,519 DEBUG CV Batch 18/200 loss 5.479064 loss_att 5.155665 loss_ctc 6.233662 history loss 5.992457 rank 0\n",
            "2022-05-23 20:55:47,512 DEBUG CV Batch 18/300 loss 4.277451 loss_att 4.285387 loss_ctc 4.258934 history loss 5.625001 rank 0\n",
            "2022-05-23 20:55:59,260 DEBUG CV Batch 18/400 loss 9.186431 loss_att 9.191462 loss_ctc 9.174693 history loss 5.181154 rank 0\n",
            "2022-05-23 20:56:11,285 DEBUG CV Batch 18/500 loss 3.047142 loss_att 3.158301 loss_ctc 2.787771 history loss 5.106068 rank 0\n",
            "2022-05-23 20:56:23,005 DEBUG CV Batch 18/600 loss 4.566672 loss_att 4.564788 loss_ctc 4.571069 history loss 5.072986 rank 0\n",
            "2022-05-23 20:56:34,043 DEBUG CV Batch 18/700 loss 3.987978 loss_att 3.622607 loss_ctc 4.840509 history loss 4.962723 rank 0\n",
            "2022-05-23 20:56:45,534 DEBUG CV Batch 18/800 loss 4.631281 loss_att 4.207798 loss_ctc 5.619408 history loss 4.924915 rank 0\n",
            "2022-05-23 20:56:57,030 INFO Epoch 18 CV info cv_loss 4.93050370416397\n",
            "2022-05-23 20:56:57,030 INFO Checkpoint: save to checkpoint exp/conformer/18.pt\n",
            "2022-05-23 20:56:57,395 INFO Epoch 19 TRAIN info lr 0.0016745227923716057\n",
            "2022-05-23 20:56:57,397 INFO using accumulate grad, new batch size is 4 times larger than before\n",
            "2022-05-23 20:57:22,494 DEBUG TRAIN Batch 19/0 loss 5.357040 loss_att 4.870261 loss_ctc 6.492860 lr 0.00167450 rank 0\n",
            "2022-05-23 20:57:52,308 DEBUG TRAIN Batch 19/100 loss 3.636986 loss_att 3.405816 loss_ctc 4.176382 lr 0.00167391 rank 0\n",
            "2022-05-23 20:58:21,931 DEBUG TRAIN Batch 19/200 loss 5.964654 loss_att 5.777606 loss_ctc 6.401100 lr 0.00167333 rank 0\n",
            "2022-05-23 20:58:51,430 DEBUG TRAIN Batch 19/300 loss 6.711569 loss_att 6.403847 loss_ctc 7.429587 lr 0.00167274 rank 0\n",
            "2022-05-23 20:59:21,537 DEBUG TRAIN Batch 19/400 loss 8.705089 loss_att 8.171759 loss_ctc 9.949526 lr 0.00167216 rank 0\n",
            "2022-05-23 20:59:52,250 DEBUG TRAIN Batch 19/500 loss 7.987574 loss_att 6.773867 loss_ctc 10.819555 lr 0.00167157 rank 0\n",
            "2022-05-23 21:00:21,742 DEBUG TRAIN Batch 19/600 loss 5.083531 loss_att 4.874914 loss_ctc 5.570303 lr 0.00167099 rank 0\n",
            "2022-05-23 21:00:51,357 DEBUG TRAIN Batch 19/700 loss 8.627399 loss_att 8.324982 loss_ctc 9.333042 lr 0.00167041 rank 0\n",
            "2022-05-23 21:01:21,072 DEBUG TRAIN Batch 19/800 loss 5.896184 loss_att 5.294186 loss_ctc 7.300847 lr 0.00166982 rank 0\n",
            "2022-05-23 21:01:50,869 DEBUG TRAIN Batch 19/900 loss 8.594124 loss_att 8.137256 loss_ctc 9.660151 lr 0.00166924 rank 0\n",
            "2022-05-23 21:02:21,427 DEBUG TRAIN Batch 19/1000 loss 6.404000 loss_att 5.477880 loss_ctc 8.564945 lr 0.00166866 rank 0\n",
            "2022-05-23 21:02:50,878 DEBUG TRAIN Batch 19/1100 loss 5.292871 loss_att 4.671330 loss_ctc 6.743133 lr 0.00166808 rank 0\n",
            "2022-05-23 21:03:20,550 DEBUG TRAIN Batch 19/1200 loss 6.750894 loss_att 5.788727 loss_ctc 8.995949 lr 0.00166750 rank 0\n",
            "2022-05-23 21:03:50,629 DEBUG TRAIN Batch 19/1300 loss 8.553226 loss_att 7.939414 loss_ctc 9.985456 lr 0.00166692 rank 0\n",
            "2022-05-23 21:04:20,621 DEBUG TRAIN Batch 19/1400 loss 10.243126 loss_att 9.643293 loss_ctc 11.642735 lr 0.00166634 rank 0\n",
            "2022-05-23 21:04:51,014 DEBUG TRAIN Batch 19/1500 loss 9.084205 loss_att 8.441119 loss_ctc 10.584738 lr 0.00166576 rank 0\n",
            "2022-05-23 21:05:20,386 DEBUG TRAIN Batch 19/1600 loss 6.373170 loss_att 5.851724 loss_ctc 7.589879 lr 0.00166519 rank 0\n",
            "2022-05-23 21:05:50,264 DEBUG TRAIN Batch 19/1700 loss 7.018435 loss_att 6.552557 loss_ctc 8.105485 lr 0.00166461 rank 0\n",
            "2022-05-23 21:06:20,315 DEBUG TRAIN Batch 19/1800 loss 6.502677 loss_att 5.905929 loss_ctc 7.895090 lr 0.00166403 rank 0\n",
            "2022-05-23 21:06:50,235 DEBUG TRAIN Batch 19/1900 loss 9.435154 loss_att 8.467351 loss_ctc 11.693361 lr 0.00166346 rank 0\n",
            "2022-05-23 21:07:20,753 DEBUG TRAIN Batch 19/2000 loss 5.785772 loss_att 5.146015 loss_ctc 7.278540 lr 0.00166288 rank 0\n",
            "2022-05-23 21:07:50,086 DEBUG TRAIN Batch 19/2100 loss 4.791288 loss_att 4.433548 loss_ctc 5.626016 lr 0.00166231 rank 0\n",
            "2022-05-23 21:08:19,938 DEBUG TRAIN Batch 19/2200 loss 7.915614 loss_att 7.049980 loss_ctc 9.935425 lr 0.00166173 rank 0\n",
            "2022-05-23 21:08:49,921 DEBUG TRAIN Batch 19/2300 loss 6.365581 loss_att 6.280528 loss_ctc 6.564036 lr 0.00166116 rank 0\n",
            "2022-05-23 21:09:19,833 DEBUG TRAIN Batch 19/2400 loss 11.404860 loss_att 10.410742 loss_ctc 13.724471 lr 0.00166059 rank 0\n",
            "2022-05-23 21:09:50,801 DEBUG TRAIN Batch 19/2500 loss 5.980949 loss_att 5.553039 loss_ctc 6.979408 lr 0.00166002 rank 0\n",
            "2022-05-23 21:10:19,914 DEBUG TRAIN Batch 19/2600 loss 6.205836 loss_att 5.530748 loss_ctc 7.781039 lr 0.00165945 rank 0\n",
            "2022-05-23 21:10:49,790 DEBUG TRAIN Batch 19/2700 loss 5.277062 loss_att 4.957827 loss_ctc 6.021946 lr 0.00165887 rank 0\n",
            "2022-05-23 21:11:19,630 DEBUG TRAIN Batch 19/2800 loss 7.374359 loss_att 6.818863 loss_ctc 8.670516 lr 0.00165830 rank 0\n",
            "2022-05-23 21:11:49,753 DEBUG TRAIN Batch 19/2900 loss 9.464849 loss_att 8.699404 loss_ctc 11.250891 lr 0.00165773 rank 0\n",
            "2022-05-23 21:12:20,433 DEBUG TRAIN Batch 19/3000 loss 7.178419 loss_att 5.986169 loss_ctc 9.960337 lr 0.00165717 rank 0\n",
            "2022-05-23 21:12:49,811 DEBUG TRAIN Batch 19/3100 loss 7.391464 loss_att 6.723190 loss_ctc 8.950771 lr 0.00165660 rank 0\n",
            "2022-05-23 21:13:19,386 DEBUG TRAIN Batch 19/3200 loss 7.887512 loss_att 7.129963 loss_ctc 9.655126 lr 0.00165603 rank 0\n",
            "2022-05-23 21:13:48,932 DEBUG TRAIN Batch 19/3300 loss 6.993686 loss_att 6.326632 loss_ctc 8.550142 lr 0.00165546 rank 0\n",
            "2022-05-23 21:14:19,063 DEBUG TRAIN Batch 19/3400 loss 9.080015 loss_att 8.538008 loss_ctc 10.344700 lr 0.00165489 rank 0\n",
            "2022-05-23 21:14:49,821 DEBUG TRAIN Batch 19/3500 loss 6.031985 loss_att 5.338051 loss_ctc 7.651167 lr 0.00165433 rank 0\n",
            "2022-05-23 21:15:18,994 DEBUG TRAIN Batch 19/3600 loss 5.947092 loss_att 5.238867 loss_ctc 7.599617 lr 0.00165376 rank 0\n",
            "2022-05-23 21:15:48,445 DEBUG TRAIN Batch 19/3700 loss 7.114012 loss_att 6.309956 loss_ctc 8.990141 lr 0.00165320 rank 0\n",
            "2022-05-23 21:16:18,168 DEBUG TRAIN Batch 19/3800 loss 6.317685 loss_att 5.481838 loss_ctc 8.267994 lr 0.00165263 rank 0\n",
            "2022-05-23 21:16:48,446 DEBUG TRAIN Batch 19/3900 loss 9.469538 loss_att 8.565830 loss_ctc 11.578188 lr 0.00165207 rank 0\n",
            "2022-05-23 21:17:19,429 DEBUG TRAIN Batch 19/4000 loss 10.664217 loss_att 9.108374 loss_ctc 14.294518 lr 0.00165151 rank 0\n",
            "2022-05-23 21:17:49,558 DEBUG TRAIN Batch 19/4100 loss 5.324342 loss_att 4.784225 loss_ctc 6.584615 lr 0.00165094 rank 0\n",
            "2022-05-23 21:18:19,066 DEBUG TRAIN Batch 19/4200 loss 5.455201 loss_att 4.636737 loss_ctc 7.364949 lr 0.00165038 rank 0\n",
            "2022-05-23 21:18:48,646 DEBUG TRAIN Batch 19/4300 loss 7.551557 loss_att 6.790483 loss_ctc 9.327397 lr 0.00164982 rank 0\n",
            "2022-05-23 21:19:18,343 DEBUG TRAIN Batch 19/4400 loss 10.933626 loss_att 9.887123 loss_ctc 13.375468 lr 0.00164926 rank 0\n",
            "2022-05-23 21:19:48,877 DEBUG TRAIN Batch 19/4500 loss 7.490867 loss_att 6.226628 loss_ctc 10.440757 lr 0.00164870 rank 0\n",
            "2022-05-23 21:20:18,666 DEBUG TRAIN Batch 19/4600 loss 8.014103 loss_att 7.483585 loss_ctc 9.251979 lr 0.00164814 rank 0\n",
            "2022-05-23 21:20:48,492 DEBUG TRAIN Batch 19/4700 loss 5.838957 loss_att 5.414029 loss_ctc 6.830453 lr 0.00164758 rank 0\n",
            "2022-05-23 21:21:18,450 DEBUG TRAIN Batch 19/4800 loss 7.673368 loss_att 7.522974 loss_ctc 8.024288 lr 0.00164702 rank 0\n",
            "2022-05-23 21:21:48,268 DEBUG TRAIN Batch 19/4900 loss 11.333691 loss_att 10.685347 loss_ctc 12.846495 lr 0.00164646 rank 0\n",
            "2022-05-23 21:22:19,103 DEBUG TRAIN Batch 19/5000 loss 8.329782 loss_att 7.330003 loss_ctc 10.662600 lr 0.00164590 rank 0\n",
            "2022-05-23 21:22:48,699 DEBUG TRAIN Batch 19/5100 loss 7.525872 loss_att 6.851483 loss_ctc 9.099446 lr 0.00164535 rank 0\n",
            "2022-05-23 21:23:18,331 DEBUG TRAIN Batch 19/5200 loss 6.307478 loss_att 5.680851 loss_ctc 7.769609 lr 0.00164479 rank 0\n",
            "2022-05-23 21:23:48,124 DEBUG TRAIN Batch 19/5300 loss 7.539931 loss_att 6.920451 loss_ctc 8.985386 lr 0.00164423 rank 0\n",
            "2022-05-23 21:24:18,374 DEBUG TRAIN Batch 19/5400 loss 12.846066 loss_att 11.640271 loss_ctc 15.659585 lr 0.00164368 rank 0\n",
            "2022-05-23 21:24:49,768 DEBUG TRAIN Batch 19/5500 loss 11.550652 loss_att 9.742368 loss_ctc 15.769979 lr 0.00164312 rank 0\n",
            "2022-05-23 21:25:18,975 DEBUG TRAIN Batch 19/5600 loss 7.606155 loss_att 6.665415 loss_ctc 9.801216 lr 0.00164257 rank 0\n",
            "2022-05-23 21:25:48,417 DEBUG TRAIN Batch 19/5700 loss 9.566876 loss_att 8.552294 loss_ctc 11.934237 lr 0.00164202 rank 0\n",
            "2022-05-23 21:26:18,514 DEBUG TRAIN Batch 19/5800 loss 7.768151 loss_att 7.096266 loss_ctc 9.335885 lr 0.00164146 rank 0\n",
            "2022-05-23 21:26:47,928 DEBUG TRAIN Batch 19/5900 loss 9.073606 loss_att 8.561228 loss_ctc 10.269154 lr 0.00164091 rank 0\n",
            "2022-05-23 21:27:19,078 DEBUG TRAIN Batch 19/6000 loss 7.525105 loss_att 6.487706 loss_ctc 9.945703 lr 0.00164036 rank 0\n",
            "2022-05-23 21:27:48,547 DEBUG TRAIN Batch 19/6100 loss 5.212895 loss_att 4.852098 loss_ctc 6.054758 lr 0.00163981 rank 0\n",
            "2022-05-23 21:28:18,055 DEBUG TRAIN Batch 19/6200 loss 6.928773 loss_att 6.387542 loss_ctc 8.191648 lr 0.00163926 rank 0\n",
            "2022-05-23 21:28:48,234 DEBUG TRAIN Batch 19/6300 loss 7.461230 loss_att 7.255015 loss_ctc 7.942397 lr 0.00163871 rank 0\n",
            "2022-05-23 21:29:18,468 DEBUG TRAIN Batch 19/6400 loss 11.582069 loss_att 10.271230 loss_ctc 14.640697 lr 0.00163816 rank 0\n",
            "2022-05-23 21:29:49,288 DEBUG TRAIN Batch 19/6500 loss 7.270705 loss_att 6.557838 loss_ctc 8.934060 lr 0.00163761 rank 0\n",
            "2022-05-23 21:30:18,675 DEBUG TRAIN Batch 19/6600 loss 5.114436 loss_att 4.771048 loss_ctc 5.915675 lr 0.00163706 rank 0\n",
            "2022-05-23 21:30:47,689 DEBUG TRAIN Batch 19/6700 loss 6.477541 loss_att 5.736879 loss_ctc 8.205752 lr 0.00163651 rank 0\n",
            "2022-05-23 21:31:17,567 DEBUG TRAIN Batch 19/6800 loss 7.367432 loss_att 7.361609 loss_ctc 7.381018 lr 0.00163596 rank 0\n",
            "2022-05-23 21:31:47,456 DEBUG TRAIN Batch 19/6900 loss 8.690923 loss_att 7.875416 loss_ctc 10.593771 lr 0.00163542 rank 0\n",
            "2022-05-23 21:32:18,464 DEBUG TRAIN Batch 19/7000 loss 9.608730 loss_att 8.569094 loss_ctc 12.034549 lr 0.00163487 rank 0\n",
            "2022-05-23 21:32:47,582 DEBUG TRAIN Batch 19/7100 loss 7.133877 loss_att 6.434160 loss_ctc 8.766551 lr 0.00163432 rank 0\n",
            "2022-05-23 21:33:17,179 DEBUG TRAIN Batch 19/7200 loss 10.260938 loss_att 8.970976 loss_ctc 13.270847 lr 0.00163378 rank 0\n",
            "2022-05-23 21:33:46,759 DEBUG TRAIN Batch 19/7300 loss 8.778996 loss_att 7.555552 loss_ctc 11.633699 lr 0.00163323 rank 0\n",
            "2022-05-23 21:34:17,007 DEBUG TRAIN Batch 19/7400 loss 6.697461 loss_att 6.536522 loss_ctc 7.072984 lr 0.00163269 rank 0\n",
            "2022-05-23 21:34:48,029 DEBUG TRAIN Batch 19/7500 loss 7.741986 loss_att 6.949663 loss_ctc 9.590742 lr 0.00163214 rank 0\n",
            "2022-05-23 21:34:53,920 DEBUG CV Batch 19/0 loss 4.178343 loss_att 3.801242 loss_ctc 5.058246 history loss 3.932558 rank 0\n",
            "2022-05-23 21:35:05,566 DEBUG CV Batch 19/100 loss 2.954669 loss_att 2.961226 loss_ctc 2.939369 history loss 5.883300 rank 0\n",
            "2022-05-23 21:35:16,204 DEBUG CV Batch 19/200 loss 5.151861 loss_att 4.607453 loss_ctc 6.422147 history loss 5.858373 rank 0\n",
            "2022-05-23 21:35:27,271 DEBUG CV Batch 19/300 loss 4.141258 loss_att 4.283998 loss_ctc 3.808196 history loss 5.484987 rank 0\n",
            "2022-05-23 21:35:38,970 DEBUG CV Batch 19/400 loss 8.437466 loss_att 8.024738 loss_ctc 9.400497 history loss 5.051888 rank 0\n",
            "2022-05-23 21:35:51,361 DEBUG CV Batch 19/500 loss 2.853485 loss_att 2.827062 loss_ctc 2.915141 history loss 4.967294 rank 0\n",
            "2022-05-23 21:36:03,249 DEBUG CV Batch 19/600 loss 3.996111 loss_att 3.917411 loss_ctc 4.179744 history loss 4.917280 rank 0\n",
            "2022-05-23 21:36:14,414 DEBUG CV Batch 19/700 loss 3.920903 loss_att 3.731994 loss_ctc 4.361691 history loss 4.814675 rank 0\n",
            "2022-05-23 21:36:26,017 DEBUG CV Batch 19/800 loss 4.604990 loss_att 4.499611 loss_ctc 4.850874 history loss 4.783774 rank 0\n",
            "2022-05-23 21:36:37,649 INFO Epoch 19 CV info cv_loss 4.787002689017831\n",
            "2022-05-23 21:36:37,650 INFO Checkpoint: save to checkpoint exp/conformer/19.pt\n",
            "2022-05-23 21:36:37,993 INFO Epoch 20 TRAIN info lr 0.001632122928294128\n",
            "2022-05-23 21:36:37,995 INFO using accumulate grad, new batch size is 4 times larger than before\n",
            "2022-05-23 21:37:03,502 DEBUG TRAIN Batch 20/0 loss 6.005891 loss_att 5.424918 loss_ctc 7.361494 lr 0.00163210 rank 0\n",
            "2022-05-23 21:37:33,684 DEBUG TRAIN Batch 20/100 loss 5.907332 loss_att 5.476522 loss_ctc 6.912556 lr 0.00163156 rank 0\n",
            "2022-05-23 21:38:03,536 DEBUG TRAIN Batch 20/200 loss 3.582394 loss_att 3.415007 loss_ctc 3.972964 lr 0.00163102 rank 0\n",
            "2022-05-23 21:38:33,063 DEBUG TRAIN Batch 20/300 loss 7.375823 loss_att 6.546262 loss_ctc 9.311464 lr 0.00163047 rank 0\n",
            "2022-05-23 21:39:03,552 DEBUG TRAIN Batch 20/400 loss 9.632977 loss_att 9.726814 loss_ctc 9.414023 lr 0.00162993 rank 0\n",
            "2022-05-23 21:39:34,665 DEBUG TRAIN Batch 20/500 loss 6.797340 loss_att 5.829307 loss_ctc 9.056086 lr 0.00162939 rank 0\n",
            "2022-05-23 21:40:04,243 DEBUG TRAIN Batch 20/600 loss 5.528256 loss_att 5.017162 loss_ctc 6.720808 lr 0.00162885 rank 0\n",
            "2022-05-23 21:40:34,170 DEBUG TRAIN Batch 20/700 loss 5.521300 loss_att 4.937223 loss_ctc 6.884146 lr 0.00162831 rank 0\n",
            "2022-05-23 21:41:04,246 DEBUG TRAIN Batch 20/800 loss 6.110740 loss_att 5.890121 loss_ctc 6.625517 lr 0.00162777 rank 0\n",
            "2022-05-23 21:41:34,670 DEBUG TRAIN Batch 20/900 loss 10.354252 loss_att 8.911598 loss_ctc 13.720446 lr 0.00162723 rank 0\n",
            "2022-05-23 21:42:05,167 DEBUG TRAIN Batch 20/1000 loss 8.000018 loss_att 6.878804 loss_ctc 10.616184 lr 0.00162669 rank 0\n",
            "2022-05-23 21:42:34,787 DEBUG TRAIN Batch 20/1100 loss 6.430019 loss_att 5.667228 loss_ctc 8.209865 lr 0.00162616 rank 0\n",
            "2022-05-23 21:43:04,864 DEBUG TRAIN Batch 20/1200 loss 5.555617 loss_att 5.018517 loss_ctc 6.808850 lr 0.00162562 rank 0\n",
            "2022-05-23 21:43:34,770 DEBUG TRAIN Batch 20/1300 loss 7.757834 loss_att 6.988112 loss_ctc 9.553852 lr 0.00162508 rank 0\n",
            "2022-05-23 21:44:04,909 DEBUG TRAIN Batch 20/1400 loss 10.000011 loss_att 9.342793 loss_ctc 11.533519 lr 0.00162455 rank 0\n",
            "2022-05-23 21:44:35,267 DEBUG TRAIN Batch 20/1500 loss 8.498100 loss_att 7.407660 loss_ctc 11.042459 lr 0.00162401 rank 0\n",
            "2022-05-23 21:45:04,466 DEBUG TRAIN Batch 20/1600 loss 6.126985 loss_att 5.653783 loss_ctc 7.231120 lr 0.00162348 rank 0\n",
            "2022-05-23 21:45:33,648 DEBUG TRAIN Batch 20/1700 loss 9.470800 loss_att 8.208522 loss_ctc 12.416118 lr 0.00162294 rank 0\n",
            "2022-05-23 21:46:03,584 DEBUG TRAIN Batch 20/1800 loss 8.861504 loss_att 8.136166 loss_ctc 10.553957 lr 0.00162241 rank 0\n",
            "2022-05-23 21:46:33,796 DEBUG TRAIN Batch 20/1900 loss 8.453360 loss_att 7.925382 loss_ctc 9.685308 lr 0.00162187 rank 0\n",
            "2022-05-23 21:47:04,516 DEBUG TRAIN Batch 20/2000 loss 8.924535 loss_att 7.436888 loss_ctc 12.395710 lr 0.00162134 rank 0\n",
            "2022-05-23 21:47:34,165 DEBUG TRAIN Batch 20/2100 loss 6.692732 loss_att 6.119138 loss_ctc 8.031120 lr 0.00162081 rank 0\n",
            "2022-05-23 21:48:03,564 DEBUG TRAIN Batch 20/2200 loss 5.384038 loss_att 5.043380 loss_ctc 6.178907 lr 0.00162028 rank 0\n",
            "2022-05-23 21:48:33,186 DEBUG TRAIN Batch 20/2300 loss 6.354827 loss_att 5.883993 loss_ctc 7.453439 lr 0.00161974 rank 0\n",
            "2022-05-23 21:49:02,562 DEBUG TRAIN Batch 20/2400 loss 7.528157 loss_att 7.358317 loss_ctc 7.924449 lr 0.00161921 rank 0\n",
            "2022-05-23 21:49:33,439 DEBUG TRAIN Batch 20/2500 loss 8.376791 loss_att 7.244667 loss_ctc 11.018415 lr 0.00161868 rank 0\n",
            "2022-05-23 21:50:02,791 DEBUG TRAIN Batch 20/2600 loss 8.549861 loss_att 7.337394 loss_ctc 11.378950 lr 0.00161815 rank 0\n",
            "2022-05-23 21:50:32,494 DEBUG TRAIN Batch 20/2700 loss 6.013520 loss_att 5.617902 loss_ctc 6.936628 lr 0.00161762 rank 0\n",
            "2022-05-23 21:51:02,286 DEBUG TRAIN Batch 20/2800 loss 9.165762 loss_att 8.091087 loss_ctc 11.673335 lr 0.00161709 rank 0\n",
            "2022-05-23 21:51:32,385 DEBUG TRAIN Batch 20/2900 loss 8.805982 loss_att 8.831164 loss_ctc 8.747222 lr 0.00161657 rank 0\n",
            "2022-05-23 21:52:03,170 DEBUG TRAIN Batch 20/3000 loss 7.774952 loss_att 7.068937 loss_ctc 9.422319 lr 0.00161604 rank 0\n",
            "2022-05-23 21:52:32,694 DEBUG TRAIN Batch 20/3100 loss 7.339867 loss_att 6.429914 loss_ctc 9.463091 lr 0.00161551 rank 0\n",
            "2022-05-23 21:53:02,620 DEBUG TRAIN Batch 20/3200 loss 7.383922 loss_att 6.506656 loss_ctc 9.430877 lr 0.00161498 rank 0\n",
            "2022-05-23 21:53:32,323 DEBUG TRAIN Batch 20/3300 loss 6.887530 loss_att 6.694779 loss_ctc 7.337283 lr 0.00161446 rank 0\n",
            "2022-05-23 21:54:02,726 DEBUG TRAIN Batch 20/3400 loss 10.340631 loss_att 9.624641 loss_ctc 12.011271 lr 0.00161393 rank 0\n",
            "2022-05-23 21:54:33,213 DEBUG TRAIN Batch 20/3500 loss 4.146111 loss_att 3.492107 loss_ctc 5.672120 lr 0.00161341 rank 0\n",
            "2022-05-23 21:55:02,519 DEBUG TRAIN Batch 20/3600 loss 6.933831 loss_att 6.247551 loss_ctc 8.535151 lr 0.00161288 rank 0\n",
            "2022-05-23 21:55:32,585 DEBUG TRAIN Batch 20/3700 loss 6.309939 loss_att 5.622737 loss_ctc 7.913411 lr 0.00161236 rank 0\n",
            "2022-05-23 21:56:01,792 DEBUG TRAIN Batch 20/3800 loss 7.903379 loss_att 7.088244 loss_ctc 9.805361 lr 0.00161183 rank 0\n",
            "2022-05-23 21:56:31,910 DEBUG TRAIN Batch 20/3900 loss 9.308264 loss_att 9.035766 loss_ctc 9.944094 lr 0.00161131 rank 0\n",
            "2022-05-23 21:57:02,996 DEBUG TRAIN Batch 20/4000 loss 7.040836 loss_att 6.207823 loss_ctc 8.984535 lr 0.00161079 rank 0\n",
            "2022-05-23 21:57:32,213 DEBUG TRAIN Batch 20/4100 loss 7.239717 loss_att 6.592857 loss_ctc 8.749056 lr 0.00161027 rank 0\n",
            "2022-05-23 21:58:01,712 DEBUG TRAIN Batch 20/4200 loss 5.018414 loss_att 4.985528 loss_ctc 5.095147 lr 0.00160974 rank 0\n",
            "2022-05-23 21:58:31,746 DEBUG TRAIN Batch 20/4300 loss 7.042057 loss_att 6.257362 loss_ctc 8.873011 lr 0.00160922 rank 0\n",
            "2022-05-23 21:59:01,671 DEBUG TRAIN Batch 20/4400 loss 9.247164 loss_att 8.530043 loss_ctc 10.920447 lr 0.00160870 rank 0\n",
            "2022-05-23 21:59:32,621 DEBUG TRAIN Batch 20/4500 loss 6.813357 loss_att 5.989885 loss_ctc 8.734794 lr 0.00160818 rank 0\n",
            "2022-05-23 22:00:02,085 DEBUG TRAIN Batch 20/4600 loss 6.515845 loss_att 5.315158 loss_ctc 9.317448 lr 0.00160766 rank 0\n",
            "2022-05-23 22:00:31,880 DEBUG TRAIN Batch 20/4700 loss 5.278377 loss_att 4.956224 loss_ctc 6.030065 lr 0.00160714 rank 0\n",
            "2022-05-23 22:01:01,325 DEBUG TRAIN Batch 20/4800 loss 7.349104 loss_att 6.834970 loss_ctc 8.548750 lr 0.00160663 rank 0\n",
            "2022-05-23 22:01:31,365 DEBUG TRAIN Batch 20/4900 loss 8.687162 loss_att 8.325622 loss_ctc 9.530758 lr 0.00160611 rank 0\n",
            "2022-05-23 22:02:01,880 DEBUG TRAIN Batch 20/5000 loss 6.326501 loss_att 5.955691 loss_ctc 7.191722 lr 0.00160559 rank 0\n",
            "2022-05-23 22:02:31,498 DEBUG TRAIN Batch 20/5100 loss 6.003011 loss_att 5.660184 loss_ctc 6.802940 lr 0.00160507 rank 0\n",
            "2022-05-23 22:03:01,158 DEBUG TRAIN Batch 20/5200 loss 9.189386 loss_att 8.452668 loss_ctc 10.908397 lr 0.00160456 rank 0\n",
            "2022-05-23 22:03:31,062 DEBUG TRAIN Batch 20/5300 loss 8.868684 loss_att 7.968023 loss_ctc 10.970224 lr 0.00160404 rank 0\n",
            "2022-05-23 22:04:01,280 DEBUG TRAIN Batch 20/5400 loss 7.535583 loss_att 7.286727 loss_ctc 8.116247 lr 0.00160352 rank 0\n",
            "2022-05-23 22:04:31,760 DEBUG TRAIN Batch 20/5500 loss 7.549817 loss_att 6.247084 loss_ctc 10.589526 lr 0.00160301 rank 0\n",
            "2022-05-23 22:05:01,395 DEBUG TRAIN Batch 20/5600 loss 7.141270 loss_att 6.378580 loss_ctc 8.920879 lr 0.00160249 rank 0\n",
            "2022-05-23 22:05:31,003 DEBUG TRAIN Batch 20/5700 loss 6.663084 loss_att 5.965971 loss_ctc 8.289682 lr 0.00160198 rank 0\n",
            "2022-05-23 22:06:00,651 DEBUG TRAIN Batch 20/5800 loss 7.110257 loss_att 6.438221 loss_ctc 8.678342 lr 0.00160147 rank 0\n",
            "2022-05-23 22:06:30,222 DEBUG TRAIN Batch 20/5900 loss 8.111313 loss_att 7.697400 loss_ctc 9.077110 lr 0.00160095 rank 0\n",
            "2022-05-23 22:07:00,795 DEBUG TRAIN Batch 20/6000 loss 7.041006 loss_att 5.721491 loss_ctc 10.119873 lr 0.00160044 rank 0\n",
            "2022-05-23 22:07:30,018 DEBUG TRAIN Batch 20/6100 loss 5.488523 loss_att 4.922795 loss_ctc 6.808554 lr 0.00159993 rank 0\n",
            "2022-05-23 22:07:59,734 DEBUG TRAIN Batch 20/6200 loss 8.080013 loss_att 7.392709 loss_ctc 9.683724 lr 0.00159942 rank 0\n",
            "2022-05-23 22:08:29,240 DEBUG TRAIN Batch 20/6300 loss 5.912829 loss_att 5.149989 loss_ctc 7.692790 lr 0.00159891 rank 0\n",
            "2022-05-23 22:08:58,992 DEBUG TRAIN Batch 20/6400 loss 8.748768 loss_att 8.068997 loss_ctc 10.334900 lr 0.00159839 rank 0\n",
            "2022-05-23 22:09:29,591 DEBUG TRAIN Batch 20/6500 loss 4.198109 loss_att 3.788312 loss_ctc 5.154300 lr 0.00159788 rank 0\n",
            "2022-05-23 22:09:58,781 DEBUG TRAIN Batch 20/6600 loss 7.102409 loss_att 6.663179 loss_ctc 8.127278 lr 0.00159737 rank 0\n",
            "2022-05-23 22:10:28,089 DEBUG TRAIN Batch 20/6700 loss 4.774479 loss_att 4.856364 loss_ctc 4.583416 lr 0.00159687 rank 0\n",
            "2022-05-23 22:10:57,726 DEBUG TRAIN Batch 20/6800 loss 6.099200 loss_att 5.588117 loss_ctc 7.291727 lr 0.00159636 rank 0\n",
            "2022-05-23 22:11:27,946 DEBUG TRAIN Batch 20/6900 loss 7.734859 loss_att 7.164733 loss_ctc 9.065155 lr 0.00159585 rank 0\n",
            "2022-05-23 22:11:58,870 DEBUG TRAIN Batch 20/7000 loss 7.654668 loss_att 6.590086 loss_ctc 10.138692 lr 0.00159534 rank 0\n",
            "2022-05-23 22:12:28,253 DEBUG TRAIN Batch 20/7100 loss 9.132711 loss_att 7.984169 loss_ctc 11.812643 lr 0.00159483 rank 0\n",
            "2022-05-23 22:12:58,035 DEBUG TRAIN Batch 20/7200 loss 7.060016 loss_att 6.515235 loss_ctc 8.331173 lr 0.00159433 rank 0\n",
            "2022-05-23 22:13:28,115 DEBUG TRAIN Batch 20/7300 loss 10.505533 loss_att 9.070848 loss_ctc 13.853129 lr 0.00159382 rank 0\n",
            "2022-05-23 22:13:58,528 DEBUG TRAIN Batch 20/7400 loss 4.994906 loss_att 5.272996 loss_ctc 4.346031 lr 0.00159331 rank 0\n",
            "2022-05-23 22:14:28,997 DEBUG TRAIN Batch 20/7500 loss 5.121382 loss_att 4.385746 loss_ctc 6.837866 lr 0.00159281 rank 0\n",
            "2022-05-23 22:14:35,218 DEBUG CV Batch 20/0 loss 4.119452 loss_att 3.517490 loss_ctc 5.524028 history loss 3.877131 rank 0\n",
            "2022-05-23 22:14:46,720 DEBUG CV Batch 20/100 loss 2.634028 loss_att 2.680211 loss_ctc 2.526268 history loss 5.672413 rank 0\n",
            "2022-05-23 22:14:57,058 DEBUG CV Batch 20/200 loss 5.220418 loss_att 4.719543 loss_ctc 6.389128 history loss 5.671298 rank 0\n",
            "2022-05-23 22:15:07,926 DEBUG CV Batch 20/300 loss 4.629353 loss_att 4.614920 loss_ctc 4.663031 history loss 5.371195 rank 0\n",
            "2022-05-23 22:15:19,485 DEBUG CV Batch 20/400 loss 8.748641 loss_att 8.020759 loss_ctc 10.447032 history loss 4.967537 rank 0\n",
            "2022-05-23 22:15:31,399 DEBUG CV Batch 20/500 loss 3.097851 loss_att 3.116794 loss_ctc 3.053652 history loss 4.906096 rank 0\n",
            "2022-05-23 22:15:42,958 DEBUG CV Batch 20/600 loss 3.395007 loss_att 3.244218 loss_ctc 3.746847 history loss 4.859363 rank 0\n",
            "2022-05-23 22:15:53,874 DEBUG CV Batch 20/700 loss 3.930003 loss_att 3.606512 loss_ctc 4.684814 history loss 4.748826 rank 0\n",
            "2022-05-23 22:16:05,294 DEBUG CV Batch 20/800 loss 4.243413 loss_att 3.843516 loss_ctc 5.176507 history loss 4.714554 rank 0\n",
            "2022-05-23 22:16:16,908 INFO Epoch 20 CV info cv_loss 4.7165971395066935\n",
            "2022-05-23 22:16:16,908 INFO Checkpoint: save to checkpoint exp/conformer/20.pt\n",
            "2022-05-23 22:16:17,242 INFO Epoch 21 TRAIN info lr 0.0015927888847832132\n",
            "2022-05-23 22:16:17,244 INFO using accumulate grad, new batch size is 4 times larger than before\n",
            "2022-05-23 22:16:42,063 DEBUG TRAIN Batch 21/0 loss 7.951852 loss_att 6.896352 loss_ctc 10.414685 lr 0.00159277 rank 0\n",
            "2022-05-23 22:17:12,207 DEBUG TRAIN Batch 21/100 loss 4.936555 loss_att 4.453687 loss_ctc 6.063249 lr 0.00159226 rank 0\n",
            "2022-05-23 22:17:42,284 DEBUG TRAIN Batch 21/200 loss 7.027657 loss_att 6.004242 loss_ctc 9.415625 lr 0.00159176 rank 0\n",
            "2022-05-23 22:18:11,448 DEBUG TRAIN Batch 21/300 loss 6.596386 loss_att 5.982427 loss_ctc 8.028957 lr 0.00159126 rank 0\n",
            "2022-05-23 22:18:41,068 DEBUG TRAIN Batch 21/400 loss 7.384174 loss_att 6.730416 loss_ctc 8.909609 lr 0.00159075 rank 0\n",
            "2022-05-23 22:19:12,096 DEBUG TRAIN Batch 21/500 loss 5.796814 loss_att 5.443554 loss_ctc 6.621088 lr 0.00159025 rank 0\n",
            "2022-05-23 22:19:41,899 DEBUG TRAIN Batch 21/600 loss 7.991203 loss_att 7.376273 loss_ctc 9.426039 lr 0.00158975 rank 0\n",
            "2022-05-23 22:20:11,538 DEBUG TRAIN Batch 21/700 loss 5.108438 loss_att 4.446381 loss_ctc 6.653238 lr 0.00158924 rank 0\n",
            "2022-05-23 22:20:40,754 DEBUG TRAIN Batch 21/800 loss 7.007545 loss_att 5.939858 loss_ctc 9.498812 lr 0.00158874 rank 0\n",
            "2022-05-23 22:21:11,342 DEBUG TRAIN Batch 21/900 loss 7.701172 loss_att 6.991236 loss_ctc 9.357688 lr 0.00158824 rank 0\n",
            "2022-05-23 22:21:42,336 DEBUG TRAIN Batch 21/1000 loss 6.388081 loss_att 5.035334 loss_ctc 9.544492 lr 0.00158774 rank 0\n",
            "2022-05-23 22:22:11,612 DEBUG TRAIN Batch 21/1100 loss 7.391801 loss_att 6.725408 loss_ctc 8.946718 lr 0.00158724 rank 0\n",
            "2022-05-23 22:22:41,410 DEBUG TRAIN Batch 21/1200 loss 5.775114 loss_att 5.459916 loss_ctc 6.510577 lr 0.00158674 rank 0\n",
            "2022-05-23 22:23:11,433 DEBUG TRAIN Batch 21/1300 loss 6.603407 loss_att 6.082713 loss_ctc 7.818358 lr 0.00158624 rank 0\n",
            "2022-05-23 22:23:41,406 DEBUG TRAIN Batch 21/1400 loss 9.169910 loss_att 8.001831 loss_ctc 11.895429 lr 0.00158574 rank 0\n",
            "2022-05-23 22:24:12,349 DEBUG TRAIN Batch 21/1500 loss 7.826582 loss_att 7.106119 loss_ctc 9.507662 lr 0.00158525 rank 0\n",
            "2022-05-23 22:24:42,048 DEBUG TRAIN Batch 21/1600 loss 8.024738 loss_att 6.657409 loss_ctc 11.215174 lr 0.00158475 rank 0\n",
            "2022-05-23 22:25:11,753 DEBUG TRAIN Batch 21/1700 loss 7.952027 loss_att 7.402101 loss_ctc 9.235188 lr 0.00158425 rank 0\n",
            "2022-05-23 22:25:41,555 DEBUG TRAIN Batch 21/1800 loss 7.684649 loss_att 7.026540 loss_ctc 9.220238 lr 0.00158375 rank 0\n",
            "2022-05-23 22:26:11,310 DEBUG TRAIN Batch 21/1900 loss 12.152456 loss_att 11.000818 loss_ctc 14.839612 lr 0.00158326 rank 0\n",
            "2022-05-23 22:26:41,658 DEBUG TRAIN Batch 21/2000 loss 6.091195 loss_att 5.240435 loss_ctc 8.076302 lr 0.00158276 rank 0\n",
            "2022-05-23 22:27:11,100 DEBUG TRAIN Batch 21/2100 loss 5.943007 loss_att 5.292662 loss_ctc 7.460481 lr 0.00158227 rank 0\n",
            "2022-05-23 22:27:40,237 DEBUG TRAIN Batch 21/2200 loss 3.561941 loss_att 3.381620 loss_ctc 3.982690 lr 0.00158177 rank 0\n",
            "2022-05-23 22:28:10,142 DEBUG TRAIN Batch 21/2300 loss 7.608554 loss_att 6.565866 loss_ctc 10.041491 lr 0.00158128 rank 0\n",
            "2022-05-23 22:28:40,333 DEBUG TRAIN Batch 21/2400 loss 5.245991 loss_att 5.157921 loss_ctc 5.451489 lr 0.00158078 rank 0\n",
            "2022-05-23 22:29:11,207 DEBUG TRAIN Batch 21/2500 loss 7.224545 loss_att 6.670841 loss_ctc 8.516520 lr 0.00158029 rank 0\n",
            "2022-05-23 22:29:40,398 DEBUG TRAIN Batch 21/2600 loss 4.605884 loss_att 4.240158 loss_ctc 5.459244 lr 0.00157980 rank 0\n",
            "2022-05-23 22:30:10,151 DEBUG TRAIN Batch 21/2700 loss 6.445296 loss_att 6.262219 loss_ctc 6.872474 lr 0.00157930 rank 0\n",
            "2022-05-23 22:30:39,750 DEBUG TRAIN Batch 21/2800 loss 5.388614 loss_att 5.044042 loss_ctc 6.192616 lr 0.00157881 rank 0\n",
            "2022-05-23 22:31:09,804 DEBUG TRAIN Batch 21/2900 loss 11.037602 loss_att 10.088120 loss_ctc 13.253059 lr 0.00157832 rank 0\n",
            "2022-05-23 22:31:40,601 DEBUG TRAIN Batch 21/3000 loss 7.242390 loss_att 6.608177 loss_ctc 8.722220 lr 0.00157783 rank 0\n",
            "2022-05-23 22:32:09,918 DEBUG TRAIN Batch 21/3100 loss 5.099506 loss_att 4.603862 loss_ctc 6.256008 lr 0.00157734 rank 0\n",
            "2022-05-23 22:32:39,645 DEBUG TRAIN Batch 21/3200 loss 5.996527 loss_att 5.488511 loss_ctc 7.181897 lr 0.00157685 rank 0\n",
            "2022-05-23 22:33:09,322 DEBUG TRAIN Batch 21/3300 loss 5.644291 loss_att 5.249885 loss_ctc 6.564572 lr 0.00157636 rank 0\n",
            "2022-05-23 22:33:39,263 DEBUG TRAIN Batch 21/3400 loss 6.654625 loss_att 6.439176 loss_ctc 7.157341 lr 0.00157587 rank 0\n",
            "2022-05-23 22:34:10,573 DEBUG TRAIN Batch 21/3500 loss 7.411141 loss_att 6.569951 loss_ctc 9.373919 lr 0.00157538 rank 0\n",
            "2022-05-23 22:34:40,248 DEBUG TRAIN Batch 21/3600 loss 4.731706 loss_att 4.208086 loss_ctc 5.953484 lr 0.00157489 rank 0\n",
            "2022-05-23 22:35:09,921 DEBUG TRAIN Batch 21/3700 loss 4.434266 loss_att 4.134005 loss_ctc 5.134873 lr 0.00157440 rank 0\n",
            "2022-05-23 22:35:39,989 DEBUG TRAIN Batch 21/3800 loss 7.276684 loss_att 6.837547 loss_ctc 8.301336 lr 0.00157392 rank 0\n",
            "2022-05-23 22:36:10,076 DEBUG TRAIN Batch 21/3900 loss 9.991085 loss_att 8.881348 loss_ctc 12.580474 lr 0.00157343 rank 0\n",
            "2022-05-23 22:36:40,912 DEBUG TRAIN Batch 21/4000 loss 7.411413 loss_att 6.375905 loss_ctc 9.827600 lr 0.00157294 rank 0\n",
            "2022-05-23 22:37:10,791 DEBUG TRAIN Batch 21/4100 loss 5.543811 loss_att 4.889057 loss_ctc 7.071571 lr 0.00157246 rank 0\n",
            "2022-05-23 22:37:40,267 DEBUG TRAIN Batch 21/4200 loss 5.424535 loss_att 4.635893 loss_ctc 7.264699 lr 0.00157197 rank 0\n",
            "2022-05-23 22:38:10,139 DEBUG TRAIN Batch 21/4300 loss 9.089342 loss_att 8.514378 loss_ctc 10.430924 lr 0.00157148 rank 0\n",
            "2022-05-23 22:38:40,152 DEBUG TRAIN Batch 21/4400 loss 10.364408 loss_att 9.333297 loss_ctc 12.770336 lr 0.00157100 rank 0\n",
            "2022-05-23 22:39:11,245 DEBUG TRAIN Batch 21/4500 loss 7.629895 loss_att 6.704077 loss_ctc 9.790138 lr 0.00157051 rank 0\n",
            "2022-05-23 22:39:40,774 DEBUG TRAIN Batch 21/4600 loss 8.329404 loss_att 7.385033 loss_ctc 10.532936 lr 0.00157003 rank 0\n",
            "2022-05-23 22:40:10,166 DEBUG TRAIN Batch 21/4700 loss 8.402575 loss_att 7.707803 loss_ctc 10.023708 lr 0.00156955 rank 0\n",
            "2022-05-23 22:40:39,565 DEBUG TRAIN Batch 21/4800 loss 6.421279 loss_att 5.842532 loss_ctc 7.771690 lr 0.00156906 rank 0\n",
            "2022-05-23 22:41:09,656 DEBUG TRAIN Batch 21/4900 loss 8.252178 loss_att 7.768181 loss_ctc 9.381506 lr 0.00156858 rank 0\n",
            "2022-05-23 22:41:40,352 DEBUG TRAIN Batch 21/5000 loss 4.879348 loss_att 4.004724 loss_ctc 6.920140 lr 0.00156810 rank 0\n",
            "2022-05-23 22:42:09,855 DEBUG TRAIN Batch 21/5100 loss 6.151123 loss_att 5.690949 loss_ctc 7.224862 lr 0.00156762 rank 0\n",
            "2022-05-23 22:42:38,819 DEBUG TRAIN Batch 21/5200 loss 6.863957 loss_att 5.957585 loss_ctc 8.978827 lr 0.00156714 rank 0\n",
            "2022-05-23 22:43:08,333 DEBUG TRAIN Batch 21/5300 loss 8.589676 loss_att 7.934852 loss_ctc 10.117598 lr 0.00156666 rank 0\n",
            "2022-05-23 22:43:38,346 DEBUG TRAIN Batch 21/5400 loss 10.019988 loss_att 9.482052 loss_ctc 11.275171 lr 0.00156618 rank 0\n",
            "2022-05-23 22:44:09,405 DEBUG TRAIN Batch 21/5500 loss 8.467707 loss_att 7.363715 loss_ctc 11.043687 lr 0.00156570 rank 0\n",
            "2022-05-23 22:44:38,877 DEBUG TRAIN Batch 21/5600 loss 4.507335 loss_att 4.035212 loss_ctc 5.608954 lr 0.00156522 rank 0\n",
            "2022-05-23 22:45:08,311 DEBUG TRAIN Batch 21/5700 loss 4.746904 loss_att 4.400495 loss_ctc 5.555194 lr 0.00156474 rank 0\n",
            "2022-05-23 22:45:38,080 DEBUG TRAIN Batch 21/5800 loss 8.380703 loss_att 7.998103 loss_ctc 9.273435 lr 0.00156426 rank 0\n",
            "2022-05-23 22:46:08,172 DEBUG TRAIN Batch 21/5900 loss 8.881407 loss_att 8.037554 loss_ctc 10.850396 lr 0.00156378 rank 0\n",
            "2022-05-23 22:46:38,729 DEBUG TRAIN Batch 21/6000 loss 7.174895 loss_att 6.755701 loss_ctc 8.153016 lr 0.00156330 rank 0\n",
            "2022-05-23 22:47:08,480 DEBUG TRAIN Batch 21/6100 loss 4.544124 loss_att 3.879584 loss_ctc 6.094716 lr 0.00156282 rank 0\n",
            "2022-05-23 22:47:38,129 DEBUG TRAIN Batch 21/6200 loss 7.592095 loss_att 6.365481 loss_ctc 10.454195 lr 0.00156235 rank 0\n",
            "2022-05-23 22:48:07,557 DEBUG TRAIN Batch 21/6300 loss 10.068666 loss_att 8.778283 loss_ctc 13.079560 lr 0.00156187 rank 0\n",
            "2022-05-23 22:48:37,771 DEBUG TRAIN Batch 21/6400 loss 11.806181 loss_att 11.111074 loss_ctc 13.428096 lr 0.00156139 rank 0\n",
            "2022-05-23 22:49:08,628 DEBUG TRAIN Batch 21/6500 loss 5.466517 loss_att 5.056231 loss_ctc 6.423851 lr 0.00156092 rank 0\n",
            "2022-05-23 22:49:37,979 DEBUG TRAIN Batch 21/6600 loss 5.677726 loss_att 5.127469 loss_ctc 6.961658 lr 0.00156044 rank 0\n",
            "2022-05-23 22:50:07,535 DEBUG TRAIN Batch 21/6700 loss 7.347977 loss_att 6.671312 loss_ctc 8.926861 lr 0.00155997 rank 0\n",
            "2022-05-23 22:50:37,413 DEBUG TRAIN Batch 21/6800 loss 6.363584 loss_att 5.763073 loss_ctc 7.764774 lr 0.00155950 rank 0\n",
            "2022-05-23 22:51:07,599 DEBUG TRAIN Batch 21/6900 loss 7.590508 loss_att 7.539940 loss_ctc 7.708498 lr 0.00155902 rank 0\n",
            "2022-05-23 22:51:38,433 DEBUG TRAIN Batch 21/7000 loss 6.465574 loss_att 5.841988 loss_ctc 7.920610 lr 0.00155855 rank 0\n",
            "2022-05-23 22:52:07,571 DEBUG TRAIN Batch 21/7100 loss 6.429708 loss_att 5.761863 loss_ctc 7.988015 lr 0.00155807 rank 0\n",
            "2022-05-23 22:52:37,335 DEBUG TRAIN Batch 21/7200 loss 5.806790 loss_att 4.945472 loss_ctc 7.816534 lr 0.00155760 rank 0\n",
            "2022-05-23 22:53:07,086 DEBUG TRAIN Batch 21/7300 loss 8.380862 loss_att 7.643201 loss_ctc 10.102071 lr 0.00155713 rank 0\n",
            "2022-05-23 22:53:37,440 DEBUG TRAIN Batch 21/7400 loss 6.156035 loss_att 5.485834 loss_ctc 7.719839 lr 0.00155666 rank 0\n",
            "2022-05-23 22:54:07,712 DEBUG TRAIN Batch 21/7500 loss 6.760695 loss_att 5.775256 loss_ctc 9.060053 lr 0.00155619 rank 0\n",
            "2022-05-23 22:54:13,919 DEBUG CV Batch 21/0 loss 4.159365 loss_att 3.746423 loss_ctc 5.122897 history loss 3.914697 rank 0\n",
            "2022-05-23 22:54:25,276 DEBUG CV Batch 21/100 loss 2.718246 loss_att 2.734355 loss_ctc 2.680656 history loss 5.585479 rank 0\n",
            "2022-05-23 22:54:35,858 DEBUG CV Batch 21/200 loss 4.751038 loss_att 4.459935 loss_ctc 5.430278 history loss 5.581521 rank 0\n",
            "2022-05-23 22:54:46,918 DEBUG CV Batch 21/300 loss 3.573835 loss_att 3.635974 loss_ctc 3.428843 history loss 5.277466 rank 0\n",
            "2022-05-23 22:54:58,509 DEBUG CV Batch 21/400 loss 8.566269 loss_att 8.298506 loss_ctc 9.191050 history loss 4.864466 rank 0\n",
            "2022-05-23 22:55:10,656 DEBUG CV Batch 21/500 loss 2.825304 loss_att 2.834114 loss_ctc 2.804747 history loss 4.807513 rank 0\n",
            "2022-05-23 22:55:22,379 DEBUG CV Batch 21/600 loss 4.110804 loss_att 3.874044 loss_ctc 4.663244 history loss 4.758575 rank 0\n",
            "2022-05-23 22:55:33,385 DEBUG CV Batch 21/700 loss 3.515381 loss_att 3.096980 loss_ctc 4.491649 history loss 4.669172 rank 0\n",
            "2022-05-23 22:55:44,553 DEBUG CV Batch 21/800 loss 4.952355 loss_att 4.550632 loss_ctc 5.889708 history loss 4.634740 rank 0\n",
            "2022-05-23 22:55:56,017 INFO Epoch 21 CV info cv_loss 4.649455483463809\n",
            "2022-05-23 22:55:56,017 INFO Checkpoint: save to checkpoint exp/conformer/21.pt\n",
            "2022-05-23 22:55:56,376 INFO Epoch 22 TRAIN info lr 0.0015561681531784174\n",
            "2022-05-23 22:55:56,378 INFO using accumulate grad, new batch size is 4 times larger than before\n",
            "2022-05-23 22:56:20,898 DEBUG TRAIN Batch 22/0 loss 9.786478 loss_att 8.354689 loss_ctc 13.127320 lr 0.00155615 rank 0\n",
            "2022-05-23 22:56:50,922 DEBUG TRAIN Batch 22/100 loss 6.293460 loss_att 5.000471 loss_ctc 9.310434 lr 0.00155568 rank 0\n",
            "2022-05-23 22:57:20,878 DEBUG TRAIN Batch 22/200 loss 5.364341 loss_att 4.520278 loss_ctc 7.333822 lr 0.00155521 rank 0\n",
            "2022-05-23 22:57:50,685 DEBUG TRAIN Batch 22/300 loss 7.893613 loss_att 7.015968 loss_ctc 9.941452 lr 0.00155474 rank 0\n",
            "2022-05-23 22:58:20,915 DEBUG TRAIN Batch 22/400 loss 7.756915 loss_att 7.457797 loss_ctc 8.454859 lr 0.00155427 rank 0\n",
            "2022-05-23 22:58:51,748 DEBUG TRAIN Batch 22/500 loss 5.914153 loss_att 5.306115 loss_ctc 7.332909 lr 0.00155380 rank 0\n",
            "2022-05-23 22:59:21,074 DEBUG TRAIN Batch 22/600 loss 7.054202 loss_att 6.272048 loss_ctc 8.879229 lr 0.00155333 rank 0\n",
            "2022-05-23 22:59:50,635 DEBUG TRAIN Batch 22/700 loss 4.622749 loss_att 4.354856 loss_ctc 5.247833 lr 0.00155286 rank 0\n",
            "2022-05-23 23:00:20,553 DEBUG TRAIN Batch 22/800 loss 6.879911 loss_att 6.348606 loss_ctc 8.119624 lr 0.00155239 rank 0\n",
            "2022-05-23 23:00:50,543 DEBUG TRAIN Batch 22/900 loss 5.994806 loss_att 6.413235 loss_ctc 5.018475 lr 0.00155193 rank 0\n",
            "2022-05-23 23:01:21,094 DEBUG TRAIN Batch 22/1000 loss 5.522894 loss_att 5.023607 loss_ctc 6.687896 lr 0.00155146 rank 0\n",
            "2022-05-23 23:01:50,443 DEBUG TRAIN Batch 22/1100 loss 6.175710 loss_att 5.395816 loss_ctc 7.995463 lr 0.00155099 rank 0\n",
            "2022-05-23 23:02:20,430 DEBUG TRAIN Batch 22/1200 loss 6.267279 loss_att 5.787817 loss_ctc 7.386023 lr 0.00155053 rank 0\n",
            "2022-05-23 23:02:50,112 DEBUG TRAIN Batch 22/1300 loss 6.883250 loss_att 7.024960 loss_ctc 6.552595 lr 0.00155006 rank 0\n",
            "2022-05-23 23:03:20,194 DEBUG TRAIN Batch 22/1400 loss 8.179556 loss_att 7.620121 loss_ctc 9.484904 lr 0.00154960 rank 0\n",
            "2022-05-23 23:03:50,852 DEBUG TRAIN Batch 22/1500 loss 7.161559 loss_att 5.825780 loss_ctc 10.278374 lr 0.00154913 rank 0\n",
            "2022-05-23 23:04:20,535 DEBUG TRAIN Batch 22/1600 loss 4.282411 loss_att 4.054418 loss_ctc 4.814394 lr 0.00154867 rank 0\n",
            "2022-05-23 23:04:50,087 DEBUG TRAIN Batch 22/1700 loss 6.592550 loss_att 5.862160 loss_ctc 8.296793 lr 0.00154820 rank 0\n",
            "2022-05-23 23:05:19,801 DEBUG TRAIN Batch 22/1800 loss 8.439648 loss_att 7.539270 loss_ctc 10.540529 lr 0.00154774 rank 0\n",
            "2022-05-23 23:05:49,670 DEBUG TRAIN Batch 22/1900 loss 8.546962 loss_att 7.788043 loss_ctc 10.317771 lr 0.00154728 rank 0\n",
            "2022-05-23 23:06:20,089 DEBUG TRAIN Batch 22/2000 loss 5.400671 loss_att 4.718250 loss_ctc 6.992987 lr 0.00154681 rank 0\n",
            "2022-05-23 23:06:49,390 DEBUG TRAIN Batch 22/2100 loss 6.378322 loss_att 5.524751 loss_ctc 8.369987 lr 0.00154635 rank 0\n",
            "2022-05-23 23:07:19,364 DEBUG TRAIN Batch 22/2200 loss 5.814338 loss_att 4.982800 loss_ctc 7.754594 lr 0.00154589 rank 0\n",
            "2022-05-23 23:07:49,241 DEBUG TRAIN Batch 22/2300 loss 6.854725 loss_att 5.996538 loss_ctc 8.857162 lr 0.00154543 rank 0\n",
            "2022-05-23 23:08:19,319 DEBUG TRAIN Batch 22/2400 loss 8.840504 loss_att 7.685134 loss_ctc 11.536367 lr 0.00154497 rank 0\n",
            "2022-05-23 23:08:50,261 DEBUG TRAIN Batch 22/2500 loss 4.705820 loss_att 4.383707 loss_ctc 5.457417 lr 0.00154451 rank 0\n",
            "2022-05-23 23:09:19,723 DEBUG TRAIN Batch 22/2600 loss 6.434894 loss_att 5.353755 loss_ctc 8.957552 lr 0.00154404 rank 0\n",
            "2022-05-23 23:09:49,435 DEBUG TRAIN Batch 22/2700 loss 4.698654 loss_att 4.192279 loss_ctc 5.880197 lr 0.00154358 rank 0\n",
            "2022-05-23 23:10:19,335 DEBUG TRAIN Batch 22/2800 loss 9.247768 loss_att 8.090946 loss_ctc 11.947020 lr 0.00154313 rank 0\n",
            "2022-05-23 23:10:49,545 DEBUG TRAIN Batch 22/2900 loss 8.796795 loss_att 8.359390 loss_ctc 9.817405 lr 0.00154267 rank 0\n",
            "2022-05-23 23:11:20,090 DEBUG TRAIN Batch 22/3000 loss 6.939911 loss_att 6.045663 loss_ctc 9.026489 lr 0.00154221 rank 0\n",
            "2022-05-23 23:11:49,570 DEBUG TRAIN Batch 22/3100 loss 6.962937 loss_att 5.567840 loss_ctc 10.218164 lr 0.00154175 rank 0\n",
            "2022-05-23 23:12:19,017 DEBUG TRAIN Batch 22/3200 loss 6.278876 loss_att 6.024328 loss_ctc 6.872823 lr 0.00154129 rank 0\n",
            "2022-05-23 23:12:48,398 DEBUG TRAIN Batch 22/3300 loss 5.231461 loss_att 5.135935 loss_ctc 5.454353 lr 0.00154083 rank 0\n",
            "2022-05-23 23:13:18,896 DEBUG TRAIN Batch 22/3400 loss 6.921407 loss_att 6.833211 loss_ctc 7.127198 lr 0.00154038 rank 0\n",
            "2022-05-23 23:13:49,965 DEBUG TRAIN Batch 22/3500 loss 4.075233 loss_att 3.547751 loss_ctc 5.306024 lr 0.00153992 rank 0\n",
            "2022-05-23 23:14:19,542 DEBUG TRAIN Batch 22/3600 loss 6.318915 loss_att 5.595277 loss_ctc 8.007404 lr 0.00153946 rank 0\n",
            "2022-05-23 23:14:48,917 DEBUG TRAIN Batch 22/3700 loss 6.011011 loss_att 5.785697 loss_ctc 6.536744 lr 0.00153901 rank 0\n",
            "2022-05-23 23:15:18,226 DEBUG TRAIN Batch 22/3800 loss 7.964577 loss_att 7.381968 loss_ctc 9.323996 lr 0.00153855 rank 0\n",
            "2022-05-23 23:15:48,172 DEBUG TRAIN Batch 22/3900 loss 5.884765 loss_att 5.190062 loss_ctc 7.505740 lr 0.00153810 rank 0\n",
            "2022-05-23 23:16:18,777 DEBUG TRAIN Batch 22/4000 loss 6.702892 loss_att 5.989525 loss_ctc 8.367414 lr 0.00153764 rank 0\n",
            "2022-05-23 23:16:48,396 DEBUG TRAIN Batch 22/4100 loss 5.417730 loss_att 4.589612 loss_ctc 7.350006 lr 0.00153719 rank 0\n",
            "2022-05-23 23:17:17,789 DEBUG TRAIN Batch 22/4200 loss 5.906299 loss_att 5.319003 loss_ctc 7.276657 lr 0.00153673 rank 0\n",
            "2022-05-23 23:17:47,737 DEBUG TRAIN Batch 22/4300 loss 6.516727 loss_att 6.023974 loss_ctc 7.666483 lr 0.00153628 rank 0\n",
            "2022-05-23 23:18:17,767 DEBUG TRAIN Batch 22/4400 loss 7.089541 loss_att 6.741557 loss_ctc 7.901505 lr 0.00153583 rank 0\n",
            "2022-05-23 23:18:48,447 DEBUG TRAIN Batch 22/4500 loss 7.390751 loss_att 6.643313 loss_ctc 9.134771 lr 0.00153538 rank 0\n",
            "2022-05-23 23:19:17,877 DEBUG TRAIN Batch 22/4600 loss 4.316007 loss_att 4.075804 loss_ctc 4.876481 lr 0.00153492 rank 0\n",
            "2022-05-23 23:19:47,722 DEBUG TRAIN Batch 22/4700 loss 5.519861 loss_att 4.935563 loss_ctc 6.883223 lr 0.00153447 rank 0\n",
            "2022-05-23 23:20:17,149 DEBUG TRAIN Batch 22/4800 loss 5.624037 loss_att 5.114419 loss_ctc 6.813147 lr 0.00153402 rank 0\n",
            "2022-05-23 23:20:46,746 DEBUG TRAIN Batch 22/4900 loss 7.000615 loss_att 6.542708 loss_ctc 8.069064 lr 0.00153357 rank 0\n",
            "2022-05-23 23:21:17,291 DEBUG TRAIN Batch 22/5000 loss 5.434276 loss_att 4.853358 loss_ctc 6.789750 lr 0.00153312 rank 0\n",
            "2022-05-23 23:21:46,884 DEBUG TRAIN Batch 22/5100 loss 6.041420 loss_att 5.224998 loss_ctc 7.946404 lr 0.00153267 rank 0\n",
            "2022-05-23 23:22:16,043 DEBUG TRAIN Batch 22/5200 loss 6.292632 loss_att 5.791570 loss_ctc 7.461777 lr 0.00153222 rank 0\n",
            "2022-05-23 23:22:45,847 DEBUG TRAIN Batch 22/5300 loss 7.742573 loss_att 7.140831 loss_ctc 9.146637 lr 0.00153177 rank 0\n",
            "2022-05-23 23:23:15,688 DEBUG TRAIN Batch 22/5400 loss 6.691587 loss_att 6.027317 loss_ctc 8.241549 lr 0.00153132 rank 0\n",
            "2022-05-23 23:23:46,374 DEBUG TRAIN Batch 22/5500 loss 4.924685 loss_att 4.033360 loss_ctc 7.004444 lr 0.00153087 rank 0\n",
            "2022-05-23 23:24:15,833 DEBUG TRAIN Batch 22/5600 loss 7.019645 loss_att 6.081571 loss_ctc 9.208487 lr 0.00153042 rank 0\n",
            "2022-05-23 23:24:45,330 DEBUG TRAIN Batch 22/5700 loss 6.719627 loss_att 5.834073 loss_ctc 8.785920 lr 0.00152998 rank 0\n",
            "2022-05-23 23:25:14,771 DEBUG TRAIN Batch 22/5800 loss 6.977857 loss_att 6.053609 loss_ctc 9.134435 lr 0.00152953 rank 0\n",
            "2022-05-23 23:25:45,289 DEBUG TRAIN Batch 22/5900 loss 7.053747 loss_att 6.662171 loss_ctc 7.967423 lr 0.00152908 rank 0\n",
            "2022-05-23 23:26:15,764 DEBUG TRAIN Batch 22/6000 loss 5.983594 loss_att 5.607127 loss_ctc 6.862017 lr 0.00152863 rank 0\n",
            "2022-05-23 23:26:45,188 DEBUG TRAIN Batch 22/6100 loss 6.967200 loss_att 6.007792 loss_ctc 9.205822 lr 0.00152819 rank 0\n",
            "2022-05-23 23:27:14,790 DEBUG TRAIN Batch 22/6200 loss 6.938535 loss_att 6.688514 loss_ctc 7.521917 lr 0.00152774 rank 0\n",
            "2022-05-23 23:27:44,830 DEBUG TRAIN Batch 22/6300 loss 5.675726 loss_att 5.231693 loss_ctc 6.711804 lr 0.00152730 rank 0\n",
            "2022-05-23 23:28:14,937 DEBUG TRAIN Batch 22/6400 loss 6.935410 loss_att 6.429111 loss_ctc 8.116775 lr 0.00152685 rank 0\n",
            "2022-05-23 23:28:45,665 DEBUG TRAIN Batch 22/6500 loss 5.014090 loss_att 4.539972 loss_ctc 6.120366 lr 0.00152641 rank 0\n",
            "2022-05-23 23:29:15,425 DEBUG TRAIN Batch 22/6600 loss 6.717477 loss_att 6.460651 loss_ctc 7.316737 lr 0.00152596 rank 0\n",
            "2022-05-23 23:29:44,726 DEBUG TRAIN Batch 22/6700 loss 6.323906 loss_att 6.221294 loss_ctc 6.563333 lr 0.00152552 rank 0\n",
            "2022-05-23 23:30:14,523 DEBUG TRAIN Batch 22/6800 loss 6.507877 loss_att 6.357909 loss_ctc 6.857801 lr 0.00152507 rank 0\n",
            "2022-05-23 23:30:44,399 DEBUG TRAIN Batch 22/6900 loss 8.485413 loss_att 7.940357 loss_ctc 9.757210 lr 0.00152463 rank 0\n",
            "2022-05-23 23:31:15,326 DEBUG TRAIN Batch 22/7000 loss 5.053828 loss_att 4.341662 loss_ctc 6.715548 lr 0.00152419 rank 0\n",
            "2022-05-23 23:31:44,695 DEBUG TRAIN Batch 22/7100 loss 9.091297 loss_att 8.063599 loss_ctc 11.489260 lr 0.00152375 rank 0\n",
            "2022-05-23 23:32:13,980 DEBUG TRAIN Batch 22/7200 loss 5.976445 loss_att 5.551542 loss_ctc 6.967886 lr 0.00152330 rank 0\n",
            "2022-05-23 23:32:43,830 DEBUG TRAIN Batch 22/7300 loss 4.334210 loss_att 3.946780 loss_ctc 5.238215 lr 0.00152286 rank 0\n",
            "2022-05-23 23:33:13,909 DEBUG TRAIN Batch 22/7400 loss 8.295073 loss_att 7.990761 loss_ctc 9.005132 lr 0.00152242 rank 0\n",
            "2022-05-23 23:33:44,060 DEBUG TRAIN Batch 22/7500 loss 7.835810 loss_att 6.595948 loss_ctc 10.728823 lr 0.00152198 rank 0\n",
            "2022-05-23 23:33:50,106 DEBUG CV Batch 22/0 loss 4.137341 loss_att 3.983361 loss_ctc 4.496629 history loss 3.893968 rank 0\n",
            "2022-05-23 23:34:01,374 DEBUG CV Batch 22/100 loss 2.950355 loss_att 2.794122 loss_ctc 3.314897 history loss 5.363378 rank 0\n",
            "2022-05-23 23:34:11,830 DEBUG CV Batch 22/200 loss 5.123296 loss_att 4.874396 loss_ctc 5.704061 history loss 5.384529 rank 0\n",
            "2022-05-23 23:34:22,766 DEBUG CV Batch 22/300 loss 3.663891 loss_att 3.674724 loss_ctc 3.638613 history loss 5.052967 rank 0\n",
            "2022-05-23 23:34:34,284 DEBUG CV Batch 22/400 loss 8.327941 loss_att 7.879702 loss_ctc 9.373832 history loss 4.677210 rank 0\n",
            "2022-05-23 23:34:46,394 DEBUG CV Batch 22/500 loss 2.557771 loss_att 2.609813 loss_ctc 2.436339 history loss 4.629810 rank 0\n",
            "2022-05-23 23:34:57,957 DEBUG CV Batch 22/600 loss 3.157071 loss_att 3.148686 loss_ctc 3.176638 history loss 4.594023 rank 0\n",
            "2022-05-23 23:35:08,881 DEBUG CV Batch 22/700 loss 3.462708 loss_att 3.250971 loss_ctc 3.956761 history loss 4.489919 rank 0\n",
            "2022-05-23 23:35:20,203 DEBUG CV Batch 22/800 loss 4.234729 loss_att 4.131379 loss_ctc 4.475880 history loss 4.452222 rank 0\n",
            "2022-05-23 23:35:31,857 INFO Epoch 22 CV info cv_loss 4.458601014632577\n",
            "2022-05-23 23:35:31,858 INFO Checkpoint: save to checkpoint exp/conformer/22.pt\n",
            "2022-05-23 23:35:32,190 INFO Epoch 23 TRAIN info lr 0.0015219624782735351\n",
            "2022-05-23 23:35:32,192 INFO using accumulate grad, new batch size is 4 times larger than before\n",
            "2022-05-23 23:35:57,385 DEBUG TRAIN Batch 23/0 loss 6.457026 loss_att 5.377132 loss_ctc 8.976779 lr 0.00152194 rank 0\n",
            "2022-05-23 23:36:27,305 DEBUG TRAIN Batch 23/100 loss 3.483222 loss_att 3.346291 loss_ctc 3.802730 lr 0.00152150 rank 0\n",
            "2022-05-23 23:36:56,951 DEBUG TRAIN Batch 23/200 loss 5.136374 loss_att 4.750662 loss_ctc 6.036368 lr 0.00152106 rank 0\n",
            "2022-05-23 23:37:26,513 DEBUG TRAIN Batch 23/300 loss 7.992654 loss_att 6.989926 loss_ctc 10.332351 lr 0.00152062 rank 0\n",
            "2022-05-23 23:37:56,237 DEBUG TRAIN Batch 23/400 loss 7.304304 loss_att 7.033265 loss_ctc 7.936730 lr 0.00152019 rank 0\n",
            "2022-05-23 23:38:27,010 DEBUG TRAIN Batch 23/500 loss 6.359093 loss_att 5.418109 loss_ctc 8.554722 lr 0.00151975 rank 0\n",
            "2022-05-23 23:38:56,459 DEBUG TRAIN Batch 23/600 loss 5.121330 loss_att 4.643421 loss_ctc 6.236452 lr 0.00151931 rank 0\n",
            "2022-05-23 23:39:26,103 DEBUG TRAIN Batch 23/700 loss 5.433189 loss_att 5.035460 loss_ctc 6.361224 lr 0.00151887 rank 0\n",
            "2022-05-23 23:39:56,094 DEBUG TRAIN Batch 23/800 loss 4.888312 loss_att 4.556476 loss_ctc 5.662598 lr 0.00151843 rank 0\n",
            "2022-05-23 23:40:26,143 DEBUG TRAIN Batch 23/900 loss 9.463231 loss_att 8.632038 loss_ctc 11.402680 lr 0.00151799 rank 0\n",
            "2022-05-23 23:40:56,810 DEBUG TRAIN Batch 23/1000 loss 5.289206 loss_att 5.121425 loss_ctc 5.680696 lr 0.00151756 rank 0\n",
            "2022-05-23 23:41:26,755 DEBUG TRAIN Batch 23/1100 loss 7.921884 loss_att 7.246262 loss_ctc 9.498335 lr 0.00151712 rank 0\n",
            "2022-05-23 23:41:55,992 DEBUG TRAIN Batch 23/1200 loss 9.157974 loss_att 8.172283 loss_ctc 11.457921 lr 0.00151668 rank 0\n",
            "2022-05-23 23:42:26,081 DEBUG TRAIN Batch 23/1300 loss 6.097026 loss_att 5.389369 loss_ctc 7.748226 lr 0.00151625 rank 0\n",
            "2022-05-23 23:42:56,124 DEBUG TRAIN Batch 23/1400 loss 8.324578 loss_att 7.303144 loss_ctc 10.707924 lr 0.00151581 rank 0\n",
            "2022-05-23 23:43:27,046 DEBUG TRAIN Batch 23/1500 loss 6.623666 loss_att 5.464999 loss_ctc 9.327223 lr 0.00151538 rank 0\n",
            "2022-05-23 23:43:56,314 DEBUG TRAIN Batch 23/1600 loss 6.347926 loss_att 5.866060 loss_ctc 7.472281 lr 0.00151494 rank 0\n",
            "2022-05-23 23:44:26,167 DEBUG TRAIN Batch 23/1700 loss 5.828652 loss_att 5.407191 loss_ctc 6.812062 lr 0.00151451 rank 0\n",
            "2022-05-23 23:44:55,987 DEBUG TRAIN Batch 23/1800 loss 7.095719 loss_att 6.596887 loss_ctc 8.259660 lr 0.00151407 rank 0\n",
            "2022-05-23 23:45:25,778 DEBUG TRAIN Batch 23/1900 loss 5.099528 loss_att 4.517564 loss_ctc 6.457444 lr 0.00151364 rank 0\n",
            "2022-05-23 23:45:56,425 DEBUG TRAIN Batch 23/2000 loss 5.588662 loss_att 5.119339 loss_ctc 6.683749 lr 0.00151321 rank 0\n",
            "2022-05-23 23:46:25,998 DEBUG TRAIN Batch 23/2100 loss 4.174359 loss_att 3.601384 loss_ctc 5.511301 lr 0.00151277 rank 0\n",
            "2022-05-23 23:46:55,176 DEBUG TRAIN Batch 23/2200 loss 5.398976 loss_att 4.639216 loss_ctc 7.171749 lr 0.00151234 rank 0\n",
            "2022-05-23 23:47:25,043 DEBUG TRAIN Batch 23/2300 loss 12.553183 loss_att 11.030618 loss_ctc 16.105835 lr 0.00151191 rank 0\n",
            "2022-05-23 23:47:55,284 DEBUG TRAIN Batch 23/2400 loss 7.238141 loss_att 6.610602 loss_ctc 8.702397 lr 0.00151148 rank 0\n",
            "2022-05-23 23:48:26,358 DEBUG TRAIN Batch 23/2500 loss 5.545153 loss_att 4.943155 loss_ctc 6.949814 lr 0.00151105 rank 0\n",
            "2022-05-23 23:48:55,655 DEBUG TRAIN Batch 23/2600 loss 6.082694 loss_att 5.685628 loss_ctc 7.009182 lr 0.00151062 rank 0\n",
            "2022-05-23 23:49:24,960 DEBUG TRAIN Batch 23/2700 loss 3.631463 loss_att 3.252536 loss_ctc 4.515624 lr 0.00151018 rank 0\n",
            "2022-05-23 23:49:54,587 DEBUG TRAIN Batch 23/2800 loss 5.461144 loss_att 5.159835 loss_ctc 6.164198 lr 0.00150975 rank 0\n",
            "2022-05-23 23:50:24,639 DEBUG TRAIN Batch 23/2900 loss 6.557220 loss_att 6.115703 loss_ctc 7.587427 lr 0.00150932 rank 0\n",
            "2022-05-23 23:50:55,226 DEBUG TRAIN Batch 23/3000 loss 5.803613 loss_att 5.502696 loss_ctc 6.505754 lr 0.00150889 rank 0\n",
            "2022-05-23 23:51:24,369 DEBUG TRAIN Batch 23/3100 loss 5.598178 loss_att 5.271774 loss_ctc 6.359787 lr 0.00150847 rank 0\n",
            "2022-05-23 23:51:54,160 DEBUG TRAIN Batch 23/3200 loss 7.145048 loss_att 6.651665 loss_ctc 8.296275 lr 0.00150804 rank 0\n",
            "2022-05-23 23:52:23,869 DEBUG TRAIN Batch 23/3300 loss 7.767713 loss_att 7.334031 loss_ctc 8.779638 lr 0.00150761 rank 0\n",
            "2022-05-23 23:52:54,090 DEBUG TRAIN Batch 23/3400 loss 6.821660 loss_att 6.568706 loss_ctc 7.411888 lr 0.00150718 rank 0\n",
            "2022-05-23 23:53:25,302 DEBUG TRAIN Batch 23/3500 loss 6.388945 loss_att 5.369454 loss_ctc 8.767754 lr 0.00150675 rank 0\n",
            "2022-05-23 23:53:54,915 DEBUG TRAIN Batch 23/3600 loss 4.685748 loss_att 4.189588 loss_ctc 5.843456 lr 0.00150632 rank 0\n",
            "2022-05-23 23:54:24,356 DEBUG TRAIN Batch 23/3700 loss 6.377977 loss_att 5.328221 loss_ctc 8.827407 lr 0.00150590 rank 0\n",
            "2022-05-23 23:54:54,051 DEBUG TRAIN Batch 23/3800 loss 6.615689 loss_att 6.027440 loss_ctc 7.988270 lr 0.00150547 rank 0\n",
            "2022-05-23 23:55:24,271 DEBUG TRAIN Batch 23/3900 loss 8.052539 loss_att 7.529784 loss_ctc 9.272300 lr 0.00150504 rank 0\n",
            "2022-05-23 23:55:55,010 DEBUG TRAIN Batch 23/4000 loss 5.097177 loss_att 4.399749 loss_ctc 6.724507 lr 0.00150462 rank 0\n",
            "2022-05-23 23:56:24,064 DEBUG TRAIN Batch 23/4100 loss 5.964795 loss_att 5.141242 loss_ctc 7.886419 lr 0.00150419 rank 0\n",
            "2022-05-23 23:56:53,514 DEBUG TRAIN Batch 23/4200 loss 7.502090 loss_att 6.796333 loss_ctc 9.148856 lr 0.00150377 rank 0\n",
            "2022-05-23 23:57:23,395 DEBUG TRAIN Batch 23/4300 loss 9.120623 loss_att 8.080296 loss_ctc 11.548055 lr 0.00150334 rank 0\n",
            "2022-05-23 23:57:53,370 DEBUG TRAIN Batch 23/4400 loss 8.136358 loss_att 7.786070 loss_ctc 8.953697 lr 0.00150292 rank 0\n",
            "2022-05-23 23:58:24,278 DEBUG TRAIN Batch 23/4500 loss 7.431192 loss_att 6.157959 loss_ctc 10.402069 lr 0.00150249 rank 0\n",
            "2022-05-23 23:58:53,742 DEBUG TRAIN Batch 23/4600 loss 7.045026 loss_att 6.024001 loss_ctc 9.427420 lr 0.00150207 rank 0\n",
            "2022-05-23 23:59:22,980 DEBUG TRAIN Batch 23/4700 loss 8.565419 loss_att 7.578599 loss_ctc 10.867998 lr 0.00150165 rank 0\n",
            "2022-05-23 23:59:52,689 DEBUG TRAIN Batch 23/4800 loss 5.734393 loss_att 5.226683 loss_ctc 6.919048 lr 0.00150122 rank 0\n",
            "2022-05-24 00:00:22,982 DEBUG TRAIN Batch 23/4900 loss 10.874963 loss_att 10.165369 loss_ctc 12.530681 lr 0.00150080 rank 0\n",
            "2022-05-24 00:00:53,680 DEBUG TRAIN Batch 23/5000 loss 4.697078 loss_att 4.659723 loss_ctc 4.784239 lr 0.00150038 rank 0\n",
            "2022-05-24 00:01:23,092 DEBUG TRAIN Batch 23/5100 loss 3.895032 loss_att 3.393857 loss_ctc 5.064440 lr 0.00149996 rank 0\n",
            "2022-05-24 00:01:52,573 DEBUG TRAIN Batch 23/5200 loss 6.550159 loss_att 6.288354 loss_ctc 7.161036 lr 0.00149954 rank 0\n",
            "2022-05-24 00:02:22,092 DEBUG TRAIN Batch 23/5300 loss 8.043427 loss_att 7.451018 loss_ctc 9.425713 lr 0.00149911 rank 0\n",
            "2022-05-24 00:02:52,100 DEBUG TRAIN Batch 23/5400 loss 5.687497 loss_att 5.171664 loss_ctc 6.891108 lr 0.00149869 rank 0\n",
            "2022-05-24 00:03:22,483 DEBUG TRAIN Batch 23/5500 loss 5.549692 loss_att 4.387622 loss_ctc 8.261189 lr 0.00149827 rank 0\n",
            "2022-05-24 00:03:51,750 DEBUG TRAIN Batch 23/5600 loss 4.634543 loss_att 4.102764 loss_ctc 5.875362 lr 0.00149785 rank 0\n",
            "2022-05-24 00:04:21,239 DEBUG TRAIN Batch 23/5700 loss 6.754985 loss_att 6.007450 loss_ctc 8.499232 lr 0.00149743 rank 0\n",
            "2022-05-24 00:04:51,245 DEBUG TRAIN Batch 23/5800 loss 6.673811 loss_att 6.402715 loss_ctc 7.306371 lr 0.00149701 rank 0\n",
            "2022-05-24 00:05:20,858 DEBUG TRAIN Batch 23/5900 loss 7.970663 loss_att 7.397397 loss_ctc 9.308285 lr 0.00149659 rank 0\n",
            "2022-05-24 00:05:51,582 DEBUG TRAIN Batch 23/6000 loss 5.107908 loss_att 4.856230 loss_ctc 5.695157 lr 0.00149617 rank 0\n",
            "2022-05-24 00:06:20,832 DEBUG TRAIN Batch 23/6100 loss 5.303208 loss_att 4.666512 loss_ctc 6.788831 lr 0.00149576 rank 0\n",
            "2022-05-24 00:06:50,856 DEBUG TRAIN Batch 23/6200 loss 8.101772 loss_att 7.168609 loss_ctc 10.279152 lr 0.00149534 rank 0\n",
            "2022-05-24 00:07:20,795 DEBUG TRAIN Batch 23/6300 loss 6.536537 loss_att 6.397795 loss_ctc 6.860267 lr 0.00149492 rank 0\n",
            "2022-05-24 00:07:50,744 DEBUG TRAIN Batch 23/6400 loss 4.642114 loss_att 4.594374 loss_ctc 4.753508 lr 0.00149450 rank 0\n",
            "2022-05-24 00:08:21,490 DEBUG TRAIN Batch 23/6500 loss 6.562156 loss_att 6.114101 loss_ctc 7.607616 lr 0.00149409 rank 0\n",
            "2022-05-24 00:08:51,090 DEBUG TRAIN Batch 23/6600 loss 3.521732 loss_att 3.317484 loss_ctc 3.998312 lr 0.00149367 rank 0\n",
            "2022-05-24 00:09:20,787 DEBUG TRAIN Batch 23/6700 loss 3.645983 loss_att 3.578620 loss_ctc 3.803164 lr 0.00149325 rank 0\n",
            "2022-05-24 00:09:51,083 DEBUG TRAIN Batch 23/6800 loss 5.320088 loss_att 4.813198 loss_ctc 6.502831 lr 0.00149284 rank 0\n",
            "2022-05-24 00:10:20,945 DEBUG TRAIN Batch 23/6900 loss 8.215706 loss_att 7.822248 loss_ctc 9.133772 lr 0.00149242 rank 0\n",
            "2022-05-24 00:10:51,831 DEBUG TRAIN Batch 23/7000 loss 6.686840 loss_att 5.879419 loss_ctc 8.570824 lr 0.00149201 rank 0\n",
            "2022-05-24 00:11:20,899 DEBUG TRAIN Batch 23/7100 loss 4.434244 loss_att 4.228824 loss_ctc 4.913558 lr 0.00149159 rank 0\n",
            "2022-05-24 00:11:50,683 DEBUG TRAIN Batch 23/7200 loss 4.511566 loss_att 3.908978 loss_ctc 5.917605 lr 0.00149118 rank 0\n",
            "2022-05-24 00:12:20,524 DEBUG TRAIN Batch 23/7300 loss 7.655655 loss_att 6.816113 loss_ctc 9.614584 lr 0.00149076 rank 0\n",
            "2022-05-24 00:12:50,618 DEBUG TRAIN Batch 23/7400 loss 7.651305 loss_att 7.703803 loss_ctc 7.528810 lr 0.00149035 rank 0\n",
            "2022-05-24 00:13:21,078 DEBUG TRAIN Batch 23/7500 loss 7.682865 loss_att 6.399049 loss_ctc 10.678434 lr 0.00148993 rank 0\n",
            "2022-05-24 00:13:26,960 DEBUG CV Batch 23/0 loss 4.551133 loss_att 4.015450 loss_ctc 5.801062 history loss 4.283419 rank 0\n",
            "2022-05-24 00:13:38,233 DEBUG CV Batch 23/100 loss 2.499090 loss_att 2.303002 loss_ctc 2.956630 history loss 5.553493 rank 0\n",
            "2022-05-24 00:13:48,759 DEBUG CV Batch 23/200 loss 4.856747 loss_att 4.141540 loss_ctc 6.525564 history loss 5.485388 rank 0\n",
            "2022-05-24 00:13:59,579 DEBUG CV Batch 23/300 loss 3.713759 loss_att 3.674156 loss_ctc 3.806166 history loss 5.110776 rank 0\n",
            "2022-05-24 00:14:11,159 DEBUG CV Batch 23/400 loss 8.935804 loss_att 8.249571 loss_ctc 10.537015 history loss 4.704600 rank 0\n",
            "2022-05-24 00:14:22,951 DEBUG CV Batch 23/500 loss 3.011250 loss_att 2.786153 loss_ctc 3.536476 history loss 4.631308 rank 0\n",
            "2022-05-24 00:14:34,381 DEBUG CV Batch 23/600 loss 3.127760 loss_att 3.053177 loss_ctc 3.301789 history loss 4.582109 rank 0\n",
            "2022-05-24 00:14:45,249 DEBUG CV Batch 23/700 loss 3.136541 loss_att 2.975743 loss_ctc 3.511737 history loss 4.485206 rank 0\n",
            "2022-05-24 00:14:56,352 DEBUG CV Batch 23/800 loss 4.092697 loss_att 3.795681 loss_ctc 4.785733 history loss 4.452139 rank 0\n",
            "2022-05-24 00:15:07,844 INFO Epoch 23 CV info cv_loss 4.445548136768299\n",
            "2022-05-24 00:15:07,844 INFO Checkpoint: save to checkpoint exp/conformer/23.pt\n",
            "2022-05-24 00:15:08,238 INFO Epoch 24 TRAIN info lr 0.0014899175740801342\n",
            "2022-05-24 00:15:08,240 INFO using accumulate grad, new batch size is 4 times larger than before\n",
            "2022-05-24 00:15:33,329 DEBUG TRAIN Batch 24/0 loss 5.898411 loss_att 4.901396 loss_ctc 8.224780 lr 0.00148990 rank 0\n",
            "2022-05-24 00:16:03,147 DEBUG TRAIN Batch 24/100 loss 6.376866 loss_att 5.470006 loss_ctc 8.492872 lr 0.00148949 rank 0\n",
            "2022-05-24 00:16:33,341 DEBUG TRAIN Batch 24/200 loss 4.250193 loss_att 3.949770 loss_ctc 4.951178 lr 0.00148907 rank 0\n",
            "2022-05-24 00:17:02,593 DEBUG TRAIN Batch 24/300 loss 8.034529 loss_att 7.255309 loss_ctc 9.852708 lr 0.00148866 rank 0\n",
            "2022-05-24 00:17:32,600 DEBUG TRAIN Batch 24/400 loss 8.307024 loss_att 7.783952 loss_ctc 9.527525 lr 0.00148825 rank 0\n",
            "2022-05-24 00:18:03,296 DEBUG TRAIN Batch 24/500 loss 4.782794 loss_att 4.012608 loss_ctc 6.579895 lr 0.00148784 rank 0\n",
            "2022-05-24 00:18:32,711 DEBUG TRAIN Batch 24/600 loss 5.039907 loss_att 4.042948 loss_ctc 7.366146 lr 0.00148743 rank 0\n",
            "2022-05-24 00:19:02,362 DEBUG TRAIN Batch 24/700 loss 6.214636 loss_att 5.520363 loss_ctc 7.834605 lr 0.00148702 rank 0\n",
            "2022-05-24 00:19:32,313 DEBUG TRAIN Batch 24/800 loss 5.428561 loss_att 5.039576 loss_ctc 6.336193 lr 0.00148660 rank 0\n",
            "2022-05-24 00:20:02,243 DEBUG TRAIN Batch 24/900 loss 7.722783 loss_att 7.047954 loss_ctc 9.297384 lr 0.00148619 rank 0\n",
            "2022-05-24 00:20:32,907 DEBUG TRAIN Batch 24/1000 loss 8.954433 loss_att 7.726949 loss_ctc 11.818563 lr 0.00148578 rank 0\n",
            "2022-05-24 00:21:02,373 DEBUG TRAIN Batch 24/1100 loss 4.786445 loss_att 4.183928 loss_ctc 6.192316 lr 0.00148537 rank 0\n",
            "2022-05-24 00:21:31,774 DEBUG TRAIN Batch 24/1200 loss 5.178754 loss_att 4.351777 loss_ctc 7.108366 lr 0.00148496 rank 0\n",
            "2022-05-24 00:22:01,557 DEBUG TRAIN Batch 24/1300 loss 6.283022 loss_att 6.263215 loss_ctc 6.329240 lr 0.00148456 rank 0\n",
            "2022-05-24 00:22:31,732 DEBUG TRAIN Batch 24/1400 loss 6.436570 loss_att 5.559870 loss_ctc 8.482203 lr 0.00148415 rank 0\n",
            "2022-05-24 00:23:02,429 DEBUG TRAIN Batch 24/1500 loss 5.735743 loss_att 4.845108 loss_ctc 7.813890 lr 0.00148374 rank 0\n",
            "2022-05-24 00:23:31,707 DEBUG TRAIN Batch 24/1600 loss 7.309693 loss_att 6.694387 loss_ctc 8.745406 lr 0.00148333 rank 0\n",
            "2022-05-24 00:24:01,097 DEBUG TRAIN Batch 24/1700 loss 7.804054 loss_att 6.819029 loss_ctc 10.102445 lr 0.00148292 rank 0\n",
            "2022-05-24 00:24:30,990 DEBUG TRAIN Batch 24/1800 loss 4.935908 loss_att 4.722843 loss_ctc 5.433061 lr 0.00148251 rank 0\n",
            "2022-05-24 00:25:01,476 DEBUG TRAIN Batch 24/1900 loss 9.191673 loss_att 8.753731 loss_ctc 10.213538 lr 0.00148211 rank 0\n",
            "2022-05-24 00:25:32,115 DEBUG TRAIN Batch 24/2000 loss 6.544291 loss_att 5.580018 loss_ctc 8.794261 lr 0.00148170 rank 0\n",
            "2022-05-24 00:26:01,019 DEBUG TRAIN Batch 24/2100 loss 6.429358 loss_att 5.553251 loss_ctc 8.473604 lr 0.00148129 rank 0\n",
            "2022-05-24 00:26:30,463 DEBUG TRAIN Batch 24/2200 loss 6.954724 loss_att 6.007848 loss_ctc 9.164101 lr 0.00148089 rank 0\n",
            "2022-05-24 00:26:59,782 DEBUG TRAIN Batch 24/2300 loss 6.397951 loss_att 6.297764 loss_ctc 6.631722 lr 0.00148048 rank 0\n",
            "2022-05-24 00:27:29,798 DEBUG TRAIN Batch 24/2400 loss 8.960701 loss_att 8.208460 loss_ctc 10.715930 lr 0.00148008 rank 0\n",
            "2022-05-24 00:28:00,422 DEBUG TRAIN Batch 24/2500 loss 9.150703 loss_att 7.834523 loss_ctc 12.221790 lr 0.00147967 rank 0\n",
            "2022-05-24 00:28:30,059 DEBUG TRAIN Batch 24/2600 loss 8.852418 loss_att 7.737674 loss_ctc 11.453489 lr 0.00147927 rank 0\n",
            "2022-05-24 00:28:59,644 DEBUG TRAIN Batch 24/2700 loss 9.084903 loss_att 7.519351 loss_ctc 12.737857 lr 0.00147886 rank 0\n",
            "2022-05-24 00:29:29,485 DEBUG TRAIN Batch 24/2800 loss 7.225867 loss_att 6.246822 loss_ctc 9.510305 lr 0.00147846 rank 0\n",
            "2022-05-24 00:29:59,364 DEBUG TRAIN Batch 24/2900 loss 7.083724 loss_att 6.575370 loss_ctc 8.269884 lr 0.00147805 rank 0\n",
            "2022-05-24 00:30:30,144 DEBUG TRAIN Batch 24/3000 loss 9.605710 loss_att 8.678028 loss_ctc 11.770300 lr 0.00147765 rank 0\n",
            "2022-05-24 00:30:59,571 DEBUG TRAIN Batch 24/3100 loss 6.923737 loss_att 5.977394 loss_ctc 9.131868 lr 0.00147725 rank 0\n",
            "2022-05-24 00:31:29,267 DEBUG TRAIN Batch 24/3200 loss 7.495084 loss_att 6.938090 loss_ctc 8.794735 lr 0.00147685 rank 0\n",
            "2022-05-24 00:31:59,298 DEBUG TRAIN Batch 24/3300 loss 5.005033 loss_att 4.679409 loss_ctc 5.764824 lr 0.00147644 rank 0\n",
            "2022-05-24 00:32:29,384 DEBUG TRAIN Batch 24/3400 loss 6.449672 loss_att 5.882278 loss_ctc 7.773592 lr 0.00147604 rank 0\n",
            "2022-05-24 00:32:59,999 DEBUG TRAIN Batch 24/3500 loss 8.393736 loss_att 7.123248 loss_ctc 11.358207 lr 0.00147564 rank 0\n",
            "2022-05-24 00:33:29,455 DEBUG TRAIN Batch 24/3600 loss 6.563888 loss_att 5.755452 loss_ctc 8.450237 lr 0.00147524 rank 0\n",
            "2022-05-24 00:33:59,048 DEBUG TRAIN Batch 24/3700 loss 6.332265 loss_att 5.271964 loss_ctc 8.806299 lr 0.00147484 rank 0\n",
            "2022-05-24 00:34:28,821 DEBUG TRAIN Batch 24/3800 loss 5.810110 loss_att 5.513495 loss_ctc 6.502210 lr 0.00147444 rank 0\n",
            "2022-05-24 00:34:59,152 DEBUG TRAIN Batch 24/3900 loss 8.558306 loss_att 7.911261 loss_ctc 10.068075 lr 0.00147404 rank 0\n",
            "2022-05-24 00:35:30,258 DEBUG TRAIN Batch 24/4000 loss 4.549585 loss_att 4.009104 loss_ctc 5.810707 lr 0.00147363 rank 0\n",
            "2022-05-24 00:35:59,914 DEBUG TRAIN Batch 24/4100 loss 4.523116 loss_att 4.207755 loss_ctc 5.258957 lr 0.00147324 rank 0\n",
            "2022-05-24 00:36:29,523 DEBUG TRAIN Batch 24/4200 loss 5.409283 loss_att 4.963362 loss_ctc 6.449767 lr 0.00147284 rank 0\n",
            "2022-05-24 00:36:59,555 DEBUG TRAIN Batch 24/4300 loss 7.292320 loss_att 6.627048 loss_ctc 8.844624 lr 0.00147244 rank 0\n",
            "2022-05-24 00:37:29,520 DEBUG TRAIN Batch 24/4400 loss 5.945828 loss_att 5.866660 loss_ctc 6.130554 lr 0.00147204 rank 0\n",
            "2022-05-24 00:38:00,074 DEBUG TRAIN Batch 24/4500 loss 5.882205 loss_att 5.171927 loss_ctc 7.539519 lr 0.00147164 rank 0\n",
            "2022-05-24 00:38:29,359 DEBUG TRAIN Batch 24/4600 loss 6.656848 loss_att 6.128650 loss_ctc 7.889311 lr 0.00147124 rank 0\n",
            "2022-05-24 00:38:58,998 DEBUG TRAIN Batch 24/4700 loss 5.932655 loss_att 5.544231 loss_ctc 6.838978 lr 0.00147084 rank 0\n",
            "2022-05-24 00:39:28,917 DEBUG TRAIN Batch 24/4800 loss 6.569266 loss_att 6.168762 loss_ctc 7.503776 lr 0.00147045 rank 0\n",
            "2022-05-24 00:39:59,040 DEBUG TRAIN Batch 24/4900 loss 5.496432 loss_att 5.565738 loss_ctc 5.334719 lr 0.00147005 rank 0\n",
            "2022-05-24 00:40:29,757 DEBUG TRAIN Batch 24/5000 loss 6.995242 loss_att 5.883230 loss_ctc 9.589938 lr 0.00146965 rank 0\n",
            "2022-05-24 00:40:58,717 DEBUG TRAIN Batch 24/5100 loss 4.822476 loss_att 4.167185 loss_ctc 6.351490 lr 0.00146925 rank 0\n",
            "2022-05-24 00:41:28,374 DEBUG TRAIN Batch 24/5200 loss 5.842187 loss_att 4.686670 loss_ctc 8.538392 lr 0.00146886 rank 0\n",
            "2022-05-24 00:41:57,840 DEBUG TRAIN Batch 24/5300 loss 7.754375 loss_att 6.908460 loss_ctc 9.728178 lr 0.00146846 rank 0\n",
            "2022-05-24 00:42:27,839 DEBUG TRAIN Batch 24/5400 loss 7.275343 loss_att 6.913202 loss_ctc 8.120339 lr 0.00146807 rank 0\n",
            "2022-05-24 00:42:58,753 DEBUG TRAIN Batch 24/5500 loss 6.204394 loss_att 5.528595 loss_ctc 7.781260 lr 0.00146767 rank 0\n",
            "2022-05-24 00:43:28,070 DEBUG TRAIN Batch 24/5600 loss 4.394979 loss_att 4.017747 loss_ctc 5.275186 lr 0.00146728 rank 0\n",
            "2022-05-24 00:43:57,813 DEBUG TRAIN Batch 24/5700 loss 6.225204 loss_att 5.616954 loss_ctc 7.644450 lr 0.00146688 rank 0\n",
            "2022-05-24 00:44:27,278 DEBUG TRAIN Batch 24/5800 loss 7.021753 loss_att 5.803304 loss_ctc 9.864800 lr 0.00146649 rank 0\n",
            "2022-05-24 00:44:57,538 DEBUG TRAIN Batch 24/5900 loss 5.405111 loss_att 4.961840 loss_ctc 6.439411 lr 0.00146609 rank 0\n",
            "2022-05-24 00:45:28,569 DEBUG TRAIN Batch 24/6000 loss 6.179008 loss_att 5.383088 loss_ctc 8.036156 lr 0.00146570 rank 0\n",
            "2022-05-24 00:45:58,147 DEBUG TRAIN Batch 24/6100 loss 6.068754 loss_att 5.732518 loss_ctc 6.853305 lr 0.00146531 rank 0\n",
            "2022-05-24 00:46:27,173 DEBUG TRAIN Batch 24/6200 loss 6.550830 loss_att 5.711693 loss_ctc 8.508816 lr 0.00146491 rank 0\n",
            "2022-05-24 00:46:56,906 DEBUG TRAIN Batch 24/6300 loss 7.878193 loss_att 7.004804 loss_ctc 9.916101 lr 0.00146452 rank 0\n",
            "2022-05-24 00:47:27,105 DEBUG TRAIN Batch 24/6400 loss 7.773796 loss_att 7.725432 loss_ctc 7.886646 lr 0.00146413 rank 0\n",
            "2022-05-24 00:47:57,739 DEBUG TRAIN Batch 24/6500 loss 5.146223 loss_att 4.490334 loss_ctc 6.676631 lr 0.00146374 rank 0\n",
            "2022-05-24 00:48:27,143 DEBUG TRAIN Batch 24/6600 loss 6.003082 loss_att 4.997529 loss_ctc 8.349375 lr 0.00146334 rank 0\n",
            "2022-05-24 00:48:56,868 DEBUG TRAIN Batch 24/6700 loss 4.661845 loss_att 4.511909 loss_ctc 5.011695 lr 0.00146295 rank 0\n",
            "2022-05-24 00:49:26,269 DEBUG TRAIN Batch 24/6800 loss 5.355274 loss_att 4.762286 loss_ctc 6.738914 lr 0.00146256 rank 0\n",
            "2022-05-24 00:49:56,332 DEBUG TRAIN Batch 24/6900 loss 6.215961 loss_att 5.934354 loss_ctc 6.873046 lr 0.00146217 rank 0\n",
            "2022-05-24 00:50:27,424 DEBUG TRAIN Batch 24/7000 loss 5.776942 loss_att 5.522820 loss_ctc 6.369893 lr 0.00146178 rank 0\n",
            "2022-05-24 00:50:57,110 DEBUG TRAIN Batch 24/7100 loss 7.227772 loss_att 6.247587 loss_ctc 9.514870 lr 0.00146139 rank 0\n",
            "2022-05-24 00:51:26,983 DEBUG TRAIN Batch 24/7200 loss 6.029041 loss_att 5.391525 loss_ctc 7.516579 lr 0.00146100 rank 0\n",
            "2022-05-24 00:51:56,665 DEBUG TRAIN Batch 24/7300 loss 5.369495 loss_att 5.192188 loss_ctc 5.783211 lr 0.00146061 rank 0\n",
            "2022-05-24 00:52:26,162 DEBUG TRAIN Batch 24/7400 loss 7.814606 loss_att 6.670643 loss_ctc 10.483854 lr 0.00146022 rank 0\n",
            "2022-05-24 00:52:56,241 DEBUG TRAIN Batch 24/7500 loss 5.944210 loss_att 4.981351 loss_ctc 8.190882 lr 0.00145983 rank 0\n",
            "2022-05-24 00:53:02,569 DEBUG CV Batch 24/0 loss 4.912708 loss_att 4.222516 loss_ctc 6.523155 history loss 4.623725 rank 0\n",
            "2022-05-24 00:53:14,223 DEBUG CV Batch 24/100 loss 2.685504 loss_att 2.619262 loss_ctc 2.840070 history loss 5.447822 rank 0\n",
            "2022-05-24 00:53:24,834 DEBUG CV Batch 24/200 loss 4.955677 loss_att 4.389215 loss_ctc 6.277422 history loss 5.421847 rank 0\n",
            "2022-05-24 00:53:36,013 DEBUG CV Batch 24/300 loss 3.449959 loss_att 3.425864 loss_ctc 3.506181 history loss 5.052784 rank 0\n",
            "2022-05-24 00:53:47,864 DEBUG CV Batch 24/400 loss 8.957143 loss_att 8.259071 loss_ctc 10.585978 history loss 4.650868 rank 0\n",
            "2022-05-24 00:54:00,226 DEBUG CV Batch 24/500 loss 2.544629 loss_att 2.630307 loss_ctc 2.344713 history loss 4.589836 rank 0\n",
            "2022-05-24 00:54:12,006 DEBUG CV Batch 24/600 loss 3.469362 loss_att 3.539969 loss_ctc 3.304610 history loss 4.540562 rank 0\n",
            "2022-05-24 00:54:23,242 DEBUG CV Batch 24/700 loss 3.238725 loss_att 3.027567 loss_ctc 3.731425 history loss 4.444352 rank 0\n",
            "2022-05-24 00:54:34,895 DEBUG CV Batch 24/800 loss 3.724323 loss_att 3.659407 loss_ctc 3.875795 history loss 4.427381 rank 0\n",
            "2022-05-24 00:54:46,633 INFO Epoch 24 CV info cv_loss 4.415556323880251\n",
            "2022-05-24 00:54:46,634 INFO Checkpoint: save to checkpoint exp/conformer/24.pt\n",
            "2022-05-24 00:54:47,001 INFO Epoch 25 TRAIN info lr 0.001459815126120674\n",
            "2022-05-24 00:54:47,003 INFO using accumulate grad, new batch size is 4 times larger than before\n",
            "2022-05-24 00:55:12,426 DEBUG TRAIN Batch 25/0 loss 4.166807 loss_att 3.712507 loss_ctc 5.226839 lr 0.00145980 rank 0\n",
            "2022-05-24 00:55:42,322 DEBUG TRAIN Batch 25/100 loss 7.345284 loss_att 6.967896 loss_ctc 8.225856 lr 0.00145941 rank 0\n",
            "2022-05-24 00:56:12,212 DEBUG TRAIN Batch 25/200 loss 4.254483 loss_att 4.052213 loss_ctc 4.726445 lr 0.00145902 rank 0\n",
            "2022-05-24 00:56:41,645 DEBUG TRAIN Batch 25/300 loss 4.005165 loss_att 3.913391 loss_ctc 4.219303 lr 0.00145863 rank 0\n",
            "2022-05-24 00:57:11,768 DEBUG TRAIN Batch 25/400 loss 7.362367 loss_att 6.789974 loss_ctc 8.697950 lr 0.00145825 rank 0\n",
            "2022-05-24 00:57:42,662 DEBUG TRAIN Batch 25/500 loss 7.880408 loss_att 7.264282 loss_ctc 9.318035 lr 0.00145786 rank 0\n",
            "2022-05-24 00:58:11,777 DEBUG TRAIN Batch 25/600 loss 6.128426 loss_att 5.131967 loss_ctc 8.453497 lr 0.00145747 rank 0\n",
            "2022-05-24 00:58:40,979 DEBUG TRAIN Batch 25/700 loss 5.107367 loss_att 4.334670 loss_ctc 6.910325 lr 0.00145709 rank 0\n",
            "2022-05-24 00:59:10,844 DEBUG TRAIN Batch 25/800 loss 5.004193 loss_att 4.611750 loss_ctc 5.919892 lr 0.00145670 rank 0\n",
            "2022-05-24 00:59:41,015 DEBUG TRAIN Batch 25/900 loss 7.110679 loss_att 6.732616 loss_ctc 7.992824 lr 0.00145631 rank 0\n",
            "2022-05-24 01:00:12,206 DEBUG TRAIN Batch 25/1000 loss 6.364083 loss_att 5.570762 loss_ctc 8.215168 lr 0.00145593 rank 0\n",
            "2022-05-24 01:00:41,826 DEBUG TRAIN Batch 25/1100 loss 4.252884 loss_att 3.604049 loss_ctc 5.766831 lr 0.00145554 rank 0\n",
            "2022-05-24 01:01:11,424 DEBUG TRAIN Batch 25/1200 loss 4.977903 loss_att 4.606275 loss_ctc 5.845037 lr 0.00145516 rank 0\n",
            "2022-05-24 01:01:41,351 DEBUG TRAIN Batch 25/1300 loss 5.854452 loss_att 5.063537 loss_ctc 7.699923 lr 0.00145477 rank 0\n",
            "2022-05-24 01:02:11,680 DEBUG TRAIN Batch 25/1400 loss 8.411302 loss_att 7.940554 loss_ctc 9.509712 lr 0.00145439 rank 0\n",
            "2022-05-24 01:02:42,203 DEBUG TRAIN Batch 25/1500 loss 4.639585 loss_att 3.997421 loss_ctc 6.137965 lr 0.00145400 rank 0\n",
            "2022-05-24 01:03:11,772 DEBUG TRAIN Batch 25/1600 loss 5.405653 loss_att 4.656721 loss_ctc 7.153160 lr 0.00145362 rank 0\n",
            "2022-05-24 01:03:41,215 DEBUG TRAIN Batch 25/1700 loss 5.824391 loss_att 4.929299 loss_ctc 7.912941 lr 0.00145323 rank 0\n",
            "2022-05-24 01:04:11,348 DEBUG TRAIN Batch 25/1800 loss 6.521490 loss_att 5.710520 loss_ctc 8.413754 lr 0.00145285 rank 0\n",
            "2022-05-24 01:04:41,147 DEBUG TRAIN Batch 25/1900 loss 5.969757 loss_att 5.713226 loss_ctc 6.568327 lr 0.00145247 rank 0\n",
            "2022-05-24 01:05:11,873 DEBUG TRAIN Batch 25/2000 loss 5.309172 loss_att 4.578001 loss_ctc 7.015236 lr 0.00145208 rank 0\n",
            "2022-05-24 01:05:41,498 DEBUG TRAIN Batch 25/2100 loss 4.856102 loss_att 4.211072 loss_ctc 6.361172 lr 0.00145170 rank 0\n",
            "2022-05-24 01:06:11,709 DEBUG TRAIN Batch 25/2200 loss 6.069595 loss_att 5.410494 loss_ctc 7.607496 lr 0.00145132 rank 0\n",
            "2022-05-24 01:06:41,579 DEBUG TRAIN Batch 25/2300 loss 5.729686 loss_att 5.346818 loss_ctc 6.623043 lr 0.00145094 rank 0\n",
            "2022-05-24 01:07:11,376 DEBUG TRAIN Batch 25/2400 loss 7.699175 loss_att 7.507427 loss_ctc 8.146587 lr 0.00145056 rank 0\n",
            "2022-05-24 01:07:42,377 DEBUG TRAIN Batch 25/2500 loss 5.720800 loss_att 4.690526 loss_ctc 8.124771 lr 0.00145017 rank 0\n",
            "2022-05-24 01:08:12,228 DEBUG TRAIN Batch 25/2600 loss 4.852517 loss_att 4.574120 loss_ctc 5.502112 lr 0.00144979 rank 0\n",
            "2022-05-24 01:08:41,440 DEBUG TRAIN Batch 25/2700 loss 7.141812 loss_att 6.579352 loss_ctc 8.454218 lr 0.00144941 rank 0\n",
            "2022-05-24 01:09:10,952 DEBUG TRAIN Batch 25/2800 loss 4.258781 loss_att 3.999515 loss_ctc 4.863736 lr 0.00144903 rank 0\n",
            "2022-05-24 01:09:41,086 DEBUG TRAIN Batch 25/2900 loss 6.063958 loss_att 5.767994 loss_ctc 6.754540 lr 0.00144865 rank 0\n",
            "2022-05-24 01:10:11,844 DEBUG TRAIN Batch 25/3000 loss 8.859416 loss_att 7.409045 loss_ctc 12.243616 lr 0.00144827 rank 0\n",
            "2022-05-24 01:10:41,454 DEBUG TRAIN Batch 25/3100 loss 7.008730 loss_att 5.962438 loss_ctc 9.450079 lr 0.00144789 rank 0\n",
            "2022-05-24 01:11:11,210 DEBUG TRAIN Batch 25/3200 loss 6.743249 loss_att 5.749545 loss_ctc 9.061892 lr 0.00144751 rank 0\n",
            "2022-05-24 01:11:40,812 DEBUG TRAIN Batch 25/3300 loss 5.373767 loss_att 5.219999 loss_ctc 5.732557 lr 0.00144713 rank 0\n",
            "2022-05-24 01:12:10,562 DEBUG TRAIN Batch 25/3400 loss 6.147114 loss_att 5.895601 loss_ctc 6.733976 lr 0.00144676 rank 0\n",
            "2022-05-24 01:12:41,081 DEBUG TRAIN Batch 25/3500 loss 5.225545 loss_att 4.736224 loss_ctc 6.367295 lr 0.00144638 rank 0\n",
            "2022-05-24 01:13:10,871 DEBUG TRAIN Batch 25/3600 loss 7.489594 loss_att 6.718962 loss_ctc 9.287736 lr 0.00144600 rank 0\n",
            "2022-05-24 01:13:40,357 DEBUG TRAIN Batch 25/3700 loss 3.802922 loss_att 3.324637 loss_ctc 4.918918 lr 0.00144562 rank 0\n",
            "2022-05-24 01:14:10,172 DEBUG TRAIN Batch 25/3800 loss 4.715876 loss_att 4.193507 loss_ctc 5.934736 lr 0.00144524 rank 0\n",
            "2022-05-24 01:14:40,204 DEBUG TRAIN Batch 25/3900 loss 6.799325 loss_att 6.111894 loss_ctc 8.403332 lr 0.00144487 rank 0\n",
            "2022-05-24 01:15:10,658 DEBUG TRAIN Batch 25/4000 loss 4.925314 loss_att 4.253107 loss_ctc 6.493798 lr 0.00144449 rank 0\n",
            "2022-05-24 01:15:40,060 DEBUG TRAIN Batch 25/4100 loss 5.416749 loss_att 5.336356 loss_ctc 5.604331 lr 0.00144411 rank 0\n",
            "2022-05-24 01:16:09,247 DEBUG TRAIN Batch 25/4200 loss 6.835556 loss_att 6.111415 loss_ctc 8.525219 lr 0.00144374 rank 0\n",
            "2022-05-24 01:16:39,385 DEBUG TRAIN Batch 25/4300 loss 5.988749 loss_att 5.095661 loss_ctc 8.072621 lr 0.00144336 rank 0\n",
            "2022-05-24 01:17:09,470 DEBUG TRAIN Batch 25/4400 loss 8.399213 loss_att 7.716142 loss_ctc 9.993047 lr 0.00144298 rank 0\n",
            "2022-05-24 01:17:40,133 DEBUG TRAIN Batch 25/4500 loss 2.701138 loss_att 2.673256 loss_ctc 2.766194 lr 0.00144261 rank 0\n",
            "2022-05-24 01:18:09,601 DEBUG TRAIN Batch 25/4600 loss 4.698383 loss_att 3.765104 loss_ctc 6.876035 lr 0.00144223 rank 0\n",
            "2022-05-24 01:18:38,901 DEBUG TRAIN Batch 25/4700 loss 4.423885 loss_att 4.034975 loss_ctc 5.331344 lr 0.00144186 rank 0\n",
            "2022-05-24 01:19:08,956 DEBUG TRAIN Batch 25/4800 loss 7.142573 loss_att 6.487188 loss_ctc 8.671805 lr 0.00144148 rank 0\n",
            "2022-05-24 01:19:39,096 DEBUG TRAIN Batch 25/4900 loss 5.119364 loss_att 4.996544 loss_ctc 5.405944 lr 0.00144111 rank 0\n",
            "2022-05-24 01:20:09,800 DEBUG TRAIN Batch 25/5000 loss 5.410695 loss_att 4.657880 loss_ctc 7.167264 lr 0.00144074 rank 0\n",
            "2022-05-24 01:20:39,551 DEBUG TRAIN Batch 25/5100 loss 6.325882 loss_att 5.634819 loss_ctc 7.938362 lr 0.00144036 rank 0\n",
            "2022-05-24 01:21:09,164 DEBUG TRAIN Batch 25/5200 loss 4.875442 loss_att 4.479250 loss_ctc 5.799891 lr 0.00143999 rank 0\n",
            "2022-05-24 01:21:38,957 DEBUG TRAIN Batch 25/5300 loss 6.370793 loss_att 5.666582 loss_ctc 8.013951 lr 0.00143962 rank 0\n",
            "2022-05-24 01:22:09,340 DEBUG TRAIN Batch 25/5400 loss 7.035333 loss_att 6.546842 loss_ctc 8.175144 lr 0.00143924 rank 0\n",
            "2022-05-24 01:22:40,120 DEBUG TRAIN Batch 25/5500 loss 7.050201 loss_att 6.338022 loss_ctc 8.711952 lr 0.00143887 rank 0\n",
            "2022-05-24 01:23:10,107 DEBUG TRAIN Batch 25/5600 loss 8.163836 loss_att 6.463978 loss_ctc 12.130172 lr 0.00143850 rank 0\n",
            "2022-05-24 01:23:39,858 DEBUG TRAIN Batch 25/5700 loss 5.551163 loss_att 5.124810 loss_ctc 6.545986 lr 0.00143813 rank 0\n",
            "2022-05-24 01:24:09,966 DEBUG TRAIN Batch 25/5800 loss 7.104869 loss_att 6.440942 loss_ctc 8.654032 lr 0.00143776 rank 0\n",
            "2022-05-24 01:24:40,322 DEBUG TRAIN Batch 25/5900 loss 6.335876 loss_att 5.556257 loss_ctc 8.154985 lr 0.00143738 rank 0\n",
            "2022-05-24 01:25:11,728 DEBUG TRAIN Batch 25/6000 loss 5.478054 loss_att 4.573426 loss_ctc 7.588852 lr 0.00143701 rank 0\n",
            "2022-05-24 01:25:40,951 DEBUG TRAIN Batch 25/6100 loss 5.487364 loss_att 4.832466 loss_ctc 7.015461 lr 0.00143664 rank 0\n",
            "2022-05-24 01:26:10,868 DEBUG TRAIN Batch 25/6200 loss 5.646524 loss_att 5.210251 loss_ctc 6.664496 lr 0.00143627 rank 0\n",
            "2022-05-24 01:26:41,033 DEBUG TRAIN Batch 25/6300 loss 5.667072 loss_att 5.341032 loss_ctc 6.427834 lr 0.00143590 rank 0\n",
            "2022-05-24 01:27:10,992 DEBUG TRAIN Batch 25/6400 loss 5.788214 loss_att 5.364004 loss_ctc 6.778035 lr 0.00143553 rank 0\n",
            "2022-05-24 01:27:41,932 DEBUG TRAIN Batch 25/6500 loss 5.764837 loss_att 4.876139 loss_ctc 7.838468 lr 0.00143516 rank 0\n",
            "2022-05-24 01:28:11,509 DEBUG TRAIN Batch 25/6600 loss 9.158049 loss_att 8.721497 loss_ctc 10.176668 lr 0.00143479 rank 0\n",
            "2022-05-24 01:28:40,825 DEBUG TRAIN Batch 25/6700 loss 7.000399 loss_att 6.131252 loss_ctc 9.028407 lr 0.00143442 rank 0\n",
            "2022-05-24 01:29:10,097 DEBUG TRAIN Batch 25/6800 loss 6.996210 loss_att 6.423928 loss_ctc 8.331534 lr 0.00143405 rank 0\n",
            "2022-05-24 01:29:40,170 DEBUG TRAIN Batch 25/6900 loss 6.055787 loss_att 5.748735 loss_ctc 6.772240 lr 0.00143369 rank 0\n",
            "2022-05-24 01:30:11,293 DEBUG TRAIN Batch 25/7000 loss 7.256726 loss_att 6.480819 loss_ctc 9.067176 lr 0.00143332 rank 0\n",
            "2022-05-24 01:30:40,766 DEBUG TRAIN Batch 25/7100 loss 5.176179 loss_att 4.439738 loss_ctc 6.894540 lr 0.00143295 rank 0\n",
            "2022-05-24 01:31:10,580 DEBUG TRAIN Batch 25/7200 loss 5.232742 loss_att 4.780596 loss_ctc 6.287749 lr 0.00143258 rank 0\n",
            "2022-05-24 01:31:40,066 DEBUG TRAIN Batch 25/7300 loss 6.974452 loss_att 6.414535 loss_ctc 8.280926 lr 0.00143222 rank 0\n",
            "2022-05-24 01:32:10,313 DEBUG TRAIN Batch 25/7400 loss 8.575447 loss_att 7.722685 loss_ctc 10.565224 lr 0.00143185 rank 0\n",
            "2022-05-24 01:32:41,341 DEBUG TRAIN Batch 25/7500 loss 6.972510 loss_att 6.324131 loss_ctc 8.485396 lr 0.00143148 rank 0\n",
            "2022-05-24 01:32:47,666 DEBUG CV Batch 25/0 loss 3.757268 loss_att 3.641877 loss_ctc 4.026512 history loss 3.536252 rank 0\n",
            "2022-05-24 01:32:59,396 DEBUG CV Batch 25/100 loss 2.573542 loss_att 2.472664 loss_ctc 2.808924 history loss 5.320909 rank 0\n",
            "2022-05-24 01:33:10,032 DEBUG CV Batch 25/200 loss 4.892863 loss_att 4.411271 loss_ctc 6.016579 history loss 5.256964 rank 0\n",
            "2022-05-24 01:33:21,153 DEBUG CV Batch 25/300 loss 3.562011 loss_att 3.581706 loss_ctc 3.516058 history loss 4.946446 rank 0\n",
            "2022-05-24 01:33:33,206 DEBUG CV Batch 25/400 loss 8.585819 loss_att 7.881976 loss_ctc 10.228119 history loss 4.577543 rank 0\n",
            "2022-05-24 01:33:45,539 DEBUG CV Batch 25/500 loss 2.643006 loss_att 2.525731 loss_ctc 2.916649 history loss 4.537795 rank 0\n",
            "2022-05-24 01:33:57,343 DEBUG CV Batch 25/600 loss 3.738594 loss_att 3.857220 loss_ctc 3.461798 history loss 4.492731 rank 0\n",
            "2022-05-24 01:34:08,592 DEBUG CV Batch 25/700 loss 2.912331 loss_att 2.685731 loss_ctc 3.441066 history loss 4.394228 rank 0\n",
            "2022-05-24 01:34:20,226 DEBUG CV Batch 25/800 loss 5.237403 loss_att 4.609933 loss_ctc 6.701499 history loss 4.367306 rank 0\n",
            "2022-05-24 01:34:31,922 INFO Epoch 25 CV info cv_loss 4.369176242509124\n",
            "2022-05-24 01:34:31,923 INFO Checkpoint: save to checkpoint exp/conformer/25.pt\n",
            "2022-05-24 01:34:32,271 INFO Epoch 26 TRAIN info lr 0.0014314665027552365\n",
            "2022-05-24 01:34:32,273 INFO using accumulate grad, new batch size is 4 times larger than before\n",
            "2022-05-24 01:34:57,453 DEBUG TRAIN Batch 26/0 loss 5.366150 loss_att 4.759890 loss_ctc 6.780758 lr 0.00143145 rank 0\n",
            "2022-05-24 01:35:27,645 DEBUG TRAIN Batch 26/100 loss 5.133965 loss_att 4.233963 loss_ctc 7.233973 lr 0.00143109 rank 0\n",
            "2022-05-24 01:35:57,753 DEBUG TRAIN Batch 26/200 loss 5.173348 loss_att 4.614010 loss_ctc 6.478469 lr 0.00143072 rank 0\n",
            "2022-05-24 01:36:26,962 DEBUG TRAIN Batch 26/300 loss 5.014818 loss_att 4.736807 loss_ctc 5.663510 lr 0.00143035 rank 0\n",
            "2022-05-24 01:36:57,042 DEBUG TRAIN Batch 26/400 loss 6.243075 loss_att 5.746693 loss_ctc 7.401301 lr 0.00142999 rank 0\n",
            "2022-05-24 01:37:28,149 DEBUG TRAIN Batch 26/500 loss 4.536835 loss_att 3.928398 loss_ctc 5.956520 lr 0.00142962 rank 0\n",
            "2022-05-24 01:37:57,182 DEBUG TRAIN Batch 26/600 loss 5.758245 loss_att 5.245009 loss_ctc 6.955794 lr 0.00142926 rank 0\n",
            "2022-05-24 01:38:26,611 DEBUG TRAIN Batch 26/700 loss 5.689915 loss_att 4.887640 loss_ctc 7.561891 lr 0.00142889 rank 0\n",
            "2022-05-24 01:38:56,271 DEBUG TRAIN Batch 26/800 loss 5.520160 loss_att 5.182839 loss_ctc 6.307240 lr 0.00142853 rank 0\n",
            "2022-05-24 01:39:26,233 DEBUG TRAIN Batch 26/900 loss 7.581425 loss_att 7.156104 loss_ctc 8.573839 lr 0.00142816 rank 0\n",
            "2022-05-24 01:39:57,221 DEBUG TRAIN Batch 26/1000 loss 6.372180 loss_att 5.689967 loss_ctc 7.964010 lr 0.00142780 rank 0\n",
            "2022-05-24 01:40:26,443 DEBUG TRAIN Batch 26/1100 loss 6.251391 loss_att 5.646542 loss_ctc 7.662706 lr 0.00142744 rank 0\n",
            "2022-05-24 01:40:56,285 DEBUG TRAIN Batch 26/1200 loss 7.995963 loss_att 6.817109 loss_ctc 10.746624 lr 0.00142707 rank 0\n",
            "2022-05-24 01:41:26,180 DEBUG TRAIN Batch 26/1300 loss 7.014156 loss_att 5.836904 loss_ctc 9.761080 lr 0.00142671 rank 0\n",
            "2022-05-24 01:41:56,382 DEBUG TRAIN Batch 26/1400 loss 5.013239 loss_att 5.262394 loss_ctc 4.431875 lr 0.00142635 rank 0\n",
            "2022-05-24 01:42:27,344 DEBUG TRAIN Batch 26/1500 loss 5.065302 loss_att 4.711545 loss_ctc 5.890737 lr 0.00142598 rank 0\n",
            "2022-05-24 01:42:56,612 DEBUG TRAIN Batch 26/1600 loss 6.709220 loss_att 5.904116 loss_ctc 8.587797 lr 0.00142562 rank 0\n",
            "2022-05-24 01:43:26,311 DEBUG TRAIN Batch 26/1700 loss 4.238432 loss_att 3.827903 loss_ctc 5.196333 lr 0.00142526 rank 0\n",
            "2022-05-24 01:43:56,111 DEBUG TRAIN Batch 26/1800 loss 5.853106 loss_att 5.544078 loss_ctc 6.574170 lr 0.00142490 rank 0\n",
            "2022-05-24 01:44:26,053 DEBUG TRAIN Batch 26/1900 loss 8.876087 loss_att 8.183072 loss_ctc 10.493123 lr 0.00142454 rank 0\n",
            "2022-05-24 01:44:56,784 DEBUG TRAIN Batch 26/2000 loss 5.652526 loss_att 4.522288 loss_ctc 8.289747 lr 0.00142417 rank 0\n",
            "2022-05-24 01:45:26,386 DEBUG TRAIN Batch 26/2100 loss 6.057128 loss_att 5.111061 loss_ctc 8.264618 lr 0.00142381 rank 0\n",
            "2022-05-24 01:45:56,022 DEBUG TRAIN Batch 26/2200 loss 6.319026 loss_att 5.652031 loss_ctc 7.875347 lr 0.00142345 rank 0\n",
            "2022-05-24 01:46:25,752 DEBUG TRAIN Batch 26/2300 loss 5.645847 loss_att 5.168303 loss_ctc 6.760115 lr 0.00142309 rank 0\n",
            "2022-05-24 01:46:56,029 DEBUG TRAIN Batch 26/2400 loss 5.453896 loss_att 5.357604 loss_ctc 5.678579 lr 0.00142273 rank 0\n",
            "2022-05-24 01:47:26,640 DEBUG TRAIN Batch 26/2500 loss 6.887602 loss_att 5.993844 loss_ctc 8.973039 lr 0.00142237 rank 0\n",
            "2022-05-24 01:47:55,644 DEBUG TRAIN Batch 26/2600 loss 4.634034 loss_att 4.181174 loss_ctc 5.690706 lr 0.00142201 rank 0\n",
            "2022-05-24 01:48:25,567 DEBUG TRAIN Batch 26/2700 loss 7.521001 loss_att 7.088824 loss_ctc 8.529415 lr 0.00142165 rank 0\n",
            "2022-05-24 01:48:54,817 DEBUG TRAIN Batch 26/2800 loss 6.181294 loss_att 5.663589 loss_ctc 7.389274 lr 0.00142130 rank 0\n",
            "2022-05-24 01:49:24,903 DEBUG TRAIN Batch 26/2900 loss 5.647306 loss_att 5.123944 loss_ctc 6.868484 lr 0.00142094 rank 0\n",
            "2022-05-24 01:49:55,737 DEBUG TRAIN Batch 26/3000 loss 6.461163 loss_att 5.211519 loss_ctc 9.376997 lr 0.00142058 rank 0\n",
            "2022-05-24 01:50:25,284 DEBUG TRAIN Batch 26/3100 loss 4.817344 loss_att 4.421851 loss_ctc 5.740162 lr 0.00142022 rank 0\n",
            "2022-05-24 01:50:55,338 DEBUG TRAIN Batch 26/3200 loss 6.676003 loss_att 5.253099 loss_ctc 9.996111 lr 0.00141986 rank 0\n",
            "2022-05-24 01:51:25,161 DEBUG TRAIN Batch 26/3300 loss 6.594580 loss_att 5.696422 loss_ctc 8.690282 lr 0.00141950 rank 0\n",
            "2022-05-24 01:51:55,015 DEBUG TRAIN Batch 26/3400 loss 4.826968 loss_att 4.755728 loss_ctc 4.993196 lr 0.00141915 rank 0\n",
            "2022-05-24 01:52:26,115 DEBUG TRAIN Batch 26/3500 loss 8.908751 loss_att 7.856651 loss_ctc 11.363651 lr 0.00141879 rank 0\n",
            "2022-05-24 01:52:55,987 DEBUG TRAIN Batch 26/3600 loss 3.517508 loss_att 3.180739 loss_ctc 4.303303 lr 0.00141843 rank 0\n",
            "2022-05-24 01:53:25,489 DEBUG TRAIN Batch 26/3700 loss 7.154128 loss_att 6.264163 loss_ctc 9.230713 lr 0.00141808 rank 0\n",
            "2022-05-24 01:53:55,405 DEBUG TRAIN Batch 26/3800 loss 6.759185 loss_att 6.343589 loss_ctc 7.728909 lr 0.00141772 rank 0\n",
            "2022-05-24 01:54:25,139 DEBUG TRAIN Batch 26/3900 loss 6.209050 loss_att 5.553189 loss_ctc 7.739390 lr 0.00141736 rank 0\n",
            "2022-05-24 01:54:56,261 DEBUG TRAIN Batch 26/4000 loss 7.669029 loss_att 6.708813 loss_ctc 9.909531 lr 0.00141701 rank 0\n",
            "2022-05-24 01:55:25,727 DEBUG TRAIN Batch 26/4100 loss 5.905467 loss_att 5.385207 loss_ctc 7.119407 lr 0.00141665 rank 0\n",
            "2022-05-24 01:55:55,066 DEBUG TRAIN Batch 26/4200 loss 3.266231 loss_att 3.031292 loss_ctc 3.814420 lr 0.00141630 rank 0\n",
            "2022-05-24 01:56:24,996 DEBUG TRAIN Batch 26/4300 loss 6.062367 loss_att 5.472975 loss_ctc 7.437616 lr 0.00141594 rank 0\n",
            "2022-05-24 01:56:55,230 DEBUG TRAIN Batch 26/4400 loss 6.629883 loss_att 6.424888 loss_ctc 7.108204 lr 0.00141559 rank 0\n",
            "2022-05-24 01:57:26,212 DEBUG TRAIN Batch 26/4500 loss 3.940383 loss_att 3.515066 loss_ctc 4.932788 lr 0.00141523 rank 0\n",
            "2022-05-24 01:57:55,631 DEBUG TRAIN Batch 26/4600 loss 5.075762 loss_att 4.223814 loss_ctc 7.063641 lr 0.00141488 rank 0\n",
            "2022-05-24 01:58:25,271 DEBUG TRAIN Batch 26/4700 loss 5.491506 loss_att 4.704496 loss_ctc 7.327861 lr 0.00141452 rank 0\n",
            "2022-05-24 01:58:55,569 DEBUG TRAIN Batch 26/4800 loss 6.149863 loss_att 5.851662 loss_ctc 6.845667 lr 0.00141417 rank 0\n",
            "2022-05-24 01:59:25,469 DEBUG TRAIN Batch 26/4900 loss 5.670256 loss_att 5.321919 loss_ctc 6.483043 lr 0.00141382 rank 0\n",
            "2022-05-24 01:59:56,217 DEBUG TRAIN Batch 26/5000 loss 5.272029 loss_att 4.948857 loss_ctc 6.026096 lr 0.00141346 rank 0\n",
            "2022-05-24 02:00:25,482 DEBUG TRAIN Batch 26/5100 loss 7.010477 loss_att 6.110914 loss_ctc 9.109457 lr 0.00141311 rank 0\n",
            "2022-05-24 02:00:55,315 DEBUG TRAIN Batch 26/5200 loss 5.699062 loss_att 5.271626 loss_ctc 6.696412 lr 0.00141276 rank 0\n",
            "2022-05-24 02:01:25,097 DEBUG TRAIN Batch 26/5300 loss 3.727562 loss_att 3.486462 loss_ctc 4.290127 lr 0.00141241 rank 0\n",
            "2022-05-24 02:01:55,258 DEBUG TRAIN Batch 26/5400 loss 6.106604 loss_att 5.448082 loss_ctc 7.643154 lr 0.00141205 rank 0\n",
            "2022-05-24 02:02:25,694 DEBUG TRAIN Batch 26/5500 loss 5.893725 loss_att 4.807436 loss_ctc 8.428402 lr 0.00141170 rank 0\n",
            "2022-05-24 02:02:54,930 DEBUG TRAIN Batch 26/5600 loss 4.871669 loss_att 4.416642 loss_ctc 5.933400 lr 0.00141135 rank 0\n",
            "2022-05-24 02:03:24,520 DEBUG TRAIN Batch 26/5700 loss 6.135225 loss_att 5.765078 loss_ctc 6.998902 lr 0.00141100 rank 0\n",
            "2022-05-24 02:03:53,884 DEBUG TRAIN Batch 26/5800 loss 5.732700 loss_att 5.183451 loss_ctc 7.014283 lr 0.00141065 rank 0\n",
            "2022-05-24 02:04:24,304 DEBUG TRAIN Batch 26/5900 loss 6.364903 loss_att 5.825364 loss_ctc 7.623827 lr 0.00141030 rank 0\n",
            "2022-05-24 02:04:54,674 DEBUG TRAIN Batch 26/6000 loss 4.800810 loss_att 4.243242 loss_ctc 6.101801 lr 0.00140995 rank 0\n",
            "2022-05-24 02:05:23,927 DEBUG TRAIN Batch 26/6100 loss 3.950666 loss_att 3.436961 loss_ctc 5.149312 lr 0.00140960 rank 0\n",
            "2022-05-24 02:05:53,861 DEBUG TRAIN Batch 26/6200 loss 5.752453 loss_att 4.948001 loss_ctc 7.629508 lr 0.00140925 rank 0\n",
            "2022-05-24 02:06:23,767 DEBUG TRAIN Batch 26/6300 loss 4.937552 loss_att 4.589230 loss_ctc 5.750306 lr 0.00140890 rank 0\n",
            "2022-05-24 02:06:53,892 DEBUG TRAIN Batch 26/6400 loss 9.673604 loss_att 8.382829 loss_ctc 12.685413 lr 0.00140855 rank 0\n",
            "2022-05-24 02:07:24,705 DEBUG TRAIN Batch 26/6500 loss 5.522866 loss_att 4.840718 loss_ctc 7.114544 lr 0.00140820 rank 0\n",
            "2022-05-24 02:07:54,008 DEBUG TRAIN Batch 26/6600 loss 5.276395 loss_att 4.346689 loss_ctc 7.445707 lr 0.00140785 rank 0\n",
            "2022-05-24 02:08:23,607 DEBUG TRAIN Batch 26/6700 loss 4.024770 loss_att 3.924788 loss_ctc 4.258061 lr 0.00140750 rank 0\n",
            "2022-05-24 02:08:53,572 DEBUG TRAIN Batch 26/6800 loss 7.546387 loss_att 6.810483 loss_ctc 9.263494 lr 0.00140715 rank 0\n",
            "2022-05-24 02:09:23,399 DEBUG TRAIN Batch 26/6900 loss 6.528955 loss_att 6.082367 loss_ctc 7.570996 lr 0.00140681 rank 0\n",
            "2022-05-24 02:09:54,277 DEBUG TRAIN Batch 26/7000 loss 4.068799 loss_att 3.560434 loss_ctc 5.254981 lr 0.00140646 rank 0\n",
            "2022-05-24 02:10:23,668 DEBUG TRAIN Batch 26/7100 loss 5.470463 loss_att 5.136987 loss_ctc 6.248573 lr 0.00140611 rank 0\n",
            "2022-05-24 02:10:53,539 DEBUG TRAIN Batch 26/7200 loss 5.171386 loss_att 4.760615 loss_ctc 6.129851 lr 0.00140576 rank 0\n",
            "2022-05-24 02:11:22,870 DEBUG TRAIN Batch 26/7300 loss 7.563001 loss_att 6.654891 loss_ctc 9.681926 lr 0.00140542 rank 0\n",
            "2022-05-24 02:11:53,254 DEBUG TRAIN Batch 26/7400 loss 6.242263 loss_att 5.902462 loss_ctc 7.035132 lr 0.00140507 rank 0\n",
            "2022-05-24 02:12:24,167 DEBUG TRAIN Batch 26/7500 loss 6.044868 loss_att 5.091913 loss_ctc 8.268427 lr 0.00140472 rank 0\n",
            "2022-05-24 02:12:30,405 DEBUG CV Batch 26/0 loss 3.680671 loss_att 3.279245 loss_ctc 4.617331 history loss 3.464161 rank 0\n",
            "2022-05-24 02:12:42,094 DEBUG CV Batch 26/100 loss 2.794497 loss_att 2.430330 loss_ctc 3.644219 history loss 5.356030 rank 0\n",
            "2022-05-24 02:12:52,649 DEBUG CV Batch 26/200 loss 4.590757 loss_att 4.320528 loss_ctc 5.221291 history loss 5.276880 rank 0\n",
            "2022-05-24 02:13:03,829 DEBUG CV Batch 26/300 loss 3.931876 loss_att 3.974722 loss_ctc 3.831901 history loss 4.968532 rank 0\n",
            "2022-05-24 02:13:15,950 DEBUG CV Batch 26/400 loss 9.081031 loss_att 7.985258 loss_ctc 11.637835 history loss 4.596851 rank 0\n",
            "2022-05-24 02:13:28,294 DEBUG CV Batch 26/500 loss 2.711626 loss_att 2.718472 loss_ctc 2.695652 history loss 4.555485 rank 0\n",
            "2022-05-24 02:13:40,306 DEBUG CV Batch 26/600 loss 2.932876 loss_att 2.976103 loss_ctc 2.832012 history loss 4.496395 rank 0\n",
            "2022-05-24 02:13:51,398 DEBUG CV Batch 26/700 loss 3.336659 loss_att 3.023619 loss_ctc 4.067085 history loss 4.395002 rank 0\n",
            "2022-05-24 02:14:03,143 DEBUG CV Batch 26/800 loss 4.081241 loss_att 3.709307 loss_ctc 4.949088 history loss 4.368607 rank 0\n",
            "2022-05-24 02:14:14,748 INFO Epoch 26 CV info cv_loss 4.363499396459884\n",
            "2022-05-24 02:14:14,748 INFO Checkpoint: save to checkpoint exp/conformer/26.pt\n",
            "2022-05-24 02:14:15,087 INFO Epoch 27 TRAIN info lr 0.0014047077600547644\n",
            "2022-05-24 02:14:15,089 INFO using accumulate grad, new batch size is 4 times larger than before\n",
            "2022-05-24 02:14:41,117 DEBUG TRAIN Batch 27/0 loss 5.669930 loss_att 5.101437 loss_ctc 6.996412 lr 0.00140469 rank 0\n",
            "2022-05-24 02:15:11,595 DEBUG TRAIN Batch 27/100 loss 5.328523 loss_att 4.666907 loss_ctc 6.872293 lr 0.00140435 rank 0\n",
            "2022-05-24 02:15:41,108 DEBUG TRAIN Batch 27/200 loss 4.262013 loss_att 3.740308 loss_ctc 5.479326 lr 0.00140400 rank 0\n",
            "2022-05-24 02:16:10,618 DEBUG TRAIN Batch 27/300 loss 4.174770 loss_att 4.074917 loss_ctc 4.407762 lr 0.00140366 rank 0\n",
            "2022-05-24 02:16:40,733 DEBUG TRAIN Batch 27/400 loss 6.360954 loss_att 5.957304 loss_ctc 7.302805 lr 0.00140331 rank 0\n",
            "2022-05-24 02:17:11,802 DEBUG TRAIN Batch 27/500 loss 4.105256 loss_att 3.662576 loss_ctc 5.138177 lr 0.00140296 rank 0\n",
            "2022-05-24 02:17:40,726 DEBUG TRAIN Batch 27/600 loss 3.138952 loss_att 3.002487 loss_ctc 3.457371 lr 0.00140262 rank 0\n",
            "2022-05-24 02:18:10,274 DEBUG TRAIN Batch 27/700 loss 5.785320 loss_att 5.173504 loss_ctc 7.212891 lr 0.00140227 rank 0\n",
            "2022-05-24 02:18:40,089 DEBUG TRAIN Batch 27/800 loss 7.905642 loss_att 7.261998 loss_ctc 9.407476 lr 0.00140193 rank 0\n",
            "2022-05-24 02:19:10,253 DEBUG TRAIN Batch 27/900 loss 5.735034 loss_att 5.715441 loss_ctc 5.780751 lr 0.00140159 rank 0\n",
            "2022-05-24 02:19:40,613 DEBUG TRAIN Batch 27/1000 loss 3.958477 loss_att 3.506037 loss_ctc 5.014171 lr 0.00140124 rank 0\n",
            "2022-05-24 02:20:10,277 DEBUG TRAIN Batch 27/1100 loss 5.923908 loss_att 5.221364 loss_ctc 7.563178 lr 0.00140090 rank 0\n",
            "2022-05-24 02:20:39,482 DEBUG TRAIN Batch 27/1200 loss 5.882871 loss_att 5.383330 loss_ctc 7.048465 lr 0.00140055 rank 0\n",
            "2022-05-24 02:21:09,567 DEBUG TRAIN Batch 27/1300 loss 3.811775 loss_att 3.693193 loss_ctc 4.088467 lr 0.00140021 rank 0\n",
            "2022-05-24 02:21:39,974 DEBUG TRAIN Batch 27/1400 loss 5.726387 loss_att 5.480017 loss_ctc 6.301252 lr 0.00139987 rank 0\n",
            "2022-05-24 02:22:10,771 DEBUG TRAIN Batch 27/1500 loss 4.909576 loss_att 4.326250 loss_ctc 6.270671 lr 0.00139953 rank 0\n",
            "2022-05-24 02:22:40,493 DEBUG TRAIN Batch 27/1600 loss 4.788073 loss_att 4.225622 loss_ctc 6.100459 lr 0.00139918 rank 0\n",
            "2022-05-24 02:23:10,131 DEBUG TRAIN Batch 27/1700 loss 6.472618 loss_att 5.789456 loss_ctc 8.066662 lr 0.00139884 rank 0\n",
            "2022-05-24 02:23:40,169 DEBUG TRAIN Batch 27/1800 loss 6.846308 loss_att 6.267817 loss_ctc 8.196119 lr 0.00139850 rank 0\n",
            "2022-05-24 02:24:10,316 DEBUG TRAIN Batch 27/1900 loss 7.472940 loss_att 6.973174 loss_ctc 8.639063 lr 0.00139816 rank 0\n",
            "2022-05-24 02:24:40,725 DEBUG TRAIN Batch 27/2000 loss 4.310050 loss_att 3.878047 loss_ctc 5.318056 lr 0.00139782 rank 0\n",
            "2022-05-24 02:25:09,864 DEBUG TRAIN Batch 27/2100 loss 6.748807 loss_att 5.855161 loss_ctc 8.833981 lr 0.00139747 rank 0\n",
            "2022-05-24 02:25:39,564 DEBUG TRAIN Batch 27/2200 loss 5.327981 loss_att 4.612528 loss_ctc 6.997370 lr 0.00139713 rank 0\n",
            "2022-05-24 02:26:09,267 DEBUG TRAIN Batch 27/2300 loss 5.787282 loss_att 5.178446 loss_ctc 7.207901 lr 0.00139679 rank 0\n",
            "2022-05-24 02:26:39,203 DEBUG TRAIN Batch 27/2400 loss 5.880402 loss_att 5.420085 loss_ctc 6.954473 lr 0.00139645 rank 0\n",
            "2022-05-24 02:27:10,055 DEBUG TRAIN Batch 27/2500 loss 4.317827 loss_att 3.712331 loss_ctc 5.730650 lr 0.00139611 rank 0\n",
            "2022-05-24 02:27:39,316 DEBUG TRAIN Batch 27/2600 loss 4.670224 loss_att 4.071124 loss_ctc 6.068124 lr 0.00139577 rank 0\n",
            "2022-05-24 02:28:09,213 DEBUG TRAIN Batch 27/2700 loss 5.225494 loss_att 5.013803 loss_ctc 5.719440 lr 0.00139543 rank 0\n",
            "2022-05-24 02:28:38,994 DEBUG TRAIN Batch 27/2800 loss 6.291588 loss_att 6.113455 loss_ctc 6.707232 lr 0.00139509 rank 0\n",
            "2022-05-24 02:29:08,938 DEBUG TRAIN Batch 27/2900 loss 5.439913 loss_att 5.286642 loss_ctc 5.797544 lr 0.00139475 rank 0\n",
            "2022-05-24 02:29:40,023 DEBUG TRAIN Batch 27/3000 loss 5.757011 loss_att 5.317065 loss_ctc 6.783552 lr 0.00139441 rank 0\n",
            "2022-05-24 02:30:09,713 DEBUG TRAIN Batch 27/3100 loss 5.304166 loss_att 4.569458 loss_ctc 7.018484 lr 0.00139408 rank 0\n",
            "2022-05-24 02:30:39,617 DEBUG TRAIN Batch 27/3200 loss 10.241388 loss_att 9.060350 loss_ctc 12.997145 lr 0.00139374 rank 0\n",
            "2022-05-24 02:31:09,167 DEBUG TRAIN Batch 27/3300 loss 7.184365 loss_att 5.948812 loss_ctc 10.067322 lr 0.00139340 rank 0\n",
            "2022-05-24 02:31:39,274 DEBUG TRAIN Batch 27/3400 loss 8.821737 loss_att 7.528288 loss_ctc 11.839785 lr 0.00139306 rank 0\n",
            "2022-05-24 02:32:10,261 DEBUG TRAIN Batch 27/3500 loss 6.842599 loss_att 5.921386 loss_ctc 8.992098 lr 0.00139272 rank 0\n",
            "2022-05-24 02:32:39,623 DEBUG TRAIN Batch 27/3600 loss 4.835318 loss_att 3.672651 loss_ctc 7.548206 lr 0.00139238 rank 0\n",
            "2022-05-24 02:33:09,148 DEBUG TRAIN Batch 27/3700 loss 7.498459 loss_att 6.916719 loss_ctc 8.855850 lr 0.00139205 rank 0\n",
            "2022-05-24 02:33:38,840 DEBUG TRAIN Batch 27/3800 loss 4.972775 loss_att 4.218678 loss_ctc 6.732334 lr 0.00139171 rank 0\n",
            "2022-05-24 02:34:09,298 DEBUG TRAIN Batch 27/3900 loss 7.254627 loss_att 6.491764 loss_ctc 9.034640 lr 0.00139137 rank 0\n",
            "2022-05-24 02:34:39,900 DEBUG TRAIN Batch 27/4000 loss 6.439059 loss_att 5.286473 loss_ctc 9.128426 lr 0.00139104 rank 0\n",
            "2022-05-24 02:35:09,320 DEBUG TRAIN Batch 27/4100 loss 5.194916 loss_att 4.364179 loss_ctc 7.133301 lr 0.00139070 rank 0\n",
            "2022-05-24 02:35:38,808 DEBUG TRAIN Batch 27/4200 loss 6.936874 loss_att 6.077404 loss_ctc 8.942304 lr 0.00139036 rank 0\n",
            "2022-05-24 02:36:08,673 DEBUG TRAIN Batch 27/4300 loss 6.752145 loss_att 6.545101 loss_ctc 7.235248 lr 0.00139003 rank 0\n",
            "2022-05-24 02:36:38,984 DEBUG TRAIN Batch 27/4400 loss 8.788664 loss_att 7.690308 loss_ctc 11.351495 lr 0.00138969 rank 0\n",
            "2022-05-24 02:37:09,612 DEBUG TRAIN Batch 27/4500 loss 3.873882 loss_att 3.243042 loss_ctc 5.345841 lr 0.00138936 rank 0\n",
            "2022-05-24 02:37:39,428 DEBUG TRAIN Batch 27/4600 loss 5.314924 loss_att 4.905016 loss_ctc 6.271377 lr 0.00138902 rank 0\n",
            "2022-05-24 02:38:09,122 DEBUG TRAIN Batch 27/4700 loss 7.095565 loss_att 6.839302 loss_ctc 7.693512 lr 0.00138869 rank 0\n",
            "2022-05-24 02:38:38,620 DEBUG TRAIN Batch 27/4800 loss 7.118878 loss_att 7.041496 loss_ctc 7.299436 lr 0.00138835 rank 0\n",
            "2022-05-24 02:39:08,448 DEBUG TRAIN Batch 27/4900 loss 5.336096 loss_att 5.488590 loss_ctc 4.980277 lr 0.00138802 rank 0\n",
            "2022-05-24 02:39:39,489 DEBUG TRAIN Batch 27/5000 loss 8.880686 loss_att 7.895940 loss_ctc 11.178426 lr 0.00138768 rank 0\n",
            "2022-05-24 02:40:08,753 DEBUG TRAIN Batch 27/5100 loss 5.726971 loss_att 5.053028 loss_ctc 7.299503 lr 0.00138735 rank 0\n",
            "2022-05-24 02:40:38,154 DEBUG TRAIN Batch 27/5200 loss 6.007269 loss_att 5.191831 loss_ctc 7.909957 lr 0.00138702 rank 0\n",
            "2022-05-24 02:41:08,034 DEBUG TRAIN Batch 27/5300 loss 7.037131 loss_att 6.133834 loss_ctc 9.144824 lr 0.00138668 rank 0\n",
            "2022-05-24 02:41:38,279 DEBUG TRAIN Batch 27/5400 loss 7.772787 loss_att 6.605899 loss_ctc 10.495525 lr 0.00138635 rank 0\n",
            "2022-05-24 02:42:09,138 DEBUG TRAIN Batch 27/5500 loss 5.569058 loss_att 4.827929 loss_ctc 7.298360 lr 0.00138602 rank 0\n",
            "2022-05-24 02:42:38,762 DEBUG TRAIN Batch 27/5600 loss 5.494420 loss_att 5.162289 loss_ctc 6.269393 lr 0.00138568 rank 0\n",
            "2022-05-24 02:43:08,337 DEBUG TRAIN Batch 27/5700 loss 5.268203 loss_att 5.117368 loss_ctc 5.620151 lr 0.00138535 rank 0\n",
            "2022-05-24 02:43:38,133 DEBUG TRAIN Batch 27/5800 loss 7.926844 loss_att 7.029870 loss_ctc 10.019783 lr 0.00138502 rank 0\n",
            "2022-05-24 02:44:07,790 DEBUG TRAIN Batch 27/5900 loss 5.545558 loss_att 5.257001 loss_ctc 6.218857 lr 0.00138469 rank 0\n",
            "2022-05-24 02:44:38,634 DEBUG TRAIN Batch 27/6000 loss 5.096576 loss_att 4.349366 loss_ctc 6.840065 lr 0.00138436 rank 0\n",
            "2022-05-24 02:45:08,558 DEBUG TRAIN Batch 27/6100 loss 3.762816 loss_att 3.510587 loss_ctc 4.351351 lr 0.00138403 rank 0\n",
            "2022-05-24 02:45:38,234 DEBUG TRAIN Batch 27/6200 loss 6.857360 loss_att 5.933537 loss_ctc 9.012949 lr 0.00138369 rank 0\n",
            "2022-05-24 02:46:08,099 DEBUG TRAIN Batch 27/6300 loss 9.992725 loss_att 8.356512 loss_ctc 13.810558 lr 0.00138336 rank 0\n",
            "2022-05-24 02:46:38,313 DEBUG TRAIN Batch 27/6400 loss 6.249754 loss_att 5.673426 loss_ctc 7.594522 lr 0.00138303 rank 0\n",
            "2022-05-24 02:47:09,301 DEBUG TRAIN Batch 27/6500 loss 9.204039 loss_att 7.888950 loss_ctc 12.272579 lr 0.00138270 rank 0\n",
            "2022-05-24 02:47:38,664 DEBUG TRAIN Batch 27/6600 loss 4.393578 loss_att 3.968487 loss_ctc 5.385457 lr 0.00138237 rank 0\n",
            "2022-05-24 02:48:08,791 DEBUG TRAIN Batch 27/6700 loss 5.936985 loss_att 5.193459 loss_ctc 7.671881 lr 0.00138204 rank 0\n",
            "2022-05-24 02:48:38,357 DEBUG TRAIN Batch 27/6800 loss 6.558008 loss_att 6.201240 loss_ctc 7.390468 lr 0.00138171 rank 0\n",
            "2022-05-24 02:49:07,898 DEBUG TRAIN Batch 27/6900 loss 7.446902 loss_att 7.302880 loss_ctc 7.782952 lr 0.00138138 rank 0\n",
            "2022-05-24 02:49:38,593 DEBUG TRAIN Batch 27/7000 loss 7.641812 loss_att 6.566847 loss_ctc 10.150064 lr 0.00138105 rank 0\n",
            "2022-05-24 02:50:08,166 DEBUG TRAIN Batch 27/7100 loss 5.462825 loss_att 4.855484 loss_ctc 6.879955 lr 0.00138072 rank 0\n",
            "2022-05-24 02:50:37,804 DEBUG TRAIN Batch 27/7200 loss 4.336786 loss_att 3.720226 loss_ctc 5.775428 lr 0.00138039 rank 0\n",
            "2022-05-24 02:51:07,562 DEBUG TRAIN Batch 27/7300 loss 5.875346 loss_att 5.264755 loss_ctc 7.300060 lr 0.00138007 rank 0\n",
            "2022-05-24 02:51:37,936 DEBUG TRAIN Batch 27/7400 loss 6.363905 loss_att 6.195052 loss_ctc 6.757895 lr 0.00137974 rank 0\n",
            "2022-05-24 02:52:08,190 DEBUG TRAIN Batch 27/7500 loss 5.223271 loss_att 4.685252 loss_ctc 6.478651 lr 0.00137941 rank 0\n",
            "2022-05-24 02:52:14,648 DEBUG CV Batch 27/0 loss 3.539940 loss_att 3.007629 loss_ctc 4.782000 history loss 3.331709 rank 0\n",
            "2022-05-24 02:52:26,239 DEBUG CV Batch 27/100 loss 2.535175 loss_att 2.506296 loss_ctc 2.602560 history loss 5.248593 rank 0\n",
            "2022-05-24 02:52:36,966 DEBUG CV Batch 27/200 loss 5.048298 loss_att 4.902491 loss_ctc 5.388515 history loss 5.208837 rank 0\n",
            "2022-05-24 02:52:48,058 DEBUG CV Batch 27/300 loss 3.438472 loss_att 3.514344 loss_ctc 3.261436 history loss 4.864616 rank 0\n",
            "2022-05-24 02:52:59,970 DEBUG CV Batch 27/400 loss 8.232949 loss_att 7.578529 loss_ctc 9.759931 history loss 4.487317 rank 0\n",
            "2022-05-24 02:53:12,257 DEBUG CV Batch 27/500 loss 2.826989 loss_att 2.569697 loss_ctc 3.427338 history loss 4.439084 rank 0\n",
            "2022-05-24 02:53:24,106 DEBUG CV Batch 27/600 loss 3.163464 loss_att 3.482318 loss_ctc 2.419471 history loss 4.395045 rank 0\n",
            "2022-05-24 02:53:35,206 DEBUG CV Batch 27/700 loss 2.642256 loss_att 2.428053 loss_ctc 3.142061 history loss 4.294550 rank 0\n",
            "2022-05-24 02:53:46,923 DEBUG CV Batch 27/800 loss 4.161305 loss_att 3.924948 loss_ctc 4.712805 history loss 4.259322 rank 0\n",
            "2022-05-24 02:53:58,579 INFO Epoch 27 CV info cv_loss 4.260732077267545\n",
            "2022-05-24 02:53:58,579 INFO Checkpoint: save to checkpoint exp/conformer/27.pt\n",
            "2022-05-24 02:53:58,915 INFO Epoch 28 TRAIN info lr 0.0013793956370877478\n",
            "2022-05-24 02:53:58,917 INFO using accumulate grad, new batch size is 4 times larger than before\n",
            "2022-05-24 02:54:24,292 DEBUG TRAIN Batch 28/0 loss 5.303364 loss_att 5.109797 loss_ctc 5.755018 lr 0.00137938 rank 0\n",
            "2022-05-24 02:54:54,337 DEBUG TRAIN Batch 28/100 loss 6.237309 loss_att 5.123350 loss_ctc 8.836548 lr 0.00137905 rank 0\n",
            "2022-05-24 02:55:23,925 DEBUG TRAIN Batch 28/200 loss 5.019186 loss_att 4.518911 loss_ctc 6.186494 lr 0.00137873 rank 0\n",
            "2022-05-24 02:55:53,166 DEBUG TRAIN Batch 28/300 loss 5.567722 loss_att 5.107921 loss_ctc 6.640594 lr 0.00137840 rank 0\n",
            "2022-05-24 02:56:23,196 DEBUG TRAIN Batch 28/400 loss 5.277012 loss_att 5.450787 loss_ctc 4.871538 lr 0.00137807 rank 0\n",
            "2022-05-24 02:56:54,065 DEBUG TRAIN Batch 28/500 loss 6.520386 loss_att 5.829404 loss_ctc 8.132676 lr 0.00137775 rank 0\n",
            "2022-05-24 02:57:23,584 DEBUG TRAIN Batch 28/600 loss 4.365344 loss_att 3.759354 loss_ctc 5.779321 lr 0.00137742 rank 0\n",
            "2022-05-24 02:57:52,959 DEBUG TRAIN Batch 28/700 loss 7.432545 loss_att 6.795003 loss_ctc 8.920143 lr 0.00137709 rank 0\n",
            "2022-05-24 02:58:22,576 DEBUG TRAIN Batch 28/800 loss 6.612910 loss_att 6.464528 loss_ctc 6.959136 lr 0.00137677 rank 0\n",
            "2022-05-24 02:58:52,808 DEBUG TRAIN Batch 28/900 loss 6.103213 loss_att 5.597300 loss_ctc 7.283677 lr 0.00137644 rank 0\n",
            "2022-05-24 02:59:23,908 DEBUG TRAIN Batch 28/1000 loss 4.277885 loss_att 3.994003 loss_ctc 4.940279 lr 0.00137611 rank 0\n",
            "2022-05-24 02:59:53,154 DEBUG TRAIN Batch 28/1100 loss 3.395736 loss_att 3.273864 loss_ctc 3.680102 lr 0.00137579 rank 0\n",
            "2022-05-24 03:00:22,894 DEBUG TRAIN Batch 28/1200 loss 6.903103 loss_att 6.243104 loss_ctc 8.443102 lr 0.00137546 rank 0\n",
            "2022-05-24 03:00:53,182 DEBUG TRAIN Batch 28/1300 loss 4.287169 loss_att 4.264192 loss_ctc 4.340783 lr 0.00137514 rank 0\n",
            "2022-05-24 03:01:22,903 DEBUG TRAIN Batch 28/1400 loss 6.953663 loss_att 6.535074 loss_ctc 7.930369 lr 0.00137481 rank 0\n",
            "2022-05-24 03:01:53,670 DEBUG TRAIN Batch 28/1500 loss 5.750537 loss_att 4.515481 loss_ctc 8.632334 lr 0.00137449 rank 0\n",
            "2022-05-24 03:02:22,891 DEBUG TRAIN Batch 28/1600 loss 3.907135 loss_att 3.759087 loss_ctc 4.252578 lr 0.00137416 rank 0\n",
            "2022-05-24 03:02:52,522 DEBUG TRAIN Batch 28/1700 loss 5.855065 loss_att 5.118392 loss_ctc 7.573970 lr 0.00137384 rank 0\n",
            "2022-05-24 03:03:21,845 DEBUG TRAIN Batch 28/1800 loss 5.908291 loss_att 5.586641 loss_ctc 6.658807 lr 0.00137351 rank 0\n",
            "2022-05-24 03:03:52,126 DEBUG TRAIN Batch 28/1900 loss 8.474143 loss_att 7.334352 loss_ctc 11.133655 lr 0.00137319 rank 0\n",
            "2022-05-24 03:04:22,827 DEBUG TRAIN Batch 28/2000 loss 4.369262 loss_att 3.808616 loss_ctc 5.677436 lr 0.00137287 rank 0\n",
            "2022-05-24 03:04:52,169 DEBUG TRAIN Batch 28/2100 loss 4.489912 loss_att 4.272973 loss_ctc 4.996104 lr 0.00137254 rank 0\n",
            "2022-05-24 03:05:21,634 DEBUG TRAIN Batch 28/2200 loss 4.959343 loss_att 4.655344 loss_ctc 5.668674 lr 0.00137222 rank 0\n",
            "2022-05-24 03:05:51,363 DEBUG TRAIN Batch 28/2300 loss 4.704126 loss_att 4.709612 loss_ctc 4.691327 lr 0.00137190 rank 0\n",
            "2022-05-24 03:06:21,367 DEBUG TRAIN Batch 28/2400 loss 5.780252 loss_att 5.073188 loss_ctc 7.430067 lr 0.00137158 rank 0\n",
            "2022-05-24 03:06:52,003 DEBUG TRAIN Batch 28/2500 loss 4.241137 loss_att 3.906715 loss_ctc 5.021454 lr 0.00137125 rank 0\n",
            "2022-05-24 03:07:21,579 DEBUG TRAIN Batch 28/2600 loss 5.008878 loss_att 4.337146 loss_ctc 6.576252 lr 0.00137093 rank 0\n",
            "2022-05-24 03:07:51,519 DEBUG TRAIN Batch 28/2700 loss 5.744555 loss_att 5.114602 loss_ctc 7.214442 lr 0.00137061 rank 0\n",
            "2022-05-24 03:08:21,450 DEBUG TRAIN Batch 28/2800 loss 6.560375 loss_att 6.542594 loss_ctc 6.601862 lr 0.00137029 rank 0\n",
            "2022-05-24 03:08:51,455 DEBUG TRAIN Batch 28/2900 loss 7.645971 loss_att 7.412777 loss_ctc 8.190089 lr 0.00136997 rank 0\n",
            "2022-05-24 03:09:22,401 DEBUG TRAIN Batch 28/3000 loss 5.146682 loss_att 4.435825 loss_ctc 6.805348 lr 0.00136964 rank 0\n",
            "2022-05-24 03:09:52,336 DEBUG TRAIN Batch 28/3100 loss 5.324307 loss_att 4.864333 loss_ctc 6.397582 lr 0.00136932 rank 0\n",
            "2022-05-24 03:10:22,069 DEBUG TRAIN Batch 28/3200 loss 3.831291 loss_att 3.580348 loss_ctc 4.416824 lr 0.00136900 rank 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "tail -f run.sh.log.stage.-1"
      ],
      "metadata": {
        "id": "uypYdPc-Qq4V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "outputId": "aca01438-e865-4b63-f19d-1bb5a17a89af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2830400K .......... .......... .......... .......... .......... 18% 10.2M 9m51s\n",
            "2830450K .......... .......... .......... .......... .......... 18%  171M 9m51s\n",
            "2830500K .......... .......... .......... .......... .......... 18%  244M 9m51s\n",
            "2830550K .......... .......... .......... .......... .......... 18% 3.46M 9m51s\n",
            "2830600K .......... .......... .......... .......... .......... 18%  206M 9m51s\n",
            "2830650K .......... .......... .......... .......... .......... 18% 14.6M 9m51s\n",
            "2830700K .......... .......... .......... .......... .......... 18% 53.4M 9m51s\n",
            "2830750K .......... .......... .......... .......... .......... 18%  152M 9m51s\n",
            "2830800K .......... .......... .......... .......... .......... 18%  162M 9m51s\n",
            "2830850K .......... .......... ...."
          ]
        },
        {
          "output_type": "error",
          "ename": "CalledProcessError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-eeff040f2994>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'shell'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tail -f run.sh.log.stage.-1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_shell_cell_magic\u001b[0;34m(args, cmd)\u001b[0m\n\u001b[1;32m    111\u001b[0m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_streamed_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparsed_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_errors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_returncode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36mcheck_returncode\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m       raise subprocess.CalledProcessError(\n\u001b[0;32m--> 139\u001b[0;31m           returncode=self.returncode, cmd=self.args, output=self.output)\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_repr_pretty_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=unused-argument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCalledProcessError\u001b[0m: Command 'tail -f run.sh.log.stage.-1' died with <Signals.SIGINT: 2>."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd local"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DApkrollWwaG",
        "outputId": "196a11fd-4e14-414f-c6b2-22f7fdd03e4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/Colab Notebooks/wenet/examples/aishell/s0/local\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzkhL1OoWz2V",
        "outputId": "be42e34b-615e-4dd2-8d41-a9fb0fe4cb3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aishell_data_prep.sh  download_and_untar.sh\n",
            "aishell_train_lms.sh  run.sh.log.stage.-1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install dos2unix.*"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kaAReMMgTg9k",
        "outputId": "660e8011-83f3-4580-c75e-c4dbc33ce152"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Note, selecting 'dos2unix' for regex 'dos2unix.*'\n",
            "The following packages were automatically installed and are no longer required:\n",
            "  libnvidia-common-460 nsight-compute-2020.2.0\n",
            "Use 'apt autoremove' to remove them.\n",
            "The following NEW packages will be installed:\n",
            "  dos2unix\n",
            "0 upgraded, 1 newly installed, 0 to remove and 42 not upgraded.\n",
            "Need to get 351 kB of archives.\n",
            "After this operation, 1,267 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 dos2unix amd64 7.3.4-3 [351 kB]\n",
            "Fetched 351 kB in 2s (220 kB/s)\n",
            "Selecting previously unselected package dos2unix.\n",
            "(Reading database ... 155203 files and directories currently installed.)\n",
            "Preparing to unpack .../dos2unix_7.3.4-3_amd64.deb ...\n",
            "Unpacking dos2unix (7.3.4-3) ...\n",
            "Setting up dos2unix (7.3.4-3) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd local\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBOm76JmUXRl",
        "outputId": "57c91166-e163-438d-d8e5-35cac55475db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'local'\n",
            "/content/gdrive/MyDrive/Colab Notebooks/wenet/examples/aishell/s0/local\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dos2unix download_and_untar.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "id": "jTSKH4QBTTp1",
        "outputId": "503ca23a-1ae9-4c17-8d43-2d60483f790b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-61-1faa704269be>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    dos2unix download_and_untar.sh\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    }
  ]
}